{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916fa5ca",
   "metadata": {},
   "source": [
    "# Edge-of-Inequality Simulations: Optimal k and Variance Analysis\n",
    "\n",
    "This script generates Monte Carlo samples for fixed q and gamma, varying rho,\n",
    "then processes them for different tau and kappa values to study optimal k and variance.\n",
    "\n",
    "Strategy: Generate (Y, Z) once per rho (expensive), then generate X on-the-fly for each kappa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41f112",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "###################### Packages\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import numba\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# we configure matplotlib settings\n",
    "plt.rcParams['figure.figsize'] = [6, 6]\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "mpl.rcParams['mathtext.fontset'] = 'cm'\n",
    "mpl.rcParams['mathtext.rm'] = 'serif'\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.size'] = 22\n",
    "mpl.rcParams['axes.formatter.limits'] = (-6, 6)\n",
    "mpl.rcParams['axes.formatter.use_mathtext'] = True\n",
    "mpl.rcParams['font.family'] = 'STIXGeneral'\n",
    "mpl.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
    "mpl.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
    "mpl.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
    "mpl.rcParams['xtick.minor.visible'] = True\n",
    "mpl.rcParams['ytick.minor.visible'] = True\n",
    "plt.rcParams['ytick.right'] = True\n",
    "plt.rcParams['xtick.top'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92261e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "###################### Function Implementations\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def Burr_quantile_function(x, theta, rho):\n",
    "    \"\"\"Burr quantile function.\"\"\"\n",
    "    return ((1-x)**(-1/theta)-1)**(1/rho)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func(d):\n",
    "    \"\"\"Beta function for index vector.\"\"\"\n",
    "    grid = np.linspace(0, 1, d)\n",
    "    norm = np.sqrt(np.sum(np.sin(2*np.pi*grid)**2)/d)\n",
    "    return np.sin(2*np.pi*grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def sigma(u, c, snr):\n",
    "    \"\"\"Sigma function for noise variance.\"\"\"\n",
    "    return (u**c)/snr\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def noise_mean(d, mu):\n",
    "    \"\"\"Noise mean function.\"\"\"\n",
    "    grid = np.linspace(0, 1, d)\n",
    "    return mu*grid\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def sort_2d_array(x):\n",
    "    \"\"\"Sort each row of a 2D array.\"\"\"\n",
    "    n, m = np.shape(x)\n",
    "    for row in numba.prange(n):\n",
    "        x[row] = np.sort(x[row])\n",
    "    return x\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_var(Y, Z, H, c, snr, mu):\n",
    "    \"\"\"Generate fBm noise that depends on Y and kappa (c).\"\"\"\n",
    "    N = Y.shape[0]\n",
    "    n = Y.shape[1]\n",
    "    d = Z.shape[2] + 1\n",
    "    out = np.zeros((N, n, d))\n",
    "    H2 = 2 * H\n",
    "    for p in numba.prange(N):\n",
    "        for q in numba.prange(n):\n",
    "            matcov = np.zeros((d-1, d-1))\n",
    "            for i in numba.prange(d-1):\n",
    "                for j in numba.prange(i, d-1):\n",
    "                    r = (sigma(Y[p, q], c, snr)**2) * (1/2) * (abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "                    r = r/(d**H2)\n",
    "                    matcov[i, j] = r\n",
    "                    matcov[j, i] = matcov[i, j]\n",
    "            L = np.linalg.cholesky(matcov)\n",
    "            fBm = np.dot(L, Z[p, q, :])\n",
    "            out[p, q, :] = np.asarray([0] + list(fBm)) + noise_mean(d, mu)\n",
    "    return out\n",
    "\n",
    "def fepls(X, Y, y_matrix, tau):\n",
    "    \"\"\"FEPLS estimator.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((N, d))\n",
    "    for j in range(d):\n",
    "        aux = np.multiply(X[:, :, j], Y**tau)\n",
    "        out2 = np.multiply(aux, np.greater_equal(Y, y_matrix))\n",
    "        out[:, j] = np.sum(out2, axis=1)/n\n",
    "    norms = np.sqrt(np.sum(out**2, axis=1)/d)\n",
    "    out2 = out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def fepls_numba(X, Y, y_matrix, tau):\n",
    "    \"\"\"FEPLS estimator (numba version).\"\"\"\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((N, d))\n",
    "    for j in numba.prange(d):\n",
    "        aux = np.multiply(X[:, :, j], Y**tau)\n",
    "        out2 = np.multiply(aux, np.greater_equal(Y, y_matrix))\n",
    "        out[:, j] = np.sum(out2, axis=1)/n\n",
    "    norms = np.sqrt(np.sum(out**2, axis=1)/d)\n",
    "    out2 = out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def concomittant_corr(X, Y, Y_sort_index, tau, m):\n",
    "    \"\"\"Compute correlation for different k values.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((N, m))\n",
    "    YY = np.copy(Y)\n",
    "    Y_sort = sort_2d_array(YY)\n",
    "    for k in numba.prange(m):\n",
    "        y_array = np.zeros((N, n, k+1))\n",
    "        aux = np.zeros((N, k+1))\n",
    "        aux2 = np.zeros((N, k+1))\n",
    "        aux3 = Y_sort[:, n-k-1:]\n",
    "        aux3_sum = np.sum(aux3, axis=1)\n",
    "        for i in numba.prange(k):\n",
    "            y_array[:, 0, i] = Y_sort[:, n-i-1]\n",
    "            for j_2 in numba.prange(N):\n",
    "                y_array[j_2, :, i] = y_array[j_2, 0, i]\n",
    "            hat_beta = fepls_numba(X, Y, y_array[:, :, i], tau)\n",
    "            for j_1 in numba.prange(N):\n",
    "                i_c = Y_sort_index[j_1, i]\n",
    "                aux[j_1, i] = (1/d)*np.sum(np.multiply(hat_beta[j_1, :], X[j_1, i_c, :]))\n",
    "                aux2[j_1, i] = np.multiply(aux[j_1, i], Y_sort[j_1, n-i-1])\n",
    "                out[j_1, k] = np.corrcoef(aux3[j_1, :], aux[j_1, :])[0, 1]\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def threshold_index(X, Y, Y_sort_index, tau, m, start):\n",
    "    \"\"\"Compute threshold index.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    out = np.zeros((N,))\n",
    "    aux = concomittant_corr(X, Y, Y_sort_index, tau, m)[:, start:]\n",
    "    return start + np.argmax(aux, axis=1)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def threshold(X, Y, Y_sort_index, tau, m, start):\n",
    "    \"\"\"Compute threshold matrix.\"\"\"\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    y_matrix_out = np.zeros((N, n))\n",
    "    YY = np.copy(Y)\n",
    "    Y_sort = sort_2d_array(YY)\n",
    "    index = threshold_index(X, Y, Y_sort_index, tau, m, start)\n",
    "    for i in numba.prange(N):\n",
    "        y_matrix_out[i, :] = Y_sort[i, n-index[i]-1]*np.ones((n,))\n",
    "    return y_matrix_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a27ec",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "###################### Configuration\n",
    "# Fixed parameters\n",
    "Q = 2.1  # fixed q (noise integrability order) - using 2.1 to satisfy q>2 constraint (user mentioned q=2, but theory requires q>2)\n",
    "GAMMA = 0.99  # fixed gamma (tail index) - using 0.99 to satisfy gamma<1 constraint (user mentioned gamma=1, but theory requires gamma<1)\n",
    "N_MC = 1000  # number of Monte Carlo replications (increased for \"a lot\" of simulations)\n",
    "N_SAMPLES = 500  # sample size n\n",
    "D = 101  # dimension d\n",
    "SNR = 10.0  # signal-to-noise ratio\n",
    "H = 1/3  # Hurst parameter\n",
    "MU = 200.0  # noise mean\n",
    "START = 4  # minimum k for threshold selection\n",
    "\n",
    "# Varying parameters\n",
    "RHO_VALUES = [-2.0, -1.0, -0.5]  # different rho values to test\n",
    "KAPPA_VALUES = np.linspace(0.5, 3.0, 11)  # kappa (c) values to test\n",
    "# we generate tau values near the edges of the inequality 0 < 2*(kappa+tau)*gamma < 1\n",
    "# for each kappa, we compute valid tau range and sample near boundaries\n",
    "TAU_GRID_SIZE = 20  # number of tau values to test per kappa\n",
    "\n",
    "# Output directory\n",
    "BASE_DIR = Path(\"data/simuls\")\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e36a5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "###################### Helper Functions\n",
    "\n",
    "def check_cond_edge(gamma, c, tau, q):\n",
    "    \"\"\"Check if parameters satisfy the edge conditions.\"\"\"\n",
    "    if tau > 0:\n",
    "        return (gamma < 1 and gamma > 0 and q > 2 and c > 0 and \n",
    "                0 < 2*(c+tau)*gamma < 1 and q*(1-2*tau*gamma) > 2 and \n",
    "                q*c*gamma > 1 and tau < 1/(2*gamma))\n",
    "    else:\n",
    "        return (gamma < 1 and gamma > 0 and q > 2 and c > 0 and \n",
    "                0 < 2*(c+tau)*gamma < 1 and q*c*gamma > 1)\n",
    "\n",
    "def get_data_path(rho):\n",
    "    \"\"\"Get the path for saving/loading data for a given rho.\"\"\"\n",
    "    rho_str = f\"{rho:.2f}\".replace('.', 'p').replace('-', 'm')\n",
    "    rho_dir = BASE_DIR / f\"rho{rho_str}\"\n",
    "    rho_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return rho_dir / \"data.npz\"\n",
    "\n",
    "def generate_data_once(rho, random_seed=0):\n",
    "    \"\"\"Generate (Y, Z) data once for a given rho. This is expensive, so we save it.\"\"\"\n",
    "    data_path = get_data_path(rho)\n",
    "    \n",
    "    # we check if data already exists\n",
    "    if data_path.exists():\n",
    "        print(f\"Data for rho={rho} already exists at {data_path}, skipping generation.\")\n",
    "        return data_path\n",
    "    \n",
    "    print(f\"Generating data for rho={rho}...\")\n",
    "    tic = time.time()\n",
    "    \n",
    "    npr.seed(random_seed)  # we fix seed for reproducibility\n",
    "    \n",
    "    # we generate Y from Burr distribution\n",
    "    # note: following the original code pattern, Burr_quantile_function is called with gamma directly\n",
    "    Y = Burr_quantile_function(npr.uniform(0, 1, size=(N_MC, N_SAMPLES)), GAMMA, rho)\n",
    "    Y_sort_index = np.argsort(Y, axis=1)\n",
    "    \n",
    "    # we generate Z for fBm (noise generation depends on kappa, so we save Z and generate X later)\n",
    "    Z = npr.normal(0, 1, size=(N_MC, N_SAMPLES, D - 1))\n",
    "    \n",
    "    # we save Y, Z, and metadata\n",
    "    np.savez_compressed(\n",
    "        data_path,\n",
    "        Y=Y,\n",
    "        Y_sort_index=Y_sort_index,\n",
    "        Z=Z,\n",
    "        rho=rho,\n",
    "        gamma=GAMMA,\n",
    "        q=Q,\n",
    "        n_mc=N_MC,\n",
    "        n_samples=N_SAMPLES,\n",
    "        d=D,\n",
    "        random_seed=random_seed\n",
    "    )\n",
    "    \n",
    "    print(f\"Data saved to {data_path} (time: {time.time()-tic:.2f}s)\")\n",
    "    return data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4a1e7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: Generating data for all rho values\n",
      "================================================================================\n",
      "[STEP 1] Number of rho values to process: 3\n",
      "[STEP 1] Rho values: [-2.0, -1.0, -0.5]\n",
      "\n",
      "[STEP 1] Processing rho 1/3: rho=-2.0\n",
      "Data for rho=-2.0 already exists at data/simuls/rhom2p00/data.npz, skipping generation.\n",
      "[STEP 1] Completed rho=-2.0\n",
      "\n",
      "[STEP 1] Processing rho 2/3: rho=-1.0\n",
      "Data for rho=-1.0 already exists at data/simuls/rhom1p00/data.npz, skipping generation.\n",
      "[STEP 1] Completed rho=-1.0\n",
      "\n",
      "[STEP 1] Processing rho 3/3: rho=-0.5\n",
      "Data for rho=-0.5 already exists at data/simuls/rhom0p50/data.npz, skipping generation.\n",
      "[STEP 1] Completed rho=-0.5\n",
      "\n",
      "[STEP 1] All data generation completed!\n",
      "[STEP 1] Generated data paths: [-2.0, -1.0, -0.5]\n"
     ]
    }
   ],
   "source": [
    "###################### Generate Data for All Rho Values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 1: Generating data for all rho values\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"[STEP 1] Number of rho values to process: {len(RHO_VALUES)}\")\n",
    "print(f\"[STEP 1] Rho values: {RHO_VALUES}\")\n",
    "\n",
    "data_paths = {}\n",
    "for i, rho in enumerate(RHO_VALUES):\n",
    "    print(f\"\\n[STEP 1] Processing rho {i+1}/{len(RHO_VALUES)}: rho={rho}\")\n",
    "    data_paths[rho] = generate_data_once(rho, random_seed=0)\n",
    "    print(f\"[STEP 1] Completed rho={rho}\")\n",
    "\n",
    "print(f\"\\n[STEP 1] All data generation completed!\")\n",
    "print(f\"[STEP 1] Generated data paths: {list(data_paths.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd562c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "###################### Process Data: Compute Optimal k and Variance for Each (tau, kappa)\n",
    "\n",
    "def get_tau_range_for_kappa(kappa, eps=1e-3):\n",
    "    \"\"\"\n",
    "    Compute valid tau range for given kappa based on inequality 0 < 2*(kappa+tau)*gamma < 1.\n",
    "    Returns (tau_min, tau_max) with small epsilon margin.\n",
    "    \"\"\"\n",
    "    tau_lower = -kappa + eps  # from 2*(kappa+tau)*gamma > 0\n",
    "    tau_upper = (1.0 / (2.0 * GAMMA)) - kappa - eps  # from 2*(kappa+tau)*gamma < 1\n",
    "    return tau_lower, tau_upper\n",
    "\n",
    "def compute_optimal_k_and_variance(rho, kappa, tau, k_range=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Load pre-generated data, generate X for given kappa, then compute optimal k and variance.\n",
    "    \n",
    "    Returns:\n",
    "        optimal_k: optimal threshold k\n",
    "        variance: variance of <hat_beta, beta> at optimal k\n",
    "        mean_alignment: mean of <hat_beta, beta> at optimal k\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] rho={rho}, kappa={kappa:.4f}, tau={tau:.4f}\")\n",
    "    \n",
    "    data_path = get_data_path(rho)\n",
    "    if not data_path.exists():\n",
    "        if verbose:\n",
    "            print(f\"    [compute_optimal_k_and_variance] ERROR: Data not found at {data_path}\")\n",
    "        raise FileNotFoundError(f\"Data not found at {data_path}. Run generation first.\")\n",
    "    \n",
    "    # we load pre-generated data\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Loading data from {data_path}...\")\n",
    "    data = np.load(data_path)\n",
    "    Y = data['Y']\n",
    "    Y_sort_index = data['Y_sort_index']\n",
    "    Z = data['Z']\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Data loaded: Y.shape={Y.shape}, Z.shape={Z.shape}\")\n",
    "    \n",
    "    # we check if kappa is valid (q*kappa*gamma > 1)\n",
    "    q_kappa_gamma = Q * kappa * GAMMA\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Checking condition: q*kappa*gamma = {q_kappa_gamma:.4f} > 1? {q_kappa_gamma > 1}\")\n",
    "    if q_kappa_gamma <= 1:\n",
    "        if verbose:\n",
    "            print(f\"    [compute_optimal_k_and_variance] Invalid: q*kappa*gamma = {q_kappa_gamma:.4f} <= 1\")\n",
    "        return None, None, None  # invalid configuration\n",
    "    \n",
    "    # we check if tau is valid (0 < 2*(kappa+tau)*gamma < 1)\n",
    "    two_kappa_tau_gamma = 2*(kappa+tau)*GAMMA\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Checking condition: 0 < 2*(kappa+tau)*gamma = {two_kappa_tau_gamma:.4f} < 1? {0 < two_kappa_tau_gamma < 1}\")\n",
    "    if not (0 < two_kappa_tau_gamma < 1):\n",
    "        if verbose:\n",
    "            print(f\"    [compute_optimal_k_and_variance] Invalid: 2*(kappa+tau)*gamma = {two_kappa_tau_gamma:.4f} not in (0, 1)\")\n",
    "        return None, None, None  # invalid configuration\n",
    "    \n",
    "    # we generate X for this specific kappa\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Generating X for kappa={kappa:.4f}...\")\n",
    "    aux = np.multiply.outer(Y ** kappa, beta_func(D))  # g(Y)*beta\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] aux computed: shape={aux.shape}\")\n",
    "    eps = coeurjolly_cholesky_fbm_var(Y, Z, H, kappa, SNR, MU)  # noise depends on kappa\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] eps computed: shape={eps.shape}, mean={eps.mean():.4f}, std={eps.std():.4f}\")\n",
    "    X = aux + eps\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] X computed: shape={X.shape}\")\n",
    "    \n",
    "    # we set k_range if not provided\n",
    "    if k_range is None:\n",
    "        k_range = np.arange(START, int(N_SAMPLES/5) + 1, 1)\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] k_range: {k_range.min()} to {k_range.max()}, length={len(k_range)}\")\n",
    "    \n",
    "    # we compute correlation for different k values to find optimal\n",
    "    m = min(100, N_SAMPLES - START)\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Computing correlations for m={m}...\")\n",
    "    correlations = concomittant_corr(X, Y, Y_sort_index, tau, m)\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Correlations computed: shape={correlations.shape}\")\n",
    "    \n",
    "    if correlations.shape[1] > START:\n",
    "        correlations = correlations[:, START:]\n",
    "        # we find optimal k (maximize correlation)\n",
    "        optimal_k_indices = np.argmax(correlations, axis=1) + START\n",
    "        optimal_k = int(np.median(optimal_k_indices))  # we use median over MC replications\n",
    "        if verbose:\n",
    "            print(f\"    [compute_optimal_k_and_variance] Optimal k indices: min={optimal_k_indices.min()}, max={optimal_k_indices.max()}, median={optimal_k}\")\n",
    "    else:\n",
    "        optimal_k = START + 10  # we use a default value if correlation computation fails\n",
    "        if verbose:\n",
    "            print(f\"    [compute_optimal_k_and_variance] Warning: Using default optimal_k={optimal_k}\")\n",
    "    \n",
    "    # we compute FEPLS estimator at optimal k using threshold function\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Computing threshold matrix...\")\n",
    "    y_matrix = threshold(X, Y, Y_sort_index, tau, 100, START)\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Computing FEPLS estimator...\")\n",
    "    E = fepls(X, Y, y_matrix, tau)\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] FEPLS estimator computed: shape={E.shape}\")\n",
    "    \n",
    "    # we compute alignment with true beta\n",
    "    beta_true = beta_func(D)\n",
    "    hatbeta_dot_beta_vals = (1.0 / D) * np.sum(E * beta_true, axis=1)\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Alignment computed: mean={hatbeta_dot_beta_vals.mean():.4f}, std={hatbeta_dot_beta_vals.std():.4f}\")\n",
    "    \n",
    "    # we compute variance and mean\n",
    "    variance = float(np.var(hatbeta_dot_beta_vals))\n",
    "    mean_alignment = float(np.mean(hatbeta_dot_beta_vals))\n",
    "    if verbose:\n",
    "        print(f\"    [compute_optimal_k_and_variance] Results: optimal_k={optimal_k}, variance={variance:.6f}, mean_alignment={mean_alignment:.4f}\")\n",
    "    \n",
    "    return optimal_k, variance, mean_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: Processing data for all (tau, kappa) combinations\n",
      "================================================================================\n",
      "[STEP 2] Determining tau values for each kappa...\n",
      "[STEP 2] TAU_GRID_SIZE = 20\n",
      "[STEP 2] Processing kappa 1/11: kappa=0.5000\n",
      "  [STEP 2]   tau range for kappa=0.5000: [-0.4900, -0.0049]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=0.5000\n",
      "[STEP 2] Processing kappa 2/11: kappa=0.7500\n",
      "  [STEP 2]   tau range for kappa=0.7500: [-0.7400, -0.2549]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=0.7500\n",
      "[STEP 2] Processing kappa 3/11: kappa=1.0000\n",
      "  [STEP 2]   tau range for kappa=1.0000: [-0.9900, -0.5049]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=1.0000\n",
      "[STEP 2] Processing kappa 4/11: kappa=1.2500\n",
      "  [STEP 2]   tau range for kappa=1.2500: [-1.2400, -0.7549]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=1.2500\n",
      "[STEP 2] Processing kappa 5/11: kappa=1.5000\n",
      "  [STEP 2]   tau range for kappa=1.5000: [-1.4900, -1.0049]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=1.5000\n",
      "[STEP 2] Processing kappa 6/11: kappa=1.7500\n",
      "  [STEP 2]   tau range for kappa=1.7500: [-1.7400, -1.2549]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=1.7500\n",
      "[STEP 2] Processing kappa 7/11: kappa=2.0000\n",
      "  [STEP 2]   tau range for kappa=2.0000: [-1.9900, -1.5049]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=2.0000\n",
      "[STEP 2] Processing kappa 8/11: kappa=2.2500\n",
      "  [STEP 2]   tau range for kappa=2.2500: [-2.2400, -1.7549]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=2.2500\n",
      "[STEP 2] Processing kappa 9/11: kappa=2.5000\n",
      "  [STEP 2]   tau range for kappa=2.5000: [-2.4900, -2.0049]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=2.5000\n",
      "[STEP 2] Processing kappa 10/11: kappa=2.7500\n",
      "  [STEP 2]   tau range for kappa=2.7500: [-2.7400, -2.2549]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=2.7500\n",
      "[STEP 2] Processing kappa 11/11: kappa=3.0000\n",
      "  [STEP 2]   tau range for kappa=3.0000: [-2.9900, -2.5049]\n",
      "  [STEP 2]   Generated 18 tau values for kappa=3.0000\n",
      "[STEP 2] Total unique tau values to test: 176\n",
      "[STEP 2] Tau value range: [-2.9900, -0.0049]\n",
      "\n",
      "================================================================================\n",
      "[STEP 2] Processing rho 1/3: rho = -2.0\n",
      "================================================================================\n",
      "[STEP 2] Valid kappas for rho=-2.0: 11/11\n",
      "\n",
      "[STEP 2] Processing kappa 1/11: kappa = 0.5000\n",
      "  [STEP 2] Testing 18 tau values for kappa=0.5000\n",
      "    [STEP 2] Processing tau 1/18: tau=-0.4900\n"
     ]
    }
   ],
   "source": [
    "###################### Process All Combinations\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Processing data for all (tau, kappa) combinations\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results = {}  # we store results as results[rho][kappa][tau] = {'optimal_k': ..., 'variance': ..., 'mean': ...}\n",
    "all_tau_values = set()  # we collect all tau values used across kappas\n",
    "\n",
    "# we first determine tau values for each kappa (near edges)\n",
    "print(f\"[STEP 2] Determining tau values for each kappa...\")\n",
    "print(f\"[STEP 2] TAU_GRID_SIZE = {TAU_GRID_SIZE}\")\n",
    "tau_by_kappa = {}\n",
    "for i, kappa in enumerate(KAPPA_VALUES):\n",
    "    print(f\"[STEP 2] Processing kappa {i+1}/{len(KAPPA_VALUES)}: kappa={kappa:.4f}\")\n",
    "    tau_lower, tau_upper = get_tau_range_for_kappa(kappa, eps=1e-2)\n",
    "    print(f\"  [STEP 2]   tau range for kappa={kappa:.4f}: [{tau_lower:.4f}, {tau_upper:.4f}]\")\n",
    "    if tau_lower < tau_upper:\n",
    "        # we sample tau values, with more points near the boundaries\n",
    "        tau_edge_lower = np.linspace(tau_lower, tau_lower + 0.1, TAU_GRID_SIZE // 3)\n",
    "        tau_middle = np.linspace(tau_lower + 0.1, tau_upper - 0.1, TAU_GRID_SIZE // 3)\n",
    "        tau_edge_upper = np.linspace(tau_upper - 0.1, tau_upper, TAU_GRID_SIZE // 3)\n",
    "        tau_by_kappa[kappa] = np.concatenate([tau_edge_lower, tau_middle, tau_edge_upper])\n",
    "        all_tau_values.update(tau_by_kappa[kappa])\n",
    "        print(f\"  [STEP 2]   Generated {len(tau_by_kappa[kappa])} tau values for kappa={kappa:.4f}\")\n",
    "    else:\n",
    "        tau_by_kappa[kappa] = np.array([])\n",
    "        print(f\"  [STEP 2]   WARNING: No valid tau range for kappa={kappa:.4f}\")\n",
    "\n",
    "TAU_VALUES = np.array(sorted(all_tau_values))\n",
    "print(f\"[STEP 2] Total unique tau values to test: {len(TAU_VALUES)}\")\n",
    "print(f\"[STEP 2] Tau value range: [{TAU_VALUES.min():.4f}, {TAU_VALUES.max():.4f}]\")\n",
    "\n",
    "for rho_idx, rho in enumerate(RHO_VALUES):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[STEP 2] Processing rho {rho_idx+1}/{len(RHO_VALUES)}: rho = {rho}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    results[rho] = {}\n",
    "    \n",
    "    valid_kappas = [k for k in KAPPA_VALUES if k in tau_by_kappa and len(tau_by_kappa[k]) > 0]\n",
    "    print(f\"[STEP 2] Valid kappas for rho={rho}: {len(valid_kappas)}/{len(KAPPA_VALUES)}\")\n",
    "    \n",
    "    for kappa_idx, kappa in enumerate(valid_kappas):\n",
    "        print(f\"\\n[STEP 2] Processing kappa {kappa_idx+1}/{len(valid_kappas)}: kappa = {kappa:.4f}\")\n",
    "        results[rho][kappa] = {}\n",
    "        valid_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        tau_list = tau_by_kappa[kappa]\n",
    "        print(f\"  [STEP 2] Testing {len(tau_list)} tau values for kappa={kappa:.4f}\")\n",
    "        \n",
    "        for tau_idx, tau in enumerate(tau_list):\n",
    "            if (tau_idx + 1) % 5 == 0 or tau_idx == 0 or tau_idx == len(tau_list) - 1:\n",
    "                print(f\"    [STEP 2] Processing tau {tau_idx+1}/{len(tau_list)}: tau={tau:.4f}\")\n",
    "            try:\n",
    "                optimal_k, variance, mean_alignment = compute_optimal_k_and_variance(rho, kappa, tau, verbose=False)\n",
    "                if optimal_k is not None:\n",
    "                    results[rho][kappa][tau] = {\n",
    "                        'optimal_k': optimal_k,\n",
    "                        'variance': variance,\n",
    "                        'mean_alignment': mean_alignment\n",
    "                    }\n",
    "                    valid_count += 1\n",
    "                else:\n",
    "                    if (tau_idx + 1) % 5 == 0:\n",
    "                        print(f\"      [STEP 2] Invalid configuration (conditions not satisfied)\")\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                if (tau_idx + 1) % 5 == 0:\n",
    "                    print(f\"      [STEP 2] ERROR: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"  [STEP 2] Completed kappa={kappa:.4f}: {valid_count}/{len(tau_list)} valid, {error_count} errors\")\n",
    "    \n",
    "    total_configs = sum(len(results[rho][k]) for k in results[rho].keys())\n",
    "    print(f\"\\n[STEP 2] Completed rho={rho}: {total_configs} total valid configurations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece57dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Save Results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Saving results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_path = BASE_DIR / \"results_tau_kappa.npz\"\n",
    "print(f\"[STEP 3] Results will be saved to: {results_path}\")\n",
    "\n",
    "# we convert results to arrays for saving\n",
    "print(f\"[STEP 3] Converting results to dictionary format...\")\n",
    "results_dict = {}\n",
    "total_entries = 0\n",
    "for rho in RHO_VALUES:\n",
    "    if rho not in results:\n",
    "        print(f\"[STEP 3] Skipping rho={rho} (no results)\")\n",
    "        continue\n",
    "    print(f\"[STEP 3] Processing rho={rho}...\")\n",
    "    for kappa in KAPPA_VALUES:\n",
    "        if kappa not in results[rho]:\n",
    "            continue\n",
    "        for tau in results[rho][kappa].keys():\n",
    "            rho_str = f\"{rho:.2f}\".replace('.', 'p').replace('-', 'm')\n",
    "            kappa_str = f\"{kappa:.2f}\".replace('.', 'p').replace('-', 'm')\n",
    "            tau_str = f\"{tau:.4f}\".replace('.', 'p').replace('-', 'm')\n",
    "            key = f\"rho{rho_str}_kappa{kappa_str}_tau{tau_str}\"\n",
    "            results_dict[f\"{key}_optimal_k\"] = results[rho][kappa][tau]['optimal_k']\n",
    "            results_dict[f\"{key}_variance\"] = results[rho][kappa][tau]['variance']\n",
    "            results_dict[f\"{key}_mean\"] = results[rho][kappa][tau]['mean_alignment']\n",
    "            total_entries += 1\n",
    "    print(f\"[STEP 3] Processed {total_entries} entries for rho={rho}\")\n",
    "\n",
    "print(f\"[STEP 3] Total entries to save: {total_entries}\")\n",
    "\n",
    "# we also save parameter grids\n",
    "print(f\"[STEP 3] Adding parameter grids to results...\")\n",
    "results_dict['rho_values'] = np.array(RHO_VALUES)\n",
    "results_dict['kappa_values'] = np.array(KAPPA_VALUES)\n",
    "results_dict['tau_values'] = TAU_VALUES  # this is a set converted to array, may have variable length\n",
    "results_dict['gamma'] = GAMMA\n",
    "results_dict['q'] = Q\n",
    "print(f\"[STEP 3] Parameter grids added: rho_values={RHO_VALUES}, kappa_values shape={KAPPA_VALUES.shape}, tau_values shape={TAU_VALUES.shape}\")\n",
    "\n",
    "print(f\"[STEP 3] Saving to {results_path}...\")\n",
    "np.savez_compressed(results_path, **results_dict)\n",
    "print(f\"[STEP 3] Results saved successfully to {results_path}\")\n",
    "file_size = results_path.stat().st_size / (1024 * 1024)  # size in MB\n",
    "print(f\"[STEP 3] File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a800377",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Create Plots: Optimal k vs tau for different kappa\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Creating plots\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rho_idx, rho in enumerate(RHO_VALUES):\n",
    "    print(f\"\\n[STEP 4] Plotting for rho {rho_idx+1}/{len(RHO_VALUES)}: rho = {rho}\")\n",
    "    \n",
    "    # we prepare data for plotting\n",
    "    if rho not in results:\n",
    "        print(f\"  [STEP 4] No results for rho={rho}, skipping plots.\")\n",
    "        continue\n",
    "    \n",
    "    valid_kappas = [k for k in KAPPA_VALUES if k in results[rho] and len(results[rho][k]) > 0]\n",
    "    print(f\"  [STEP 4] Found {len(valid_kappas)} valid kappas for plotting\")\n",
    "    \n",
    "    if len(valid_kappas) == 0:\n",
    "        print(f\"  [STEP 4] No valid data for rho={rho}, skipping plots.\")\n",
    "        continue\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # we plot optimal k vs tau for different kappa\n",
    "    ax1 = axes[0]\n",
    "    for kappa in valid_kappas[:5]:  # we plot first 5 kappa values to avoid clutter\n",
    "        tau_list = sorted([tau for tau in results[rho][kappa].keys()])\n",
    "        optimal_k_list = [results[rho][kappa][tau]['optimal_k'] for tau in tau_list]\n",
    "        if len(tau_list) > 0:\n",
    "            ax1.plot(tau_list, optimal_k_list, marker='o', label=f'κ={kappa:.2f}')\n",
    "    \n",
    "    ax1.set_xlabel('τ (tau)')\n",
    "    ax1.set_ylabel('Optimal k')\n",
    "    ax1.set_title(f'Optimal k vs τ for different κ (rho={rho:.2f})')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # we plot variance vs tau for different kappa\n",
    "    ax2 = axes[1]\n",
    "    for kappa in valid_kappas[:5]:\n",
    "        tau_list = sorted([tau for tau in results[rho][kappa].keys()])\n",
    "        variance_list = [results[rho][kappa][tau]['variance'] for tau in tau_list]\n",
    "        if len(tau_list) > 0:\n",
    "            ax2.plot(tau_list, variance_list, marker='s', label=f'κ={kappa:.2f}')\n",
    "    \n",
    "    ax2.set_xlabel('τ (tau)')\n",
    "    ax2.set_ylabel('Variance of <β̂, β>')\n",
    "    ax2.set_title(f'Variance vs τ for different κ (rho={rho:.2f})')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')  # we use log scale for variance\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    rho_str = f\"{rho:.2f}\".replace('.', 'p').replace('-', 'm')\n",
    "    plot_path = BASE_DIR / f\"plots_rho{rho_str}.png\"\n",
    "    print(f\"  [STEP 4] Saving plot to {plot_path}...\")\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  [STEP 4] Plot saved successfully to {plot_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fc0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Create 2D Heatmaps: Optimal k and Variance as function of (tau, kappa)\n",
    "for rho_idx, rho in enumerate(RHO_VALUES):\n",
    "    print(f\"\\n[STEP 4] Creating heatmaps for rho {rho_idx+1}/{len(RHO_VALUES)}: rho = {rho}\")\n",
    "    \n",
    "    if rho not in results or len(results[rho]) == 0:\n",
    "        print(f\"  [STEP 4] No valid data for rho={rho}, skipping heatmaps.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  [STEP 4] Preparing heatmap data...\")\n",
    "    \n",
    "    # we collect all tau values for this rho\n",
    "    all_taus_rho = set()\n",
    "    for kappa in results[rho].keys():\n",
    "        all_taus_rho.update(results[rho][kappa].keys())\n",
    "    all_taus_rho = sorted(all_taus_rho)\n",
    "    \n",
    "    # we create matrices for heatmaps\n",
    "    optimal_k_matrix = np.full((len(all_taus_rho), len(KAPPA_VALUES)), np.nan)\n",
    "    variance_matrix = np.full((len(all_taus_rho), len(KAPPA_VALUES)), np.nan)\n",
    "    \n",
    "    for i, tau in enumerate(all_taus_rho):\n",
    "        for j, kappa in enumerate(KAPPA_VALUES):\n",
    "            if kappa in results[rho] and tau in results[rho][kappa]:\n",
    "                optimal_k_matrix[i, j] = results[rho][kappa][tau]['optimal_k']\n",
    "                variance_matrix[i, j] = results[rho][kappa][tau]['variance']\n",
    "    \n",
    "    # we create heatmaps\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # we plot optimal k heatmap\n",
    "    im1 = axes[0].imshow(optimal_k_matrix, aspect='auto', origin='lower', cmap='viridis', interpolation='nearest')\n",
    "    axes[0].set_xlabel('κ (kappa)')\n",
    "    axes[0].set_ylabel('τ (tau)')\n",
    "    axes[0].set_title(f'Optimal k as function of (τ, κ) - rho={rho:.2f}')\n",
    "    axes[0].set_xticks(np.arange(len(KAPPA_VALUES))[::2])\n",
    "    axes[0].set_xticklabels([f'{k:.2f}' for k in KAPPA_VALUES[::2]])\n",
    "    axes[0].set_yticks(np.arange(len(all_taus_rho))[::max(1, len(all_taus_rho)//10)])\n",
    "    axes[0].set_yticklabels([f'{t:.2f}' for t in all_taus_rho[::max(1, len(all_taus_rho)//10)]])\n",
    "    plt.colorbar(im1, ax=axes[0], label='Optimal k')\n",
    "    \n",
    "    # we plot variance heatmap\n",
    "    variance_log = np.log10(variance_matrix + 1e-10)\n",
    "    im2 = axes[1].imshow(variance_log, aspect='auto', origin='lower', cmap='plasma', interpolation='nearest')\n",
    "    axes[1].set_xlabel('κ (kappa)')\n",
    "    axes[1].set_ylabel('τ (tau)')\n",
    "    axes[1].set_title(f'log10(Variance) as function of (τ, κ) - rho={rho:.2f}')\n",
    "    axes[1].set_xticks(np.arange(len(KAPPA_VALUES))[::2])\n",
    "    axes[1].set_xticklabels([f'{k:.2f}' for k in KAPPA_VALUES[::2]])\n",
    "    axes[1].set_yticks(np.arange(len(all_taus_rho))[::max(1, len(all_taus_rho)//10)])\n",
    "    axes[1].set_yticklabels([f'{t:.2f}' for t in all_taus_rho[::max(1, len(all_taus_rho)//10)]])\n",
    "    plt.colorbar(im2, ax=axes[1], label='log10(Variance)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    rho_str = f\"{rho:.2f}\".replace('.', 'p').replace('-', 'm')\n",
    "    heatmap_path = BASE_DIR / f\"heatmaps_rho{rho_str}.png\"\n",
    "    print(f\"  [STEP 4] Saving heatmap to {heatmap_path}...\")\n",
    "    plt.savefig(heatmap_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"  [STEP 4] Heatmap saved successfully to {heatmap_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7078892",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Summary Statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: Summary Statistics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for rho_idx, rho in enumerate(RHO_VALUES):\n",
    "    if rho not in results or len(results[rho]) == 0:\n",
    "        print(f\"\\n[STEP 5] No results for rho={rho}, skipping summary.\")\n",
    "        continue\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[STEP 5] Summary for rho {rho_idx+1}/{len(RHO_VALUES)}: rho = {rho}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    total_configs = 0\n",
    "    for kappa in results[rho].keys():\n",
    "        total_configs += len(results[rho][kappa])\n",
    "    print(f\"[STEP 5] Total valid (tau, kappa) configurations: {total_configs}\")\n",
    "    print(f\"[STEP 5] Number of unique kappas: {len(results[rho].keys())}\")\n",
    "    \n",
    "    # we compute average optimal k and variance across all configurations\n",
    "    print(f\"[STEP 5] Computing summary statistics...\")\n",
    "    all_optimal_k = []\n",
    "    all_variance = []\n",
    "    all_mean_alignment = []\n",
    "    for kappa in results[rho].keys():\n",
    "        for tau in results[rho][kappa].keys():\n",
    "            all_optimal_k.append(results[rho][kappa][tau]['optimal_k'])\n",
    "            all_variance.append(results[rho][kappa][tau]['variance'])\n",
    "            all_mean_alignment.append(results[rho][kappa][tau]['mean_alignment'])\n",
    "    \n",
    "    if len(all_optimal_k) > 0:\n",
    "        print(f\"[STEP 5] Optimal k statistics:\")\n",
    "        print(f\"  Mean: {np.mean(all_optimal_k):.2f}\")\n",
    "        print(f\"  Std: {np.std(all_optimal_k):.2f}\")\n",
    "        print(f\"  Min: {np.min(all_optimal_k)}\")\n",
    "        print(f\"  Max: {np.max(all_optimal_k)}\")\n",
    "        print(f\"[STEP 5] Variance statistics (log10):\")\n",
    "        print(f\"  Mean: {np.mean(np.log10(all_variance)):.4f}\")\n",
    "        print(f\"  Std: {np.std(np.log10(all_variance)):.4f}\")\n",
    "        print(f\"  Min: {np.min(np.log10(all_variance)):.4f}\")\n",
    "        print(f\"  Max: {np.max(np.log10(all_variance)):.4f}\")\n",
    "        print(f\"[STEP 5] Mean alignment statistics:\")\n",
    "        print(f\"  Mean: {np.mean(all_mean_alignment):.4f}\")\n",
    "        print(f\"  Std: {np.std(all_mean_alignment):.4f}\")\n",
    "        print(f\"  Min: {np.min(all_mean_alignment):.4f}\")\n",
    "        print(f\"  Max: {np.max(all_mean_alignment):.4f}\")\n",
    "    else:\n",
    "        print(f\"[STEP 5] WARNING: No valid configurations found!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"=\" * 80)\n",
    "print(\"ALL SIMULATIONS AND PLOTS COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"=\" * 80)\n",
    "print(f\"Summary:\")\n",
    "print(f\"  - Processed {len(RHO_VALUES)} rho values: {RHO_VALUES}\")\n",
    "print(f\"  - Processed {len(KAPPA_VALUES)} kappa values\")\n",
    "print(f\"  - Generated {len(TAU_VALUES)} unique tau values\")\n",
    "print(f\"  - Monte Carlo replications: {N_MC}\")\n",
    "print(f\"  - Sample size: {N_SAMPLES}\")\n",
    "print(f\"  - Dimension: {D}\")\n",
    "print(f\"  - Fixed parameters: gamma={GAMMA}, q={Q}\")\n",
    "print(f\"  - Results saved to: {BASE_DIR}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
