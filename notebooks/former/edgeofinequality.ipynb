{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bda7509",
   "metadata": {
    "id": "_boUx_3FvO4O"
   },
   "outputs": [],
   "source": [
    "###################### Magic commands\n",
    "\n",
    "##%%timeit -n 1 -r 1\n",
    "#%%time\n",
    "import time\n",
    "#start = time.time()\n",
    "\n",
    "###################### at the end\n",
    "\n",
    "#end = time.time()\n",
    "#print(end - start)\n",
    "\n",
    "###################### Packages\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "#import Pyarrow # for pandas\n",
    "import scipy.special\n",
    "import math \n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "from fctpls.utils import stooq_txt_to_df, stooq_to_notebook_format\n",
    "\n",
    "# we load stooq data (with\n",
    "# pip install fbm\n",
    "# from fbm import FBM\n",
    "\n",
    "###################### Options\n",
    "\n",
    "npr.seed(0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#<\n",
    "#>\n",
    "\n",
    "############### Tools transfering numpy to numba\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def np_apply_along_axis_2darray(func1d, axis, arr):\n",
    "  assert arr.ndim == 2\n",
    "  assert axis in [0, 1]\n",
    "  if axis == 0:\n",
    "    result = np.empty(arr.shape[1])\n",
    "    for i in numba.prange(len(result)):\n",
    "      result[i] = func1d(arr[:, i])\n",
    "  else:\n",
    "    result = np.empty(arr.shape[0])\n",
    "    for i in numba.prange(len(result)):\n",
    "      result[i] = func1d(arr[i, :])\n",
    "  return result\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def np_mean_2darray(array, axis):\n",
    "  return np_apply_along_axis_2darray(np.mean, axis, array)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def np_std_2darray(array, axis):\n",
    "  return np_apply_along_axis_2darray(np.std, axis, array)\n",
    "\n",
    "################################################## Non-empty\n",
    "\n",
    "def check_cond(gamma,c,tau,q):\n",
    "    if 0<tau:\n",
    "        if gamma<1 and 0<gamma and 2<q and 0<c and 2*(c+tau)*gamma<1 and q*(1-2*tau*gamma)>2 and q*c*gamma>1 and tau <1/(2*gamma):\n",
    "            print('Valid!')\n",
    "        else:\n",
    "            print('Not valid!')\n",
    "    else:\n",
    "        if gamma<1 and 0<gamma and 2<q and 0<c and 2*(c+tau)*gamma<1 and q*c*gamma>1:\n",
    "            print('Valid!')\n",
    "        else:\n",
    "            print('Not valid!')        \n",
    "    return\n",
    "\n",
    "def check_cond_no_q(gamma,c,tau):\n",
    "    if gamma<1 and 0<gamma and 0<c and 2*(c+tau)*gamma<1:\n",
    "        print('Valid!')\n",
    "    else:\n",
    "        print('Not valid!')   \n",
    "    return\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=True)\n",
    "def find_cond_all(h_gamma,h_c,h_tau,h_q,max_c,max_tau,max_q):\n",
    "    gamma_mesh = np.linspace(0.01,0.99,h_gamma)\n",
    "    c_mesh = np.linspace(0.1,max_c,h_c)\n",
    "    tau_mesh = np.linspace(-max_tau,max_tau,h_tau)\n",
    "    q_mesh = np.linspace(2.1,max_q,h_q)\n",
    "    for gamma_index in numba.prange(h_gamma):\n",
    "        for c_index in numba.prange(h_c): \n",
    "            for tau_index in numba.prange(h_tau):\n",
    "                for q_index in numba.prange(h_q):\n",
    "                    gamma=gamma_mesh[gamma_index]\n",
    "                    c=c_mesh[c_index]\n",
    "                    tau=tau_mesh[tau_index]\n",
    "                    q=q_mesh[q_index]\n",
    "                    if 0<tau:\n",
    "                        if 2<q and 0<c and 2*(c+tau)*gamma<1 and q*(1-2*tau*gamma)>2 and q*c*gamma>1:\n",
    "                            print([gamma, c, tau, q])\n",
    "                    else:\n",
    "                        if 2<q and 0<c and 2*(c+tau)*gamma<1 and q*c*gamma>1:\n",
    "                            print([gamma, c, tau, q])                        \n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_cond_no_q(grid_gamma,max_tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    c_mesh = np.arange(1,10)\n",
    "    tau_mesh = np.arange(-max_tau,max_tau+1)\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        for c_index in numba.prange(c_mesh.shape[0]): \n",
    "            for tau_index in numba.prange(tau_mesh.shape[0]):\n",
    "                gamma=gamma_mesh[gamma_index]\n",
    "                c=c_mesh[c_index]\n",
    "                tau=tau_mesh[tau_index]\n",
    "                if 2*(c+tau)*gamma<1:\n",
    "                    print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_gamma_no_q(grid_gamma,c,tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        gamma=gamma_mesh[gamma_index]\n",
    "        if 2*(c+tau)*gamma<1:\n",
    "            print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_cond_c_no_q(c,grid_gamma,max_tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    tau_mesh = np.arange(-max_tau,max_tau+1)\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        for tau_index in numba.prange(tau_mesh.shape[0]):\n",
    "            gamma=gamma_mesh[gamma_index]\n",
    "            tau=tau_mesh[tau_index]\n",
    "            if 2*(c+tau)*gamma<1:\n",
    "                print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def find_cond_c_tau_no_q(c,tau,grid_gamma,max_tau):\n",
    "    gamma_mesh = np.linspace(0,1,grid_gamma+1)[1:grid_gamma]\n",
    "    tau_mesh = np.arange(-max_tau,max_tau+1)\n",
    "    for gamma_index in numba.prange(gamma_mesh.shape[0]):\n",
    "        for tau_index in numba.prange(tau_mesh.shape[0]):\n",
    "            gamma=gamma_mesh[gamma_index]\n",
    "            tau=tau_mesh[tau_index]\n",
    "            if 2*(c+tau)*gamma<1:\n",
    "                print([gamma, c, tau])\n",
    "    return 'Done!'\n",
    "\n",
    "################################################## Generating Data\n",
    "\n",
    "@numba.njit(parallel=False, fastmath=False)\n",
    "def Lomax_quantile_function(x,theta,s):  \n",
    "    return s*((1-x)**(-1/theta)-1)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def Pareto_quantile_function(x,gamma,s): # support is  {x \\ge s} and theta=1/\\gamma\n",
    "    return s*(1-x)**(-gamma)\n",
    "\n",
    "#The Burr distribution has survival distribution $\\bar{F}(y)=(1+y^\\rho)^{-\\theta} \\in 2\\RV_{-\\theta \\rho,-\\rho}$ where $x\\ge 0$ and $\\theta,\\rho>0$.\n",
    "@numba.njit(parallel=True, fastmath=False) # rho,theta positive\n",
    "def Burr_quantile_function(x,theta,rho): \n",
    "    return ((1-x)**(-1/theta)-1)**(1/rho)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func3(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.exp(-grid**2+grid)**2)/d)\n",
    "    return np.exp(-grid**2+grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func2(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.exp(-grid)**2)/d)\n",
    "    return np.exp(-grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.sin(2*np.pi*grid)**2)/d)\n",
    "    return np.sin(2*np.pi*grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func5(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.sin(1/(1/12+grid**2))/d))\n",
    "    return np.sin(1/(1/12+grid**2))/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func6(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.cos(2*np.pi*grid+np.sin(1/(grid+1/10)))/d))\n",
    "    return np.cos(2*np.pi*grid+np.sin(1/(grid+1/10)))/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func7(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.cos(2*np.pi*grid)**2)/d)\n",
    "    return np.cos(2*np.pi*grid)/norm\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def beta_func4(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.sin(2*np.pi*grid)**2)/d)\n",
    "    return np.sin(2*np.pi*grid)/norm\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_1D(d,H,sigma):\n",
    "    H2 = 2 * H\n",
    "    matcov = np.zeros((d-1,d-1))\n",
    "    for i in numba.prange(d-1):\n",
    "        for j in numba.prange(i,d-1):\n",
    "            r = (sigma**2)*(1/2)*(abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "            r = r/(d**H2)\n",
    "            matcov[i, j] = r\n",
    "            matcov[j, i] = matcov[i, j]\n",
    "    L = np.linalg.cholesky(matcov)\n",
    "    Z = npr.normal(0,1,size=(d - 1))\n",
    "    fBm = np.dot(L , Z)\n",
    "    #out=np.concatenate(([0], fBm))\n",
    "    # out=np.hstack(([0], fBm))\n",
    "    out= np.asarray([0] + list(fBm))\n",
    "    return out\n",
    "\n",
    "# @Article{RePEc:jss:jstsof:v:005:i07,\n",
    "#  author={Coeurjolly, Jean-Francois},\n",
    "#  title={{Simulation and identification of the fractional Brownian motion: a bibliographical and comparative study}},\n",
    "#  journal={Journal of Statistical Software},\n",
    "#  year=2000,\n",
    "#  volume={5},\n",
    "#  number={i07},\n",
    "#  pages={},\n",
    "#  month={},\n",
    "#  keywords={},\n",
    "#  doi={http://hdl.handle.net/10.18637/jss.v005.i07}\n",
    "#}\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_array(Z,H,sigma): # Z=npr.normal(0,1,size=(N,n,d - 1))\n",
    "    N=Z.shape[0]\n",
    "    n=Z.shape[1]\n",
    "    d=Z.shape[2]+1\n",
    "    out = np.zeros((N,n,d))\n",
    "    for p in numba.prange(N):\n",
    "        for q in numba.prange(n):    \n",
    "            H2 = 2 * H\n",
    "            matcov = np.zeros((d-1,d-1))\n",
    "            for i in numba.prange(d-1):\n",
    "                for j in numba.prange(i,d-1):\n",
    "                    r = (sigma**2)*(1/2)*(abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "                    r = r/(d**H2)\n",
    "                    matcov[i, j] = r\n",
    "                    matcov[j, i] = matcov[i, j]\n",
    "            L = np.linalg.cholesky(matcov)\n",
    "            fBm = np.dot(L , Z[p,q,:])\n",
    "            #out=np.concatenate(([0], fBm))\n",
    "            # out=np.hstack(([0], fBm))\n",
    "            out[p,q,:]= np.asarray([0] + list(fBm))\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def sigma(u,c,snr): \n",
    "    return (u**c)/snr\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def noise_mean(d,mu):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    return mu*grid\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu): #Z = npr.normal(0,1,size=(N,n,d - 1))  \n",
    "                                     #Y = Pareto_iterated_sample(N,n,theta,s)\n",
    "    N=Y.shape[0]\n",
    "    n=Y.shape[1]\n",
    "    d=Z.shape[2]+1\n",
    "    out = np.zeros((N,n,d))\n",
    "    H2 = 2 * H\n",
    "    matcov = np.zeros((d-1,d-1))\n",
    "    for p in numba.prange(N):\n",
    "        for q in numba.prange(n):\n",
    "            matcov = np.zeros((d-1,d-1))\n",
    "            for i in numba.prange(d-1):\n",
    "                for j in numba.prange(i,d-1):\n",
    "                    r = (sigma(Y[p,q],c,snr)**2)*(1/2)*(abs(i+1)**H2 + abs(j+1)**H2 - abs(j - i)**H2)\n",
    "                    r = r/(d**H2)\n",
    "                    matcov[i, j] = r\n",
    "                    matcov[j, i] = matcov[i, j]\n",
    "            L = np.linalg.cholesky(matcov)\n",
    "            fBm = np.dot(L , Z[p,q,:])\n",
    "            out[p,q,:]= np.asarray([0]+list(fBm)) + noise_mean(d,mu)\n",
    "    return out\n",
    "\n",
    "################################################## Estimation\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def esdf(data,x): # Empirical Survival Distribution Function of Y. Here, x.shape = (N,) or (N,n) is the threshold\n",
    "    # data is Y and data.shape = (N,n)\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    if x.ndim == 1:\n",
    "        indicator_matrix = np.transpose(np.where(np.transpose(data)>x,1,0)) # binary matrix with same shape as data representating the indicator matrix 1_{X_ij<x_j}\n",
    "    if x.ndim == 2:\n",
    "        indicator_matrix = np.where(data>x,1,0)\n",
    "    return np.sum(indicator_matrix,axis=1)/n # (N,)\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def threshold_index(X,Y,Y_sort_index,tau,m,start): # 0\\le start \\le n-1\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    out=np.zeros((N,))\n",
    "    aux=concomittant_corr(X,Y,Y_sort_index,tau,m)[:,start:]\n",
    "    return start+np.argmax(aux,axis=1)\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def threshold(X,Y,Y_sort_index,tau,m,start): # 0\\le start \\le n-1\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    y_matrix_out = np.zeros((N,n))\n",
    "    YY=np.copy(Y)\n",
    "    Y_sort=sort_2d_array(YY)\n",
    "    index = threshold_index(X,Y,Y_sort_index,tau,m,start)\n",
    "    for i in numba.prange(N):\n",
    "        y_matrix_out[i,:] = Y_sort[i,n-index[i]-1]*np.ones((n,))\n",
    "    return y_matrix_out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def fepls_weight(Y,y_matrix,nu): # Y of size (N,n)\n",
    "                             # y_matrix of shape (N,n), for instance y_matrix = y*np.ones((N,n)) where y threshold\n",
    "                             # nu is such that \\psi(x) = x^nu\n",
    "    N=Y.shape[0]\n",
    "    n=Y.shape[1]\n",
    "    out=np.zeros((N,))\n",
    "    aux = Y**nu # size (N,n) - \\psi(Y_i)\n",
    "    aux2 = np.multiply(aux,np.greater_equal(Y,y_matrix)) # size (N,n) - Product \\psi(Y_i)*1_{Y_i \\ge y}\n",
    "    return np.sum(aux2,axis=1)/n # size (N,)\n",
    "\n",
    "#@numba.njit(parallel=True, fastmath=False) # It seems that \"greater_equal\" and numba don't work well together\n",
    "def fepls(X,Y,y_matrix,tau): # X of size (N,n,d) and Y of size (N,n)\n",
    "                             # y_matrix of shape (N,n), for instance y_matrix = y*np.ones((N,n)) where y threshold\n",
    "                             # tau is the tail index of \\vfi\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    out=np.zeros((N,d))\n",
    "    for j in range(d):\n",
    "        aux = np.multiply(X[:,:,j],Y**tau) # size (N,n,d) - Product \\vfi(Y_i)*X_i\n",
    "        out2 = np.multiply(aux,np.greater_equal(Y,y_matrix)) # size (N,n) - Product \\vfi(Y_i)*X_i*1_{Y_i \\ge y}\n",
    "        out[:,j]= np.sum(out2,axis=1)/n # (N,d)\n",
    "    norms=np.sqrt(np.sum(out**2,axis=1)/d) # length (N,)\n",
    "    out2 =  out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2 # size (N,d)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def fepls_numba(X,Y,y_matrix,tau): # X of size (N,n,d) and Y of size (N,n)\n",
    "                             # y_matrix of shape (N,n), for instance y_matrix = y*np.ones((N,n)) where y threshold\n",
    "                             # tau is the tail index of \\vfi\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    out=np.zeros((N,d))\n",
    "    for j in numba.prange(d):\n",
    "        aux = np.multiply(X[:,:,j],Y**tau) # size (N,n,d) - Product \\vfi(Y_i)*X_i\n",
    "        out2 = np.multiply(aux,np.greater_equal(Y,y_matrix)) # size (N,n) - Product \\vfi(Y_i)*X_i*1_{Y_i \\ge y}\n",
    "        out[:,j]= np.sum(out2,axis=1)/n # (N,d)\n",
    "    norms=np.sqrt(np.sum(out**2,axis=1)/d) # length (N,)\n",
    "    out2 =  out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2 # size (N,d)\n",
    "\n",
    "# g(t)= t^c with c \\in \\{1/4,1/2,1,3/2\\} \n",
    "# X = g(Y)\\beta + \\eps\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def Hill(Y,int_seq): # Y of size (N,n), y number, int_seq is an intermediate sequence, ie such that int_seq << n\n",
    "    N=Y.shape[0]\n",
    "    n=Y.shape[1]\n",
    "    Y_ord=np.copy(Y)\n",
    "    Y_ord=np.sort(Y_ord) \n",
    "    Y_2=Y_ord[:,n-int_seq-1]\n",
    "    aux=Y_ord/Y_2[:, None]\n",
    "    out=np.log(aux[:,0:n-int_seq])\n",
    "    return (1/int_seq)*np.sum(out,axis=1) # size (N,)\n",
    "\n",
    "#### Same as np.sort for 2D arrays but works with numba njit+parallel\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def sort_2d_array(x):\n",
    "    n,m=np.shape(x)\n",
    "    for row in numba.prange(n):\n",
    "        x[row]=np.sort(x[row])\n",
    "    return x\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def hatbeta_dot_beta(X,Y,tau,l):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    y_array=np.zeros((N,n,np.arange(int(n/l)).size))\n",
    "    out=np.zeros((N,np.arange(int(n/l)).size))\n",
    "    YY=np.copy(Y)\n",
    "    for p in numba.prange(int(n/l)):\n",
    "        y_array[:,0,p]=sort_2d_array(YY)[:,n-l*p-1]\n",
    "        for k in numba.prange(N):\n",
    "            y_array[k,:,p]=y_array[k,0,p]\n",
    "        hat_beta=fepls_numba(X,Y,y_array[:,:,p],tau) \n",
    "        out[:,p]=(1/d)*np.sum(np.multiply(hat_beta,X[:,p,:]),axis=1) \n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def concomittant_corr(X,Y,Y_sort_index,tau,m): # 1\\le m \\le n # Y_sort_index = np.argsort(Y,axis=1)\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((N,m))\n",
    "    YY=np.copy(Y)\n",
    "    Y_sort=sort_2d_array(YY)\n",
    "    for k in numba.prange(m):\n",
    "        y_array = np.zeros((N,n,k+1))\n",
    "        aux = np.zeros((N,k+1))\n",
    "        aux2 = np.zeros((N,k+1))\n",
    "        aux3 = Y_sort[:,n-k-1:] # shape (N,k+1)\n",
    "        aux3_sum = np.sum(aux3,axis=1)\n",
    "        for i in numba.prange(k):\n",
    "            y_array[:,0,i] = Y_sort[:,n-i-1]\n",
    "            for j_2 in numba.prange(N):\n",
    "                y_array[j_2,:,i] = y_array[j_2,0,i]\n",
    "            hat_beta = fepls_numba(X,Y,y_array[:,:,i],tau) \n",
    "            for j_1 in numba.prange(N):\n",
    "                i_c = Y_sort_index[j_1,i]\n",
    "                aux[j_1,i]=(1/d)*np.sum(np.multiply(hat_beta[j_1,:],X[j_1,i_c,:]))\n",
    "                aux2[j_1,i]= np.multiply(aux[j_1,i],Y_sort[j_1,n-i-1]) \n",
    "                out[j_1,k]= np.corrcoef(aux3[j_1,:],aux[j_1,:])[0,1]\n",
    "    return out\n",
    "    \n",
    "def bitcoin_concomittant_corr(X,Y,tau,m): # 1\\le m \\le n \n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((m))\n",
    "    Y_sort=np.sort(Y,axis=1)\n",
    "    Y_sort_index = np.argsort(Y,axis=1)\n",
    "    for k in range(m):\n",
    "        y_array = np.zeros((N,n,k+1))\n",
    "        aux = np.zeros((k+1))\n",
    "        aux2 = np.zeros((k+1))\n",
    "        aux3 = Y_sort[0,n-k-1:] # shape (k+1)\n",
    "        aux3_sum = np.sum(aux3)\n",
    "        for i in range(k):\n",
    "            y_array[:,:,i] = (Y_sort[0,n-i-1])*np.ones((1,n))\n",
    "            hat_beta = fepls(X,Y,y_array[:,:,i],tau) \n",
    "            i_c = Y_sort_index[0,i]\n",
    "            aux[i]=(1/d)*np.sum(np.multiply(hat_beta[0,:],X[0,i_c,:]))\n",
    "            aux2[i]= np.multiply(aux[i],Y_sort[0,n-i-1]) \n",
    "            out[k]= np.corrcoef(aux3,aux)[0,1]\n",
    "    return np.abs(out)\n",
    "\n",
    "def bitcoin_threshold_index(X,Y,tau,m,start): # 0\\le start \\le n-1\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    out = np.zeros((N,n))\n",
    "    return start+np.argmax(bitcoin_concomittant_corr(X,Y,tau,int(n/5))[start:])\n",
    "    \n",
    "def bitcoin_threshold(X,Y,tau,m,start):# 0\\le start \\le n-1\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    out = np.zeros((N,n))\n",
    "    Y_sort_index = np.argsort(Y,axis=1)\n",
    "    Y_sort=np.sort(Y,axis=1)\n",
    "    index = bitcoin_threshold_index(X,Y,tau,m,start)\n",
    "    out[0,:] = Y_sort[0,n-index-1]\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def hatbeta_dot_X(X,hat_beta): # hat_beta=fepls(X,Y,y_matrix,tau) of shape (N,d)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    out=np.zeros((N,n))\n",
    "    for i in numba.prange(n):\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(hat_beta,X[:,i,:]),axis=1) \n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) # the same as np.dot(X,beta_func(d))/d (which is preferable)\n",
    "def beta_dot_X(X):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    out=np.zeros((N,n))\n",
    "    for i in numba.prange(n):\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(X[:,i,:],beta_func(d)),axis=1) \n",
    "    return out\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False) # np.dot(X,beta_func(d))/d\n",
    "def beta_dot_X(X,beta_param):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    out=np.zeros((N,n))\n",
    "    for i in numba.prange(n):\n",
    "        if beta_param == 1:\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(X[:,i,:],beta_func(d)),axis=1)\n",
    "        elif beta_param == 2:\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(X[:,i,:],beta_func2(d)),axis=1)\n",
    "        elif beta_param == 3:\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(X[:,i,:],beta_func3(d)),axis=1)\n",
    "        elif beta_param == 4:\n",
    "            out[:,i]=(1/d)*np.sum(np.multiply(X[:,i,:],beta_func4(d)),axis=1)\n",
    "    return out\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def conditional_cov_Y_hat_beta_X(X,Y,hat_beta,y_matrix,tau): #hat_beta = fepls(X,Y,y_matrix,tau)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    A=hatbeta_dot_X(X,hat_beta)\n",
    "    cov_A=np.zeros((N,))\n",
    "    cov_B=np.zeros((N,))\n",
    "    for k in numba.prange(N):\n",
    "        cov_A[k] = (((A[k,:])[Y[k,:]>y_matrix[k,:]])*((Y[k,:])[Y[k,:]>y_matrix[k,:]])).mean() - ((A[k,:])[Y[k,:]>y_matrix[k,:]]).mean()*((Y[k,:])[Y[k,:]>y_matrix[k,:]]).mean()\n",
    "    return cov_A\n",
    "\n",
    "#@numba.njit(parallel=True, fastmath=False) # does not work with numba (no idea why)\n",
    "def conditional_cov_Y_beta_X(X,Y,y):\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]    \n",
    "    B=beta_dot_X(X)\n",
    "    cov_B=np.zeros((N,))\n",
    "    for k in numba.prange(N):\n",
    "        cov_B[k] = (((B[k,:])[Y[k,:]>y])*((Y[k,:])[Y[k,:]>y])).mean()-((B[k,:])[Y[k,:]>y]).mean()*((Y[k,:])[Y[k,:]>y]).mean()\n",
    "    return cov_B\n",
    "\n",
    "def Exponential_QQ_Plot_1D(Y,k):\n",
    "    n=Y.shape[1]\n",
    "    out=np.zeros((k))\n",
    "    out2=np.zeros((k))\n",
    "    YY=np.sort(Y,axis=1)\n",
    "    for i in range(k):\n",
    "        out[i]=np.log((k+1)/(i+1))\n",
    "        out2[i]=  np.log(YY[0,n-i-1])-np.log(YY[0,-k])\n",
    "    return np.column_stack((out,out2))\n",
    "\n",
    "################################################## Conditional quantile estimation (2D+3D array)\n",
    "################# Application: Bitcoin/SP500 quantile\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def Epanechnikov_kernel_2D(x): # x is a np.array of shape (p,q);     W=np.where(np.abs(x)<=1,1,0)\n",
    "    out = np.zeros_like(x)\n",
    "    x=np.asarray(x)\n",
    "    for i in numba.prange(x.shape[0]):\n",
    "        for j in numba.prange(x.shape[1]):\n",
    "            if x[i,j]<=1 and x[i,j]>=0:\n",
    "                out[i,j] = np.multiply(3/2,1-np.power(x[i,j],2))\n",
    "            else:\n",
    "                out[i,j]=0\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def Epanechnikov_kernel_1D(x): # x is a np.array of shape (p);     W=np.where(np.abs(x)<=1,1,0)\n",
    "    out = np.zeros_like(x)\n",
    "    x=np.asarray(x)\n",
    "    for i in numba.prange(x.shape[0]):\n",
    "        if x[i]<=1 and x[i]>=0:\n",
    "            out[i] = np.multiply(3/2,1-np.power(x[i],2))\n",
    "        else:\n",
    "            out[i]=0\n",
    "    return out\n",
    "    \n",
    "@numba.njit(parallel=False, fastmath=False) \n",
    "def Gaussian_kernel(x):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*np.power(x,2))\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def univariate_Nadaraya_weight(X_dimred,x,h,kernel): # X_dimred of shape (N,n) just as Y \n",
    "    # e.g., X_dimred = hatbeta_dot_X(X,hat_beta) = F0\n",
    "    # hat_beta=fepls(X,Y,y_matrix,tau)=E0; x real and h positive\n",
    "    N=X_dimred.shape[0]\n",
    "    n=X_dimred.shape[1]\n",
    "    out=np.zeros((N,n))\n",
    "    if kernel == 1:\n",
    "        K_h=Epanechnikov_kernel_2D((X_dimred-x)/h) # shape (N,n)\n",
    "    if kernel == 2:\n",
    "        K_h=Gaussian_kernel((X_dimred-x)/h) # shape (N,n)\n",
    "    return K_h/np.sum(K_h) ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def plot_quantile_test_2D(X,Y,X_dimred,inner_prod,x_func,alpha,a,b,m,h,h_func,kernel): # X_dimred.shape = Y.shape = (N,n), e.g., X_dimred = hatbeta_dot_X(X,hat_beta) = F0\n",
    "    # x_func = any vector in H, eg beta_func(d) or E0[0,:]; h_func is positive\n",
    "    # x_func_dot_beta.shape in \\R; it represents the inner product between x_func (where we project for the plot) and the vector in H reducing the dimension\n",
    "    # inner_prod.shape \\in\\R represents (x_func,beta)\n",
    "    #  beta is reducing the dimension, i.e., X_dimred=(X,beta).\n",
    "    # X_dimred = (X,beta); Y|X_dimred = x vs Y|X=x*x_func  \n",
    "    out = np.zeros((m,2))\n",
    "    x_grid = np.linspace(a,b,m)\n",
    "    for p in numba.prange(m):\n",
    "        weight_func=functional_Nadaraya_weight_2D(X[0,:,:],x_grid[p]*x_func,h_func,kernel) \n",
    "        weight1=univariate_Nadaraya_weight(X_dimred,x_grid[p]*inner_prod,h,kernel)[0,:]  \n",
    "        out[p,0]=weighted_quantile(Y[0,:],weight1,alpha)\n",
    "        out[p,1]=weighted_quantile(Y[0,:],weight_func,alpha)\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def univariate_Nadaraya_weight_2D(X_2D,dimred,x_func,x,h,type,kernel): # X of shape (n,d), Y of shape (n), dimred and x_func of shape (d,)\n",
    "    # dimred of shape (d,) is e.g. beta_func(d) or (fepls(X,Y,y_matrix,tau))[0,:]\n",
    "    # Y|(X,dimred)=(x_func,dimred) (type 1) vs Y|(X,dimred) = x (type 2)\n",
    "    d=x_func.shape[0]\n",
    "    if type == 1:\n",
    "        if kernel == 1:\n",
    "            K_h=Epanechnikov_kernel_1D((np.dot(X_2D,dimred)/d-np.dot(x_func,dimred)/d)/h) # shape (n,)\n",
    "        if kernel == 2:\n",
    "            K_h=Gaussian_kernel((np.dot(X_2D,dimred)/d-np.dot(x_func,dimred)/d)/h) # shape (n,)\n",
    "    if type == 2:     \n",
    "        if kernel == 1:\n",
    "            K_h=Epanechnikov_kernel_1D((np.dot(X_2D,dimred)/d-x)/h) # shape (n,)\n",
    "        if kernel == 2:\n",
    "            K_h=Gaussian_kernel((np.dot(X_2D,dimred)/d-x)/h) # shape (n,)\n",
    "    return K_h/np.sum(K_h) ### shape = (,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def functional_Nadaraya_weight_2D(X_2D,x_func,h,kernel): # X.shape = (n,d); h is positive ; x_func \\in H ie of shape (d,)\n",
    "    d=x_func.shape[0]\n",
    "    aux = (X_2D-x_func*np.ones(d))**2 # shape = (n,d)\n",
    "    norm = np.sqrt((1/d)*np.sum(aux,axis=1)) # shape = (n,)\n",
    "    if kernel == 1:\n",
    "        K_h= Epanechnikov_kernel_1D(norm/h) # shape = (n,)\n",
    "    if kernel == 2:\n",
    "        K_h= Gaussian_kernel(norm/h)## shape = (n,)\n",
    "    return K_h/np.sum(K_h) ### shape = (n,)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def univariate_Nadaraya_weight_3D(X,dimred_2D,x_func,x,h,type,kernel): # X of shape (N,n,d), Y of shape (N,n), dimred_2D of shape (N,d)\n",
    "    # x_func of shape (d,) is e.g. beta_func(d) or (fepls(X,Y,y_matrix,tau))[0,:]\n",
    "    # x is a real number (e.g., x=x_grid[p] for p \\leq m)\n",
    "    # Y|(X,dimred)=(x_func,dimred) (type 1) vs Y|(X,dimred) = x (type 2)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    K_h = np.zeros((N,n))\n",
    "    for i in numba.prange(N):\n",
    "        if type == 1:\n",
    "            if kernel == 1:\n",
    "                K_h[i,:]=Epanechnikov_kernel_1D((np.dot(X[i,:,:],dimred_2D[i,:])/d-np.dot(x_func,dimred_2D[i,:])/d)/h) # shape (n,)\n",
    "            if kernel == 2:\n",
    "                K_h[i,:]=Gaussian_kernel((np.dot(X[i,:,:],dimred_2D[i,:])/d-np.dot(x_func,dimred_2D[i,:])/d)/h) # shape (n,)\n",
    "        if type == 2:\n",
    "            if kernel == 1:\n",
    "                K_h[i,:]=Epanechnikov_kernel_1D((np.dot(X[i,:,:],dimred_2D[i,:])/d-x)/h) # shape (n,)   \n",
    "            if kernel == 2:\n",
    "                K_h[i,:]=Gaussian_kernel((np.dot(X[i,:,:],dimred_2D[i,:])/d-x)/h) # shape (n,)\n",
    "    return np.transpose(np.transpose(K_h)/np.sum(K_h,axis=1)) ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def functional_Nadaraya_weight_3D(X,x_func,h_func_2D,kernel): # X.shape = (N,n,d); h_func_2D.shape = (N,n); x_func \\in H ie of shape (d,)\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    aux = np.zeros((N,n,d))\n",
    "    for i in numba.prange(N):\n",
    "        aux[i,:,:] = (X[i,:,:]-x_func*np.ones(d))**2\n",
    "    norm = np.sqrt((1/d)*np.sum(aux,axis=2))\n",
    "    if kernel == 1:\n",
    "        K_h= Epanechnikov_kernel_2D(norm/h_func_2D)\n",
    "    if kernel == 2:\n",
    "        K_h= Gaussian_kernel(norm/h_func_2D)      \n",
    "    return np.transpose(np.transpose(K_h)/np.sum(K_h,axis=1)) ### shape = (N,n)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)   \n",
    "def weighted_quantile(data,weight,alpha):  # data.shape=weight.shape=(n,) \n",
    "    # alpha is the treshold in (0,1)\n",
    "    sorter = np.argsort(data)\n",
    "    data = data[sorter]\n",
    "    weight = weight[sorter]\n",
    "    weighted_quantiles = np.cumsum(weight) - 0.5 * weight\n",
    "    weighted_quantiles /= np.sum(weight)\n",
    "    return np.interp(alpha, weighted_quantiles, data)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def iterated_weq_2D(data_2D,weight,alpha): # same treshold for all marginals and iterations \n",
    "    # data.shape = (N,n); weight.shape=(N,n)\n",
    "    N=data_2D.shape[0]\n",
    "    n=data_2D.shape[1]\n",
    "    out = np.zeros((N,))\n",
    "    for k in numba.prange(N):\n",
    "        out[k]=weighted_quantile(data_2D[k,:],weight[k,:],alpha)\n",
    "    return out # shape = (N)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def plot_quantile_dimred_vs_func_2D(X,Y,dimred1,dimred2,x_func,alpha,a,b,m,h,h_func,kernel): # dimred1,2 and x_func of shape (d,), e.g., dimred = E0[0,:] with E0 = fepls(X,Y,y_matrix,tau)\n",
    "    # x_func = any vector in H, eg beta_func(d) or E0[0,:]\n",
    "   # Y|(X,dimred1)=(x*x_func,dimred1) (type 1) vs Y|(X,dimred2) = x (type 2) vs Y|X = x*x_func (type functional)\n",
    "    out = np.zeros((m,3))\n",
    "    x_grid = np.linspace(a,b,m)\n",
    "    for p in numba.prange(m):\n",
    "        #weight_func=functional_Nadaraya_weight(X,x_grid[p]*x_func,h)[0,:]#functional_Nadaraya_weight_Bitcoin(X,x_grid[p]*x_func,h)\n",
    "        weight_func=functional_Nadaraya_weight_2D(X[0,:,:],x_grid[p]*x_func,h_func,kernel)\n",
    "        weight1=univariate_Nadaraya_weight_2D(X[0,:,:],dimred1,x_grid[p]*x_func,x_grid[p],h,1,kernel)\n",
    "        weight2=univariate_Nadaraya_weight_2D(X[0,:,:],dimred2,x_grid[p]*x_func,x_grid[p],h,2,kernel)\n",
    "        out[p,0]=weighted_quantile(Y[0,:],weight1,alpha)\n",
    "        out[p,1]=weighted_quantile(Y[0,:],weight2,alpha)\n",
    "        out[p,2]=weighted_quantile(Y[0,:],weight_func,alpha)\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def plot_quantile_dimred_vs_func_2D_total(X,Y,dimred1,dimred2,x_func,alpha,a,b,m,h_vector,h_func_vector,kernel): # dimred1,2 and x_func of shape (d,), e.g., dimred = E0[0,:] with E0 = fepls(X,Y,y_matrix,tau)\n",
    "    # x_func = any vector in H, eg beta_func(d) or E0[0,:]\n",
    "    # h_vector.shape = h_func_vector.shape = (m,)\n",
    "   # Y|(X,dimred1)=(x*x_func,dimred1) (type 1) vs Y|(X,dimred2) = x (type 2) vs Y|X = x*x_func (type functional)\n",
    "    out = np.zeros((m,3))\n",
    "    x_grid = np.linspace(a,b,m)\n",
    "    for p in numba.prange(m):\n",
    "        #weight_func=functional_Nadaraya_weight(X,x_grid[p]*x_func,h)[0,:]#functional_Nadaraya_weight_Bitcoin(X,x_grid[p]*x_func,h)\n",
    "        weight_func=functional_Nadaraya_weight_2D(X[0,:,:],x_grid[p]*x_func,h_func_vector[p],kernel)\n",
    "        weight1=univariate_Nadaraya_weight_2D(X[0,:,:],dimred1,x_grid[p]*x_func,x_grid[p],h_vector[p],1,kernel)\n",
    "        weight2=univariate_Nadaraya_weight_2D(X[0,:,:],dimred2,x_grid[p]*x_func,x_grid[p],h_vector[p],2,kernel)\n",
    "        out[p,0]=weighted_quantile(Y[0,:],weight1,alpha)\n",
    "        out[p,1]=weighted_quantile(Y[0,:],weight2,alpha)\n",
    "        out[p,2]=weighted_quantile(Y[0,:],weight_func,alpha)\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def plot_quantile_covariate_dimred_vs_func_3D(X,Y,dimred1,dimred2,x_func,alpha,a,b,m,h,h_func_2D,kernel): # dimred1,2 of shape (N,d), e.g., dimred = E = fepls(X,Y,y_matrix,tau)\n",
    "    # x_func = any vector in H, eg beta_func(d) or E0[0,:]\n",
    "    # h_func_2D.shape = (N,n)\n",
    "   # Y|(X,dimred1)=(x*x_func,dimred1) (type 1) vs Y|(X,dimred2) = x (type 2) vs Y|X = x*x_func (type functional)\n",
    "    N=X.shape[0]\n",
    "    out = np.zeros((N,m,3))\n",
    "    x_grid = np.linspace(a,b,m)\n",
    "    for p in numba.prange(m):\n",
    "        weight_func=functional_Nadaraya_weight_3D(X,x_grid[p]*x_func,h_func_2D,kernel)\n",
    "        weight1=univariate_Nadaraya_weight_3D(X,dimred1,x_grid[p]*x_func,x_grid[p],h,1,kernel)\n",
    "        weight2=univariate_Nadaraya_weight_3D(X,dimred2,x_grid[p]*x_func,x_grid[p],h,2,kernel)\n",
    "        out[:,p,0]=iterated_weq_2D(Y,weight1,alpha)\n",
    "        out[:,p,1]=iterated_weq_2D(Y,weight2,alpha)\n",
    "        out[:,p,2]=iterated_weq_2D(Y,weight_func,alpha)\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def plot_quantile_alpha_dimred_vs_func_3D(X,Y,dimred1,dimred2,x_func,x,a,b,m,h,h_func,type,kernel): # dimred of shape (N,d), e.g., dimred = E = fepls(X,Y,y_matrix,tau)\n",
    "    # x_func = any vector in H, eg beta_func(d) or E0[0,:]\n",
    "   # Y|(X,dimred1)=(x*x_func,dimred1) (type 1) vs Y|(X,dimred2) = x (type 2) vs Y|X = x*x_func (type functional)\n",
    "    N=X.shape[0]\n",
    "    out = np.zeros((N,m,3))\n",
    "    alpha_grid = np.linspace(0,1,m)\n",
    "    for p in range(m):\n",
    "        weight_func=functional_Nadaraya_weight_3D(X,x_func,h_func,kernel)\n",
    "        weight1=univariate_Nadaraya_weight_3D(X,dimred1,x_func,x,h,1,kernel)\n",
    "        weight2=univariate_Nadaraya_weight_3D(X,dimred2,x_func,x,h,2,kernel)\n",
    "        out[:,p,0]=iterated_weq_2D(Y,weight1,alpha_grid[p])\n",
    "        out[:,p,1]=iterated_weq_2D(Y,weight1,alpha_grid[p])\n",
    "        out[:,p,2]=iterated_weq_2D(Y,weight_func,alpha_grid[p])\n",
    "    return out\n",
    "\n",
    "################################################## ][ Tail index  \n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def get_hill_estimator(ordered_data):\n",
    "    \"\"\"\n",
    "    Function to calculate Hill estimator array given an ordered data\n",
    "    sequence. Decreasing ordering is required; e.g. ordered_data = np.sort(data[0,:])[::-1] where data might be Y (of shape (N,n))\n",
    "    :param ordered_data: numpy array of ordered data for which the 1st moment (Hill estimator) is calculated.\n",
    "    :return: numpy array of Hill estimator corresponding to all possible order statistics of the dataset.\n",
    "    \"\"\"\n",
    "    logs = np.log(ordered_data)\n",
    "    logs_cumsum = np.cumsum(logs[:-1])\n",
    "    k_vector = np.arange(1, len(ordered_data))\n",
    "    m1 = (1./k_vector)*logs_cumsum - logs[1:]\n",
    "    return m1\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def get_hill_estimator_2D(ordered_data_2D): # ordered_data_2D = np.sort(-data,axis=1)*-1 with data 2D-array\n",
    "    \"\"\"\n",
    "    Function to calculate Hill estimator array given an ordered data; e.g., ordered_data_2D = np.sort(-Y,axis=1)*-1\n",
    "    sequence. Decreasing ordering is required; e.g. ordered_data = np.sort(data[0,:])[::-1] where data might be Y (of shape (N,n))\n",
    "    :param ordered_data: numpy array of ordered data for which the 1st moment (Hill estimator) is calculated.\n",
    "    :return: numpy array of Hill estimator corresponding to all possible order statistics of the dataset.\n",
    "    \"\"\"\n",
    "    N=ordered_data_2D.shape[0]\n",
    "    n=ordered_data_2D.shape[1]\n",
    "    m1=np.zeros((N,n-1))\n",
    "    for i in numba.prange(N):\n",
    "        logs = np.log(ordered_data_2D[i,:]) \n",
    "        logs_cumsum = np.cumsum(logs[:-1])\n",
    "        k_vector = np.arange(1, len(ordered_data_2D[i,:]))\n",
    "        m1[i,:] = (1./k_vector)*logs_cumsum - logs[1:]\n",
    "    return m1\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def Hill_estimator_one_value(ordered_data, k):\n",
    "    \"\"\"\n",
    "    Function to calculate the Hill estimator for a specified order statistic k. Decreasing ordering is required.\n",
    "    :param ordered_data: Decreasingly ordered sample; e.g. ordered_data = np.sort(data[0,:])[::-1] where data might be Y (of shape (N,n))\n",
    "    :param k: from 1 up to and including len(ordered_data) - 1\n",
    "    :return: float with the value of the Hill estimator\n",
    "    \"\"\"\n",
    "    selected_logs = np.log(ordered_data[:k+1])\n",
    "    return 1./k * sum(selected_logs[:-1]) - selected_logs[-1]\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def Hill_estimator_one_value_2D(ordered_data_2D, k): # ordered_data_2D = np.sort(-data,axis=1)*-1 with data 2D-array\n",
    "    N=ordered_data_2D.shape[0]\n",
    "    n=ordered_data_2D.shape[1]\n",
    "    out=np.zeros((N,))\n",
    "    for i in numba.prange(N):\n",
    "        selected_logs = np.log(ordered_data_2D[i,:k+1])\n",
    "        out[i]=1./k * sum(selected_logs[:-1]) - selected_logs[-1]\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def tail_index_gamma_estimator(data,weight,alpha,J): \n",
    "    N=data.shape[0]\n",
    "    d=data.shape[2]\n",
    "    subdivision=np.array([(1/s) for s in np.arange(1,J+1)] )\n",
    "    quantile_data2=iterated_weq(data,weight,alpha) # quantile_data2.shape=(N,d)\n",
    "    out=np.zeros((N,))\n",
    "    aux=np.zeros((N,J))\n",
    "    for k in numba.prange(N):\n",
    "        for j in numba.prange(J):\n",
    "            quantile_data1=iterated_weq(data,weight,1-subdivision[j]*(1-alpha))\n",
    "            aux[k,j] = np.log(quantile_data1[k,0])-np.log(quantile_data2[k,0])\n",
    "            aux[k,j] /= -np.sum(np.log(subdivision))\n",
    "        out[k] = np.sum(aux[k,:])\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def tail_index_gamma_estimator_2D(Y,weight,alpha,J):  # Y.shape= (N,n); weight.shape = (n,)\n",
    "    subdivision=np.array([(1/s) for s in np.arange(1,J+1)] )\n",
    "    quantile_data2=weighted_quantile(Y[0,:],weight,alpha) # \n",
    "    aux=np.zeros((J))\n",
    "    for j in numba.prange(J):\n",
    "        quantile_data1=weighted_quantile(Y[0,:],weight,1-subdivision[j]*(1-alpha))\n",
    "        aux[j] = np.log(quantile_data1)-np.log(quantile_data2)\n",
    "        aux[j] /= -np.sum(np.log(subdivision))\n",
    "    return np.sum(aux)\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False)  \n",
    "def plot_tail_index_all_2D(X,Y,dimred1,dimred2,x_func,alpha,a,b,m,h,h_func,J,kernel): # Y.shape = (N,n); X.shape = (N,n,d); Y.shape = (n); x_func.shape = (d,); J=9\n",
    "    # dimred1.shape = dimred2.shape = (d,)\n",
    "    # Y|(X,dimred1)=(x*x_func,dimred1) (type 1) vs Y|(X,dimred2) = x (type 2) vs Y|X = x*x_func (type functional)\n",
    "    x_grid=np.linspace(a,b,m)\n",
    "    out=np.zeros((m,3))\n",
    "    for p in numba.prange(m):\n",
    "        weight1=univariate_Nadaraya_weight_2D(X[0,:,:],dimred1,x_grid[p]*x_func,x_grid[p],h,1,kernel)\n",
    "        weight2=univariate_Nadaraya_weight_2D(X[0,:,:],dimred2,x_grid[p]*x_func,x_grid[p],h,2,kernel)\n",
    "        weight_func=functional_Nadaraya_weight_2D(X[0,:,:],x_grid[p]*x_func,h_func,kernel)\n",
    "        out[p,0]=tail_index_gamma_estimator_2D(Y,weight1,alpha,J)\n",
    "        out[p,1]=tail_index_gamma_estimator_2D(Y,weight2,alpha,J)\n",
    "        out[p,2]=tail_index_gamma_estimator_2D(Y,weight_func,alpha,J)\n",
    "    return out\n",
    "\n",
    "################################################## ][ Copula \n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def marginal_wecdf(data,weight,x_vect): # x row vector of length d (each marginal has its own treshold)\n",
    "    # e.g. for uniform treshold t, x_vect=x*np.ones((d,))\n",
    "    # data.shape = (N,n,d); weight.shape=(N,n)\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    out=np.zeros((N,d))\n",
    "    for k in numba.prange(N):\n",
    "        for j in numba.prange(d):\n",
    "            indicator_matrix = np.where(data[:,:,j]<x_vect[j],1,0) # binary matrix with same shape as data representating the indicator matrix 1_{X_ij<x_j}\n",
    "            out[k,j]= np.sum(np.multiply(indicator_matrix[k,:],weight[k,:]))\n",
    "    return out/n # (N,d)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def rank(U): # U.shape = (N,n,d)\n",
    "    R = np.empty_like(U)\n",
    "    for p in numba.prange(U.shape[0]):\n",
    "        for j in numba.prange(U.shape[2]):\n",
    "            R[p,:, j] = np.argsort(np.argsort(U[p,:, j]))+1\n",
    "    return R\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def weighted_ranked_data(R,weight): # ie: \\hat{F}_{j,n,y}(X_ij); R = rank(data) with shape (N,n,d); weight.shape=(N,n)\n",
    "    N=R.shape[0]\n",
    "    n=R.shape[1]\n",
    "    d=R.shape[2]\n",
    "    out=np.zeros((N,n,d))\n",
    "    for k in numba.prange(N):\n",
    "        for i in numba.prange(n):\n",
    "            for j in numba.prange(d):\n",
    "                r=np.int32(R[k,i,j])\n",
    "                out[k,i,j]=np.sum(weight[k,0:r])\n",
    "    return out/n \n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def bivariate_indicator(data,j,k,x,y): # data.shape = (N,n,d); 1\\le j\\neq k \\le d\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    A=np.where(data[:,:,j]<x,1,0)\n",
    "    B=np.where(data[:,:,k]<y,1,0)\n",
    "    return np.multiply(A,B) # shape = (N,n)\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def bivariate_anti_indicator(data,j,k,x,y): # data.shape = (N,n,d); 1\\le j\\neq k \\le d\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    A=np.where(data[:,:,j]<x,0,1)\n",
    "    B=np.where(data[:,:,k]<y,0,1)\n",
    "    return np.multiply(A,B) # shape = (N,n)\n",
    "    \n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def bivariate_wecdf(data,weight,j_1,j_2,x,y):\n",
    "    # data.shape = (N,n,d); weight.shape=(N,n); 1\\le j\\neq k \\le d\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    biv_indicator=bivariate_indicator(data,j_1,j_2,x,y)\n",
    "    return np.sum(np.multiply(biv_indicator,weight),axis=1)/n # shape = (N,)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def bivariate_ecdf(data,j_1,j_2,x,y):\n",
    "    # data.shape = (N,n,d); weight.shape=(N,n); 1\\le j\\neq k \\le d\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    biv_indicator=bivariate_indicator(data,j_1,j_2,x,y)\n",
    "    return np.sum((biv_indicator),axis=1)/n # shape = (N,)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def bivariate_survival_wecdf(data,weight,j_1,j_2,x,y):\n",
    "    # data.shape = (N,n,d); weight.shape=(N,n); 1\\le j\\neq k \\le d\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    biv_indicator=bivariate_anti_indicator(data,j_1,j_2,x,y)\n",
    "    return np.sum(np.multiply(biv_indicator,weight),axis=1)/n # shape = (N,)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def empirical_uniform_data(data,weight): \n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    out = np.zeros((N,n,d))\n",
    "    for k in numba.prange(N):\n",
    "        for i in numba.prange(n):\n",
    "            out[k,i,:]=marginal_wecdf(data,weight,data[k,i,:])[k,:]\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def weighted_empirical_copula(data,weight,j_1,j_2,u,v): # u.shape = (2,) in (0,1)\n",
    "        # data.shape = (N,n,d); weight.shape=(N,n); 1\\le j_1\\neq j_2 \\le d\n",
    "    N=data.shape[0]\n",
    "    n=data.shape[1]\n",
    "    d=data.shape[2]\n",
    "    out=np.zeros((N,n))\n",
    "    data=empirical_uniform_data(data,weight)\n",
    "    biv_indicator=bivariate_indicator(emp_unif_data,j_1,j_2,u,v)\n",
    "    out= np.multiply(biv_indicator,weight)/n \n",
    "    return np.sum(out,axis=1) # return (N)-shape\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def weighted_ranked_empirical_copula(weighted_ranked_data,weight,j_1,j_2,u,v): # u,v in (0,1)\n",
    "        # weighted_ranked_data=weighted_ranked_data(R.astype(int),weight) where R=rank(data); \n",
    "        # weighted_ranked_data.shape = (N,n,d) is ; weight.shape=(N,n); 1\\le j_1\\neq j_2 \\le d\n",
    "    N=weighted_ranked_data.shape[0]\n",
    "    n=weighted_ranked_data.shape[1]\n",
    "    d=weighted_ranked_data.shape[2]\n",
    "    out=np.zeros((N,n))\n",
    "    biv_indicator=bivariate_indicator(weighted_ranked_data,j_1,j_2,u,v)\n",
    "    out= np.multiply(biv_indicator,weight)/n \n",
    "    return np.sum(out,axis=1) # return (N)-shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e0c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## Execute (Synthetic Data)\n",
    "\n",
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "c= 1 # c is \\kappa the tail index of g\n",
    "tau=-2 # tail index of \\vfi\n",
    "#nu = 1 #  tail index of \\psi such that 2\\gamma \\nu_j < 1 for all 1\\le j \\le J\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "rho=-1/2\n",
    "mu = 200 # noise mean\n",
    "l=2 # grid parameter\n",
    "start = 4\n",
    "\n",
    "tic=time.time()\n",
    "\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Y_sim=Y\n",
    "\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix = threshold(X,Y,Y_sort_index,tau,100,start)\n",
    "E = fepls(X,Y,y_matrix,tau)\n",
    "#E2 = fepls_numba(X,Y,y_matrix,tau) # Same but not parallelizable and slower...\n",
    "#F=hatbeta_dot_X(X,E)\n",
    "\n",
    "#############################################################################\n",
    "print(check_cond_no_q(gamma,c,tau))\n",
    "print(\"Time cost\",time.time()-tic)\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76b0f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "################################## Plot Histograms (Synthetic Data)\n",
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "tau=-2 # tail index of \\vfi\n",
    "#nu = 1 #  tail index of \\psi such that 2\\gamma \\nu_j < 1 for all 1\\le j \\le J\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "gamma=9/10# 1/3 or 1/2 or 9/10\n",
    "rho=-1/2\n",
    "mu = 200 # noise mean\n",
    "l=2 # grid parameter\n",
    "start = 4\n",
    "\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m,start)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_10_2_0_9.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "gamma=1/2\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m,start)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_10_2_0_5.pdf')  \n",
    "plt.show()\n",
    "\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "gamma=1/3\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix = threshold_index(X,Y,Y_sort_index,tau,m,start)\n",
    "plt.hist(y_matrix, bins=m)\n",
    "plt.savefig('hist_k_10_2_0_333.pdf')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f46bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ Estim plot (Synthetic Data)\n",
    "\n",
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "c= 3/2 # c is \\kappa the tail index of g\n",
    "tau=-2 # tail index of \\vfi\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "rho=-1/2\n",
    "mu = 200 # noise mean\n",
    "start=4\n",
    "l=2 # grid parameter\n",
    "tic=time.time()\n",
    "\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "y_matrix2 = threshold(X,Y,Y_sort_index,tau,100,0)\n",
    "y_matrix3 = threshold(X,Y,Y_sort_index,tau,100,4)\n",
    "y_matrix4 = threshold(X,Y,Y_sort_index,tau,100,9)\n",
    "\n",
    "E2 = fepls(X,Y,y_matrix2,tau)\n",
    "E3 = fepls(X,Y,y_matrix3,tau)\n",
    "E4 = fepls(X,Y,y_matrix4,tau)\n",
    "\n",
    "#############################################################################\n",
    "print(check_cond_no_q(gamma,c,tau))\n",
    "##################################################################################################\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "#max_values = np.max(E, axis=0) # shape (d,)\n",
    "#min_values = np.min(E, axis=0) # shape (d,)\n",
    "max_values2 = np.nanquantile(E2, 0.95, axis=0) # shape (d,)\n",
    "min_values2 = np.nanquantile(E2, 0.05, axis=0) # shape (d,)\n",
    "mean_values2 = np.nanmean(E2, axis=0) # shape (d,)\n",
    "median_values2 = np.nanmedian(E2, axis=0) # shape (d,)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values = np.arange(d)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [0,1]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([0,0 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "ax.fill_between(x_values, min_values2, max_values2, color='skyblue', alpha=0.4)\n",
    "ax.plot(x_values, mean_values2)\n",
    "ax.plot(x_values, beta_func(d))\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "#plt.legend()\n",
    "# Show the plot\n",
    "plt.savefig('beta_estim_plot_conc100_0_1_5_0_333.pdf')  \n",
    "plt.show()\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "#max_values = np.max(E, axis=0) # shape (d,)\n",
    "#min_values = np.min(E, axis=0) # shape (d,)\n",
    "max_values3 = np.nanquantile(E3, 0.95, axis=0) # shape (d,)\n",
    "min_values3 = np.nanquantile(E3, 0.05, axis=0) # shape (d,)\n",
    "mean_values3 = np.nanmean(E3, axis=0) # shape (d,)\n",
    "median_values3 = np.nanmedian(E3, axis=0) # shape (d,)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [0,1]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([0,0 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "ax.fill_between(x_values, min_values3, max_values3, color='skyblue', alpha=0.4)\n",
    "ax.plot(x_values, mean_values3)\n",
    "ax.plot(x_values, beta_func(d))\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "#plt.legend()\n",
    "# Show the plot\n",
    "plt.savefig('beta_estim_plot_conc100_4_1_5_0_333.pdf')  \n",
    "plt.show()\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "#max_values = np.max(E4, axis=0) # shape (d,)\n",
    "#min_values = np.min(E4, axis=0) # shape (d,)\n",
    "max_values4 = np.nanquantile(E4, 0.95, axis=0) # shape (d,)\n",
    "min_values4 = np.nanquantile(E4, 0.05, axis=0) # shape (d,)\n",
    "mean_values4 = np.nanmean(E4, axis=0) # shape (d,)\n",
    "median_values4 = np.nanmedian(E4, axis=0) # shape (d,)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [0,1]\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "ax.set_xticklabels([0,0 , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1 ])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "ax.fill_between(x_values, min_values4, max_values4, color='skyblue', alpha=0.4)\n",
    "ax.plot(x_values, mean_values4)\n",
    "ax.plot(x_values, beta_func(d))\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "#plt.legend()\n",
    "# Show the plot\n",
    "plt.savefig('beta_estim_plot_conc100_9_1_5_0_333.pdf')  \n",
    "plt.show()\n",
    "\n",
    "print(\"Time cost\",time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### Hyperparameters and Sample\n",
    "\n",
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "c= 2 # c is \\kappa the tail index of g\n",
    "tau=-2 # tail index of \\vfi\n",
    "#nu = 1 #  tail index of \\psi such that 2\\gamma \\nu_j < 1 for all 1\\le j \\le J\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "rho=-1/2\n",
    "mu = 200 # noise mean\n",
    "l=2 # grid parameter\n",
    "start = 4\n",
    "\n",
    "tic=time.time()\n",
    "\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Y_sim=Y\n",
    "\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "\n",
    "\n",
    "#################################################### Correlation (concomittant) plot (Synthetic data)\n",
    "\n",
    "m=int(n/2) # 1\\le m \\le n\n",
    "tic=time.time()\n",
    "A=concomittant_corr(X,Y,Y_sort_index,-1,m)[:,1:]\n",
    "A2=concomittant_corr(X,Y,Y_sort_index,-2,m)[:,1:]\n",
    "A3=concomittant_corr(X,Y,Y_sort_index,-3,m)[:,1:]\n",
    "print(\"Time cost\",time.time()-tic)\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "\n",
    "mean_values_A = np.nanmean(A, axis=0) # shape (int(n/l),)\n",
    "#median_values_A = np.nanmedian(A, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A2 = np.nanmean(A2, axis=0) # shape (int(n/l),)\n",
    "#median_values_A2 = np.nanmedian(A2, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A3 = np.nanmean(A3, axis=0) # shape (int(n/l),)\n",
    "#median_values_A3 = np.nanmedian(A3, axis=0) # shape (int(n/l),)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values_A = np.arange(m-1)#np.arange(int(n/l))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Set x-axis tick labels to discretized interval [1,250]\n",
    "#ax.xaxis.set_major_locator(plt.MaxNLocator(27))\n",
    "#ax.set_xticklabels([1, 1, 10 , 20, 30,  40, 50, 60, 70, 80, 90, 100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250])\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "#ax.fill_between(x_values_A, min_values_A, max_values_A, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A, label='tau = -1')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A2, max_values_A2, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A2, label='tau = -2')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A3, max_values_A3, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A3, label='tau = -3')\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "plt.legend()\n",
    "plt.savefig('beta_estim_corr_2_0_333.pdf')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################### Hyperparameters and Sample\n",
    "\n",
    "N=500\n",
    "n=500\n",
    "d=101\n",
    "s_Y=1 # scale parameter of Pareto/Lomax distribution\n",
    "c= 1 # c is \\kappa the tail index of g\n",
    "tau=-2 # tail index of \\vfi\n",
    "#nu = 1 #  tail index of \\psi such that 2\\gamma \\nu_j < 1 for all 1\\le j \\le J\n",
    "snr=10 # signal-to-noise ratio\n",
    "H=1/3 # Hurst parameter of fBm noise\n",
    "gamma=1/3# 1/3 or 1/2 or 9/10\n",
    "rho=-1/2\n",
    "mu = 200 # noise mean\n",
    "l=2 # grid parameter\n",
    "start = 4\n",
    "\n",
    "tic=time.time()\n",
    "\n",
    "#Y=Pareto_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "#Y=Lomax_quantile_function(npr.uniform(0,1,size=(N,n)),theta,s_Y)\n",
    "Y=Burr_quantile_function(npr.uniform(0,1,size=(N,n)),gamma,rho)\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "Y_sim=Y\n",
    "\n",
    "Z=npr.normal(0,1,size=(N,n,d - 1)) # latent variables for fBm sampling\n",
    "aux=np.multiply.outer(Y**c,beta_func(d)) # g(Y)\\beta\n",
    "eps=coeurjolly_cholesky_fbm_var(Y,Z,H,c,snr,mu) \n",
    "X=aux+eps\n",
    "Y_sort_index = np.argsort(Y,axis=1)\n",
    "m=int(n/5)\n",
    "\n",
    "####################################################### hatbeta_dot_beta (Synthetic Data)\n",
    "\n",
    "l=2 # grid parameter\n",
    "A=hatbeta_dot_beta(X,Y,-1,l)\n",
    "A2=hatbeta_dot_beta(X,Y,-2,l)\n",
    "A3=hatbeta_dot_beta(X,Y,-3,l)\n",
    "\n",
    "print(\"Time cost\",time.time()-tic)\n",
    "\n",
    "# Calculate the maximum and minimum values for each position\n",
    "mean_values_A = np.nanmean(A, axis=0) # shape (int(n/l),)\n",
    "#median_values_A = np.nanmedian(A, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A2 = np.nanmean(A2, axis=0) # shape (int(n/l),)\n",
    "#median_values_A2 = np.nanmedian(A2, axis=0) # shape (int(n/l),)\n",
    "\n",
    "mean_values_A3 = np.nanmean(A3, axis=0) # shape (int(n/l),)\n",
    "#median_values_A3 = np.nanmedian(A3, axis=0) # shape (int(n/l),)\n",
    "\n",
    "# Create x values (assuming x values are just indices in this case)\n",
    "x_values_A = np.arange(int(n/l))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "# Plot the fill between the max and min values\n",
    "#ax.fill_between(x_values_A, min_values_A, max_values_A, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A, label='tau = -2')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A2, max_values_A2, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A2, label='tau = -3')\n",
    "\n",
    "#ax.fill_between(x_values_A, min_values_A3, max_values_A3, color='skyblue', alpha=0.4, label='Confidence Area')\n",
    "ax.plot(x_values_A, mean_values_A3, label='tau = -4')\n",
    "\n",
    "# Add labels and legend\n",
    "#ax.xlabel('Index')\n",
    "#ax.ylabel('Values')\n",
    "#ax.title('FEPLS Estimation of beta - Confidence intervals')\n",
    "plt.legend()\n",
    "#plt.savefig('beta_estim_exceedance_1_5_0_9.pdf')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################### Edge-of-inequality simulations (Synthetic Data)\n",
    "\n",
    "def run_edge_tau_simulations(\n",
    "    N=500,\n",
    "    n=500,\n",
    "    d=101,\n",
    "    gamma=1/3,\n",
    "    c=2.0,\n",
    "    rho=-1/2,\n",
    "    snr=10.0,\n",
    "    H=1/3,\n",
    "    mu=200.0,\n",
    "    start=4,\n",
    "    eps_tau=1e-2,\n",
    "    n_tau=3,\n",
    "    random_seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run FEPLS simulations for tau values very close to the theoretical bounds\n",
    "    0 < 2(c+tau)\\gamma < 1.                                       # we document the goal of this helper\n",
    "    \"\"\"\n",
    "    npr.seed(random_seed)                                        # we fix the random seed for reproducibility\n",
    "    tau_lower = -c                                               # we derive the lower bound from 2(c+tau)\\gamma>0\n",
    "    tau_upper = 1.0 / (2.0 * gamma) - c                          # we derive the upper bound from 2(c+tau)\\gamma<1\n",
    "    tau_min = tau_lower + eps_tau                                # we move slightly inside the interval on the left\n",
    "    tau_max = tau_upper - eps_tau                                # we move slightly inside the interval on the right\n",
    "    if tau_min >= tau_max:                                       # we guard against invalid parameter choices\n",
    "        raise ValueError(\"No admissible tau interval for these (gamma,c,gamma).\")  # we stop if interval is empty\n",
    "    tau_grid = np.linspace(tau_min, tau_max, n_tau)              # we build a small grid of tau values inside\n",
    "    print(\"tau interval: (\", tau_lower, \",\", tau_upper, \")\")     # we display the theoretical interval\n",
    "    print(\"simulating for tau values:\", tau_grid)                # we display the grid used in simulations\n",
    "\n",
    "    Y = Burr_quantile_function(npr.uniform(0, 1, size=(N, n)), gamma, rho)  # we generate heavy-tailed Y via Burr\n",
    "    Y_sort_index = np.argsort(Y, axis=1)                          # we compute sorted indices of Y for thresholds\n",
    "    Z = npr.normal(0, 1, size=(N, n, d - 1))                      # we draw latent Gaussian noise for fBm\n",
    "    aux = np.multiply.outer(Y ** c, beta_func(d))                 # we build the signal part g(Y)\\beta\n",
    "    eps = coeurjolly_cholesky_fbm_var(Y, Z, H, c, snr, mu)        # we generate functional noise eps\n",
    "    X = aux + eps                                                 # we obtain covariates X=g(Y)\\beta+eps\n",
    "\n",
    "    results = {}                                                  # we collect summary statistics here\n",
    "    m = int(n / 5)                                                # we keep the same m as in other experiments\n",
    "    for tau in tau_grid:                                          # we loop over tau values near the inequalities\n",
    "        print(\"\\n----- tau =\", tau, \"-----\")                      # we mark the beginning of a new configuration\n",
    "        check_cond_no_q(gamma, c, tau)                            # we display whether 2(c+tau)\\gamma<1 holds\n",
    "        y_matrix = threshold(X, Y, Y_sort_index, tau, 100, start) # we build the random threshold matrix\n",
    "        E = fepls(X, Y, y_matrix, tau)                            # we compute FEPLS directions for this tau\n",
    "        hatbeta_dot_beta = (1.0 / d) * np.sum(                    # we compute alignment with the true index\n",
    "            E * beta_func(d), axis=1\n",
    "        )\n",
    "        results[float(tau)] = {                                   # we store simple Monte Carlo summaries\n",
    "            \"mean_hatbeta_dot_beta\": float(np.mean(hatbeta_dot_beta)),  # we store the mean alignment\n",
    "            \"std_hatbeta_dot_beta\": float(np.std(hatbeta_dot_beta)),    # we store the std of alignment\n",
    "        }\n",
    "    return results                                                # we return the dictionary of summaries\n",
    "\n",
    "\n",
    "edge_results = run_edge_tau_simulations()                         # we launch the edge-of-inequality simulations\n",
    "print(edge_results)                                               # we print the Monte Carlo summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
