{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1ebce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d5aa2a",
   "metadata": {},
   "source": [
    "# 1. Numba Optimized Functions (Adapted from Original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113de403",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def sort_2d_array(x):\n",
    "    n,m=np.shape(x)\n",
    "    for row in numba.prange(n):\n",
    "        x[row]=np.sort(x[row])\n",
    "    return x\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def fepls_numba(X,Y,y_matrix,tau): \n",
    "    # X of size (N,n,d) and Y of size (N,n)\n",
    "    # y_matrix of shape (N,n)\n",
    "    # tau is the tail index of psi\n",
    "    N=X.shape[0]\n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    out=np.zeros((N,d))\n",
    "    for j in numba.prange(d):\n",
    "        aux = np.multiply(X[:,:,j],Y**tau) \n",
    "        out2 = np.multiply(aux,np.greater_equal(Y,y_matrix)) \n",
    "        out[:,j]= np.sum(out2,axis=1)/n \n",
    "    norms=np.sqrt(np.sum(out**2,axis=1)/d) \n",
    "    out2 =  out * (norms.reshape((norms.size, 1)))**(-1)\n",
    "    return out2 \n",
    "\n",
    "def fepls(X,Y,y_matrix,tau):\n",
    "    # Wrapper for non-numba usage if needed, but we use the numba one mostly\n",
    "    return fepls_numba(X,Y,y_matrix,tau)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def concomittant_corr(X,Y,Y_sort_index,tau,m): \n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((N,m))\n",
    "    YY=np.copy(Y)\n",
    "    Y_sort=sort_2d_array(YY)\n",
    "    for k in numba.prange(m):\n",
    "        y_array = np.zeros((N,n,k+1))\n",
    "        aux = np.zeros((N,k+1))\n",
    "        aux3 = Y_sort[:,n-k-1:] \n",
    "        for i in numba.prange(k):\n",
    "            y_array[:,0,i] = Y_sort[:,n-i-1]\n",
    "            for j_2 in numba.prange(N):\n",
    "                y_array[j_2,:,i] = y_array[j_2,0,i]\n",
    "            hat_beta = fepls_numba(X,Y,y_array[:,:,i],tau) \n",
    "            for j_1 in numba.prange(N):\n",
    "                i_c = Y_sort_index[j_1,i]\n",
    "                aux[j_1,i]=(1/d)*np.sum(np.multiply(hat_beta[j_1,:],X[j_1,i_c,:]))\n",
    "                out[j_1,k]= np.corrcoef(aux3[j_1,:],aux[j_1,:])[0,1]\n",
    "    return out\n",
    "\n",
    "def bitcoin_concomittant_corr(X,Y,tau,m): \n",
    "    # Simplified version for N=1 case usually used in main loop\n",
    "    N = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    d = X.shape[2]\n",
    "    out = np.zeros((m))\n",
    "    Y_sort=np.sort(Y,axis=1)\n",
    "    Y_sort_index = np.argsort(Y,axis=1)\n",
    "    for k in range(m):\n",
    "        y_array = np.zeros((N,n,k+1))\n",
    "        aux = np.zeros((k+1))\n",
    "        aux3 = Y_sort[0,n-k-1:] \n",
    "        for i in range(k):\n",
    "            y_array[:,:,i] = (Y_sort[0,n-i-1])*np.ones((1,n))\n",
    "            hat_beta = fepls(X,Y,y_array[:,:,i],tau) \n",
    "            i_c = Y_sort_index[0,i]\n",
    "            aux[i]=(1/d)*np.sum(np.multiply(hat_beta[0,:],X[0,i_c,:]))\n",
    "            # Check for NaN or constant input to corrcoef\n",
    "            if np.std(aux3) == 0 or np.std(aux) == 0:\n",
    "                 out[k] = 0\n",
    "            else:\n",
    "                 out[k]= np.corrcoef(aux3,aux)[0,1]\n",
    "    return np.abs(out)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)\n",
    "def get_hill_estimator(ordered_data):\n",
    "    logs = np.log(ordered_data)\n",
    "    logs_cumsum = np.cumsum(logs[:-1])\n",
    "    k_vector = np.arange(1, len(ordered_data))\n",
    "    m1 = (1./k_vector)*logs_cumsum - logs[1:]\n",
    "    return m1\n",
    "\n",
    "def Exponential_QQ_Plot_1D(Y,k):\n",
    "    n=Y.shape[1]\n",
    "    out=np.zeros((k))\n",
    "    out2=np.zeros((k))\n",
    "    YY=np.sort(Y,axis=1)\n",
    "    for i in range(k):\n",
    "        out[i]=np.log((k+1)/(i+1))\n",
    "        out2[i]=  np.log(YY[0,n-i-1])-np.log(YY[0,-k])\n",
    "    return np.column_stack((out,out2))\n",
    "\n",
    "@numba.njit(parallel=False, fastmath=False) \n",
    "def Gaussian_kernel(x):\n",
    "    return (1/np.sqrt(2*np.pi))*np.exp(-0.5*np.power(x,2))\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def Epanechnikov_kernel_1D(x):\n",
    "    out = np.zeros_like(x)\n",
    "    x=np.asarray(x)\n",
    "    for i in numba.prange(x.shape[0]):\n",
    "        if x[i]<=1 and x[i]>=-1: # Fixed range\n",
    "            out[i] = np.multiply(0.75,1-np.power(x[i],2))\n",
    "        else:\n",
    "            out[i]=0\n",
    "    return out\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def univariate_Nadaraya_weight_2D(X_2D,dimred,x_func,x,h,type,kernel): \n",
    "    d=x_func.shape[0]\n",
    "    # type 2: Y|(X,dimred) = x\n",
    "    if kernel == 1:\n",
    "        K_h=Epanechnikov_kernel_1D((np.dot(X_2D,dimred)/d-x)/h) \n",
    "    if kernel == 2:\n",
    "        K_h=Gaussian_kernel((np.dot(X_2D,dimred)/d-x)/h) \n",
    "    \n",
    "    sum_Kh = np.sum(K_h)\n",
    "    if sum_Kh == 0:\n",
    "        return np.ones_like(K_h) / K_h.shape[0]\n",
    "    return K_h/sum_Kh\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def functional_Nadaraya_weight_2D(X_2D,x_func,h,kernel): \n",
    "    d=x_func.shape[0]\n",
    "    aux = (X_2D-x_func*np.ones(d))**2 \n",
    "    norm = np.sqrt((1/d)*np.sum(aux,axis=1)) \n",
    "    if kernel == 1:\n",
    "        K_h= Epanechnikov_kernel_1D(norm/h) \n",
    "    if kernel == 2:\n",
    "        K_h= Gaussian_kernel(norm/h)\n",
    "        \n",
    "    sum_Kh = np.sum(K_h)\n",
    "    if sum_Kh == 0:\n",
    "        return np.ones_like(K_h) / K_h.shape[0]\n",
    "    return K_h/sum_Kh\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False)   \n",
    "def weighted_quantile(data,weight,alpha): \n",
    "    sorter = np.argsort(data)\n",
    "    data = data[sorter]\n",
    "    weight = weight[sorter]\n",
    "    weighted_quantiles = np.cumsum(weight) - 0.5 * weight\n",
    "    weighted_quantiles /= np.sum(weight)\n",
    "    return np.interp(alpha, weighted_quantiles, data)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def tail_index_gamma_estimator_2D(Y,weight,alpha,J): \n",
    "    subdivision=np.array([(1/s) for s in np.arange(1,J+1)] )\n",
    "    quantile_data2=weighted_quantile(Y[0,:],weight,alpha) \n",
    "    aux=np.zeros((J))\n",
    "    for j in numba.prange(J):\n",
    "        quantile_data1=weighted_quantile(Y[0,:],weight,1-subdivision[j]*(1-alpha))\n",
    "        val = np.log(quantile_data1)-np.log(quantile_data2)\n",
    "        aux[j] = val / -np.sum(np.log(subdivision))\n",
    "    return np.sum(aux)\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def plot_quantile_conditional_on_sample_new(X,Y,dimred,x_func,alpha,h_univ_vector,h_func_vector): \n",
    "    n=X.shape[1]\n",
    "    d=X.shape[2]\n",
    "    # Grid of scalar values <X, x_func>\n",
    "    proj_vals = np.dot(X[0,:,:],x_func)/d\n",
    "    S_grid = np.linspace(np.min(proj_vals),np.max(proj_vals),100) # Reduced to 100 for speed\n",
    "    out = np.zeros((S_grid.shape[0],2))\n",
    "    \n",
    "    for p in numba.prange(S_grid.shape[0]):\n",
    "        # Univariate Weight\n",
    "        K_h=Gaussian_kernel((np.dot(X[0,:,:],dimred)/d-(np.dot(x_func,dimred)/d)*S_grid[p])/h_univ_vector[0]) # Simplification: using const h\n",
    "        weight_univ=K_h/np.sum(K_h)        \n",
    "        \n",
    "        # Functional Weight\n",
    "        weight_func=functional_Nadaraya_weight_2D(X[0,:,:],S_grid[p]*x_func,h_func_vector[0],2)\n",
    "        \n",
    "        out[p,0]=weighted_quantile(Y[0,:],weight_univ,alpha)\n",
    "        out[p,1]=weighted_quantile(Y[0,:],weight_func,alpha)\n",
    "    return out, S_grid\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=False) \n",
    "def beta_func(d):\n",
    "    grid=np.linspace(0,1,d)\n",
    "    norm=np.sqrt(np.sum(np.sin(2*np.pi*grid)**2)/d)\n",
    "    return np.sin(2*np.pi*grid)/norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6428890",
   "metadata": {},
   "source": [
    "# 2. Data Loading and \"Functional\" Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77069a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stooq_file(filepath):\n",
    "    \"\"\"Parses a single Stooq .txt file into a clean DataFrame.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        # Stooq format: <TICKER>,<PER>,<DATE>,<TIME>,...\n",
    "        # Columns might be standard or capitalized.\n",
    "        df.columns = [c.replace('<','').replace('>','').lower() for c in df.columns]\n",
    "        \n",
    "        # Combine DATE and TIME into a DateTimeIndex\n",
    "        # DATE is usually YYYYMMDD, TIME is HHMMSS\n",
    "        df['datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + df['time'].astype(str).str.zfill(6), format='%Y%m%d %H%M%S')\n",
    "        df = df.set_index('datetime')\n",
    "        \n",
    "        # Keep relevant columns\n",
    "        return df[['close', 'vol']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_functional_data(df_dict, ticker_name, time_grid=None):\n",
    "    \"\"\"\n",
    "    Transforms 5-min time series into a Matrix of shape (Days, TimePoints).\n",
    "    Row i = Intraday price curve of Day i.\n",
    "    \"\"\"\n",
    "    df = df_dict[ticker_name]\n",
    "    \n",
    "    # 1. Create a Date column and Time column\n",
    "    df = df.copy()\n",
    "    df['Date'] = df.index.date\n",
    "    df['Time'] = df.index.time\n",
    "    \n",
    "    # 2. If no time_grid provided, determine the common intraday times\n",
    "    # Standard BSE hours 09:00 to 17:00 (approx)\n",
    "    if time_grid is None:\n",
    "        # We pick the most common timestamps found in the data\n",
    "        time_counts = df['Time'].value_counts()\n",
    "        # Filter times that appear in at least 50% of days to exclude artifacts\n",
    "        n_days = df['Date'].nunique()\n",
    "        common_times = time_counts[time_counts > n_days * 0.5].index.sort_values()\n",
    "        time_grid = common_times\n",
    "    \n",
    "    # 3. Pivot: Rows=Date, Cols=Time, Values=Close\n",
    "    # We need to reindex to ensure exactly the time_grid columns\n",
    "    pivot_df = df.pivot_table(index='Date', columns='Time', values='close')\n",
    "    \n",
    "    # Reindex columns to the standard grid, filling missing intraday values \n",
    "    # (forward fill first for missing ticks, then backfill)\n",
    "    pivot_df = pivot_df.reindex(columns=time_grid)\n",
    "    pivot_df = pivot_df.ffill(axis=1).bfill(axis=1)\n",
    "    \n",
    "    # Drop days that are still completely empty or have too many missing values\n",
    "    pivot_df = pivot_df.dropna()\n",
    "    \n",
    "    return pivot_df, time_grid\n",
    "\n",
    "# Define path\n",
    "DATA_DIR = \"../../data/stooq/hungary/5_hu_txt/data/5 min/hu/bse stocks/\"\n",
    "\n",
    "# List of all main BSE stocks found in the directory as possible targets\n",
    "targets = [\n",
    "    '4ig.hu.txt', 'akko.hu.txt', 'alteo.hu.txt', 'amixa.hu.txt', 'any.hu.txt',\n",
    "    'appeninn.hu.txt', 'astrasun.hu.txt', 'autowallis.hu.txt', 'bet.hu.txt',\n",
    "    'bif.hu.txt', 'chome.hu.txt', 'cigpannonia.hu.txt', 'civita.hu.txt', 'delta.hu.txt',\n",
    "    'dmker.hu.txt', 'dunahouse.hu.txt', 'enefi.hu.txt',\n",
    "    'epduferr.hu.txt', 'eproliusia.hu.txt', 'esense.hu.txt', 'forras_oe.hu.txt',\n",
    "    'forras_t.hu.txt', 'futuraqua.hu.txt', 'glia.hu.txt', 'gloster.hu.txt', 'goodwillphrm.hu.txt',\n",
    "    'gopd.hu.txt', 'granit.hu.txt', 'gspark.hu.txt', 'kermannit.hu.txt', 'masterplast.hu.txt',\n",
    "    'mbhbank.hu.txt', 'mbhjb.hu.txt', 'megakran.hu.txt', 'mol.hu.txt', 'mtelekom.hu.txt',\n",
    "    'multihome.hu.txt', 'nap.hu.txt', 'naturland.hu.txt', 'navigator.hu.txt', 'nordgeneral.hu.txt',\n",
    "    'nutex.hu.txt', 'o3pnrs.hu.txt', 'opus.hu.txt', 'ormester.hu.txt', 'otp.hu.txt',\n",
    "    'pannergy.hu.txt', 'pensum.hu.txt', 'polyduct.hu.txt', 'raba.hu.txt', 'richter.hu.txt',\n",
    "    'splus.hu.txt', 'strt.hu.txt', 'sundell.hu.txt', 'ubm.hu.txt',\n",
    "    'vertikal.hu.txt', 'vig.hu.txt', 'vvt.hu.txt', 'waberers.hu.txt'\n",
    "]\n",
    "data_store = {}\n",
    "for t in targets:\n",
    "    path = os.path.join(DATA_DIR, t)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading {t}...\")\n",
    "        data_store[t] = load_stooq_file(path)\n",
    "    else:\n",
    "        print(f\"File not found: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d001b7b4",
   "metadata": {},
   "source": [
    "# 3. Alignment and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functional data for OTP, MOL, RICHTER\n",
    "# We need a common time grid for all of them to be comparable\n",
    "# Let's inspect OTP first to get a grid\n",
    "if 'otp.hu.txt' in data_store:\n",
    "    _, master_grid = create_functional_data(data_store, 'otp.hu.txt')\n",
    "    print(f\"Established master time grid with {len(master_grid)} points per day.\")\n",
    "else:\n",
    "    # Fallback\n",
    "    master_grid = None\n",
    "\n",
    "# Create matrices\n",
    "func_data = {}\n",
    "for t in data_store.keys():\n",
    "    mat, _ = create_functional_data(data_store, t, time_grid=master_grid)\n",
    "    # Convert Prices to Log-Returns\n",
    "    # Log-Return curve R(t) = log(P(t)) - log(P(0)) or similar. \n",
    "    # Standard FDA often uses log-returns relative to opening, or just raw log prices normalized.\n",
    "    # The original script used: np.diff(np.log(prices)) which gives instantaneous returns.\n",
    "    # Let's do cumulative log returns for the curve shape: log(P_t / P_open)\n",
    "    \n",
    "    # Method 1: Daily Curve of Log Prices normalized to start at 0\n",
    "    log_prices = np.log(mat.values)\n",
    "    # Subtract the first column from all columns (start at 0)\n",
    "    curves = log_prices - log_prices[:, 0:1]\n",
    "    \n",
    "    # Method 2: Instantaneous returns (diff) - closer to original script logic\n",
    "    # The original script used `np.diff` on the whole series then meshed.\n",
    "    # Here we take diff along the time axis.\n",
    "    diff_curves = np.diff(log_prices, axis=1)\n",
    "    \n",
    "    func_data[t] = {\n",
    "        'dates': mat.index,\n",
    "        'curves': diff_curves, # Shape (N_days, d-1)\n",
    "        'max_return': np.max(diff_curves, axis=1) # The scalar extreme for that day\n",
    "    }\n",
    "    print(f\"Processed {t}: {diff_curves.shape} days x points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67c128",
   "metadata": {},
   "source": [
    "# 4. FEPLS Analysis: Multiple Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pairs to analyze (Predictor X, Target Y)\n",
    "pairs_to_analyze = [\n",
    "    ('mol.hu.txt', 'otp.hu.txt'),      # Energy predicting Banking\n",
    "    ('otp.hu.txt', 'mol.hu.txt'),      # Banking predicting Energy\n",
    "    ('richter.hu.txt', 'otp.hu.txt'),  # Pharma predicting Banking\n",
    "    ('otp.hu.txt', 'richter.hu.txt'),  # Banking predicting Pharma\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "tau = 1.0\n",
    "valid_k_start = 30 # Increased to 30 as requested\n",
    "\n",
    "for ticker_X, ticker_Y in pairs_to_analyze:\n",
    "    if ticker_X in func_data and ticker_Y in func_data:\n",
    "        print(f\"\\n--- Analyzing Pair: {ticker_X[:-7]} (X) -> {ticker_Y[:-7]} (Y) ---\")\n",
    "        \n",
    "        # Align Data\n",
    "        common_dates = func_data[ticker_X]['dates'].intersection(func_data[ticker_Y]['dates'])\n",
    "        idx_X = func_data[ticker_X]['dates'].isin(common_dates)\n",
    "        idx_Y = func_data[ticker_Y]['dates'].isin(common_dates)\n",
    "        \n",
    "        X_data = func_data[ticker_X]['curves'][idx_X]\n",
    "        Y_data = func_data[ticker_Y]['max_return'][idx_Y]\n",
    "        \n",
    "        # Reshape\n",
    "        X_fepls = np.expand_dims(X_data, axis=0)\n",
    "        Y_fepls = np.expand_dims(Y_data, axis=0)\n",
    "        \n",
    "        n_samples = Y_fepls.shape[1]\n",
    "        d_points = X_fepls.shape[2]\n",
    "        m_threshold = int(n_samples / 5)\n",
    "        \n",
    "        # 1. Correlation Curve\n",
    "        tau = 1.0\n",
    "        valid_k_start = 30 # Increased to 30 as requested\n",
    "\n",
    "        corr_curve = bitcoin_concomittant_corr(X_fepls, Y_fepls, tau, m_threshold)\n",
    "        \n",
    "        # Find best k\n",
    "        if len(corr_curve) > valid_k_start:\n",
    "            best_k = np.argmax(corr_curve[valid_k_start:]) + valid_k_start\n",
    "        else:\n",
    "            best_k = valid_k_start\n",
    "            \n",
    "        # 2. FEPLS Direction (Beta)\n",
    "        Y_sorted = np.sort(Y_fepls[0])[::-1]\n",
    "        y_n = Y_sorted[best_k]\n",
    "        y_matrix = y_n * np.ones_like(Y_fepls)\n",
    "        \n",
    "        E0 = fepls(X_fepls, Y_fepls, y_matrix, tau)\n",
    "        beta_hat = E0[0,:]\n",
    "        \n",
    "        # --- Plotting ---\n",
    "        # We will create a 2x3 grid to show all diagnostics\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        gs = fig.add_gridspec(2, 3)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0, 0]) # Correlation\n",
    "        ax2 = fig.add_subplot(gs[0, 1]) # Hill Plot\n",
    "        ax3 = fig.add_subplot(gs[0, 2]) # QQ Plot\n",
    "        ax4 = fig.add_subplot(gs[1, 0]) # Beta Curve\n",
    "        ax5 = fig.add_subplot(gs[1, 1:]) # Conditional Quantile\n",
    "        \n",
    "        # Plot 1: Correlation\n",
    "        ax1.plot(corr_curve)\n",
    "        ax1.axvline(x=best_k, color='r', linestyle='--', label=f'Best k={best_k}')\n",
    "        ax1.set_title(f'Tail Correlation vs k\\n{ticker_X[:-7]} -> {ticker_Y[:-7]}')\n",
    "        ax1.set_xlabel('Number of Exceedances (k)')\n",
    "        ax1.set_ylabel('Correlation')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Hill Plot\n",
    "        hill_est = get_hill_estimator(Y_sorted)\n",
    "        ax2.plot(hill_est)\n",
    "        ax2.set_title(f'Hill Plot (Tail Index)\\n{ticker_Y[:-7]}')\n",
    "        ax2.set_xlabel('k')\n",
    "        ax2.set_ylabel('Gamma')\n",
    "        ax2.set_xlim(10, m_threshold)\n",
    "        ax2.set_ylim(0, 1.0)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Exponential QQ Plot\n",
    "        qq_data = Exponential_QQ_Plot_1D(Y_fepls, best_k)\n",
    "        slope, intercept, r_val, _, _ = linregress(qq_data[:,0], qq_data[:,1])\n",
    "        ax3.scatter(qq_data[:,0], qq_data[:,1], alpha=0.6)\n",
    "        ax3.plot(qq_data[:,0], intercept + slope*qq_data[:,0], 'r-', label=f'R2={r_val**2:.2f}')\n",
    "        ax3.set_title(f'Exponential QQ Plot (k={best_k})')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Beta Curve\n",
    "        ax4.plot(beta_hat, color='purple')\n",
    "        ax4.set_title(f'FEPLS Direction Beta(t)')\n",
    "        ax4.set_xlabel('Intraday Time (Index)')\n",
    "        ax4.set_ylabel('Weight')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Conditional Quantile\n",
    "        h_univ = 0.2 * np.std(np.dot(X_fepls[0], beta_hat)/d_points) # Increased bandwidth slightly for smoother plot\n",
    "        h_func = 0.2 * np.mean(np.std(X_fepls[0], axis=0))\n",
    "        \n",
    "        h_univ_vec = h_univ * np.ones(n_samples)\n",
    "        h_func_vec = h_func * np.ones(n_samples)\n",
    "        \n",
    "        quantiles, s_grid = plot_quantile_conditional_on_sample_new(\n",
    "            X_fepls, Y_fepls, \n",
    "            dimred=beta_hat,     \n",
    "            x_func=beta_hat,     \n",
    "            alpha=0.95,          \n",
    "            h_univ_vector=h_univ_vec,\n",
    "            h_func_vector=h_func_vec\n",
    "        )\n",
    "        \n",
    "        ax5.plot(s_grid, quantiles[:,0], label='Univariate Est.', linestyle='--')\n",
    "        ax5.plot(s_grid, quantiles[:,1], label='Functional Est.')\n",
    "        ax5.set_title(f'Conditional 95% Quantile')\n",
    "        ax5.set_xlabel(f'Projection <X, Beta>')\n",
    "        ax5.set_ylabel('Quantile Value')\n",
    "        ax5.legend()\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Skipping pair {ticker_X} -> {ticker_Y}: Data not in func_data and ticker_Y or ticker_X (Data Missing)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
