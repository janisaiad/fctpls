\documentclass[11pt,twocolumn]{article}

% --- Packages ---
\usepackage[english]{babel}
\let\Bbbk\relax
\usepackage{amsmath, amsthm, amssymb}
\usepackage{newtxtext,newtxmath}  % Formal Times-like font
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tcolorbox}
\tcbuselibrary{theorems,breakable}
\usepackage{hyperref}
\usepackage[margin=0.3in]{geometry}
\usepackage{float}

% Column spacing for two-column layout
\setlength{\columnsep}{0.5in}  % Increase space between columns

% --- Macros ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\RV}{\text{RV}}
\newcommand{\zp}[1]{\|#1\|}
\newcommand{\f}[1]{\frac{#1}}
\newcommand{\ff}[1]{\frac{1}{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Custom boxes for explanations (adapted for two-column)
\newtcolorbox{explanationbox}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    title=#1,
    fonttitle=\bfseries,
    breakable,
    before skip=0.5\baselineskip,
    after skip=0.5\baselineskip
}

\newtcolorbox{boundbox}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    title=#1,
    fonttitle=\bfseries,
    breakable,
    before skip=0.5\baselineskip,
    after skip=0.5\baselineskip
}

\newtcolorbox{rhobox}[1]{
    colback=orange!5!white,
    colframe=orange!75!black,
    title=#1,
    fonttitle=\bfseries,
    breakable,
    before skip=0.5\baselineskip,
    after skip=0.5\baselineskip
}

% --- Metadata ---
\title{Functional Extreme Partial Least Squares:\\Unraveling the Intuition and Empirical Validation}

\author{Janis Aiad and Simon Elis\\
Master MVA - Statistical Learning with Extreme Values\\
ENS Paris Saclay, Paris, France
\href{GitHub}{https://github.com/janisaiad/FEPLS}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents an empirical analysis of the Functional Extreme Partial Least Squares (FEPLS) method applied to financial time series data. We conduct two complementary analyses: (1) a cross-asset analysis focusing on pairs of stocks from the Hungarian market, specifically examining the relationship between 4IG and AKKO stocks, and (2) a subsampling-based high-frequency analysis of AAPL stock data exploring multiple temporal resolutions from milliseconds to minutes. Both analyses include hypothesis verification for different values of the parameter $\tau$, visualization of the FEPLS direction estimation, and assessment of the method's performance. The theoretical framework is based on the FEPLS methodology which extends Extreme-PLS to functional data in Hilbert spaces, providing dimension reduction for extreme value analysis.
\end{abstract}

\section{Introduction}

Extreme events—rare, significant deviations from normal behavior—play a crucial role in finance, signal processing, risk management, and many scientific fields. The way such extremes manifest in data, especially when those data are high-dimensional or functional (such as entire intraday price curves), presents substantial challenges both for modeling and for prediction. Conventional dimension reduction tools like Principal Component Analysis (PCA) or classical Partial Least Squares (PLS) are effective for understanding average behavior but may fail to capture the patterns associated specifically with rare, high-impact events.

Functional Extreme Partial Least Squares (FEPLS) is a methodology developed to address precisely this challenge. It extends the classical PLS approach into the domain of extremes, targeting the features of high-dimensional or infinite-dimensional covariates that are most informative for explaining or predicting the occurrence of extreme events in a response variable. 

The central research question motivating this work is the following:
\begin{center}
    \emph{What is the most likely shape of the covariate $X$ when the response $Y$ is extreme?}
\end{center}
Answering this question, for instance, can reveal which specific features of a price curve tend to occur in the lead-up to major market moves—a task of paramount importance in financial engineering, but also relevant in fields like signal processing and detection theory, where characterization of rare patterns is required.

In this report, we offer both a theoretical and comprehensive empirical study of the FEPLS framework. We begin by unpacking the intuition and mathematical underpinnings of FEPLS, clarifying how it focuses on conditional extremes, and describing the signal-to-noise assumptions required for consistent estimation. Our goal is to provide a clear narrative that connects the theory to practical considerations.

To bridge theory and practice, we conduct extensive experiments on open financial datasets. These analyses illuminate the strengths and limitations of FEPLS in realistic, noisy, heavy-tailed environments. We also move beyond the day-to-day context of classical financial studies, exploring high-frequency data using a subsampling-based methodology. This allows us to probe FEPLS performance and the validity of its foundational assumptions across a variety of time scales and predictive settings.

Our contributions in this work include:
\begin{itemize}
    \item A concise evaluation of FEPLS's statistical assumptions and estimator stability in real-world data, along with practical guidance on parameter calibration and computational considerations, culminating in the first empirical analysis of FEPLS applied to high-frequency financial data across milliseconds to minutes.
    \item A detailed empirical evaluation of FEPLS under challenging market noise, providing clear illustrations of when and where the method succeeds or faces limitations.
\end{itemize}

Through this combination of theoretical insight, methodological clarification, and practical experimentation, we aim to both advance the understanding of FEPLS and provide a resource for practitioners seeking to apply it in domains where extremes govern the risk landscape.


















\section{Intuitive Understanding of the FEPLS Framework}

\subsection{Difference Between PLS and PCA}

The fundamental distinction between Partial Least Squares (PLS) and Principal Component Analysis (PCA) lies in their use of supervision. See Table~\ref{tab:pca_pls} in the appendix for a detailed comparison.

\textbf{FEPLS (PLS for extremes):}
FEPLS extends this supervised approach to the extreme regime. The fundamental idea is to find a \textbf{single direction} in the infinite-dimensional space of functional covariates that best explains extreme events. Think of it as finding the ``recipe'' for extreme market movements: if you could only look at one linear combination of all the intraday price movements, which combination would best predict tomorrow's crash?

\subsection{The Mathematical Intuition: From Covariance to Direction}

The optimization problem \eqref{eq:fepls} asks: ``Which direction $w$ maximizes the covariance between the projection $\langle w, X \rangle$ and $Y$, \textit{when we only look at extreme values of $Y$}?''

This is different from standard PLS because:
\begin{itemize}
    \item Standard PLS: Maximizes covariance over all data points (average behavior).
    \item FEPLS: Maximizes covariance \textit{conditionally} on $Y \ge y$ (tail behavior).
\end{itemize}

The key insight is that by conditioning on extremes, we focus the optimization on the regime that matters most for risk prediction.
















\section{Notation}

\subsection{Notation}

A summary of the main notation used in the FEPLS framework is provided in Table~\ref{tab:notation} in the appendix.








\section{Theoretical Framework}

\subsection{The FEPLS Problem}

Suppose we observe data pairs $(Y, X)$, where $Y$ is a real-valued variable we care about (for example, the size of a market move), and $X$ contains other observable information (for example, a time series or a function we record alongside $Y$). The central question is: \textbf{Given $X$ and $Y$, can we find a summary of $X$ that best explains the most extreme values of $Y$?}

FEPLS answers this by searching for a direction $w$ in the space of possible summaries of $X$ that is most strongly related to the largest values of $Y$. Formally, for a high threshold $y$, we solve:
\begin{align}
\label{eq:fepls}
w(y) := \argmax_{\zp{w} = 1} \Cov\big( \langle w, X \rangle, Y \mid Y \ge y\big), \quad y \in \R,
\end{align}
where $\langle w, X \rangle$ simply means forming a linear combination of $X$ using $w$ (for instance, by integrating $w(t) X(t)$ if $X$ is a function of $t$), and the $\Cov$ is computed only among the cases where $Y$ exceeds $y$---that is, focusing only on the extremes.


\subsection{Inverse Model: $g$ (Model) vs.\ $\varphi$ (Method)}

The paper \textbf{suppose} the inverse model
\begin{align}
\label{eq:single_index_model}
X = g(Y)\beta + \varepsilon, \quad \beta \in H, \quad \zp{\beta} = 1,
\end{align}
where $g \in \RV_\kappa(+\infty)$ ($\kappa > 0$) is an unknown link function and $\varepsilon: \Omega \to H$ is noise. When $Y$ is extreme, $g(Y)$ tends to dominate $X$, making the signal $g(Y)\beta$ far larger than the noise.

\begin{explanationbox}{What’s fixed and what’s chosen?}
\begin{itemize}
    \item \textbf{$g$ ($\kappa$):} Describes the underlying, real-world relationship between $Y$ and $X$ (imposed by the data-generating process; not under our control). Example: $g(y) = y^{0.5}$.
    \item \textbf{$\varphi$ ($\tau$):} A user-chosen, ``test'' function (tuning parameter); not part of nature, but introduced in the estimator $\hat{\beta}_\varphi$ to optimize statistical performance for extremes.
\end{itemize}
\end{explanationbox}


\begin{rhobox}{Moment Condition and Tuning of $\tau$}\label{rhobox:tau_tuning}
The first key condition, $0 < 2(\kappa+\tau)\gamma < 1$, ensures moments exist for reliable estimation. If $g$ (large $\kappa$) grows too fast, sums like $\sum X_i$ may diverge; picking a decreasing $\varphi$ (negative $\tau$) counteracts this, keeping sums finite.

The estimator uses weights $\varphi(Y_i)$ for extremes, e.g.
\[
\sum X_i\, \varphi(Y_i)\, \mathbf{1}_{\{Y_i \ge y\}}
\]
so tuning $\tau$ adjusts influence: big $\tau$ (positive) focuses on the deepest extremes (low bias, high variance); small or negative $\tau$ spreads weight (better stability).
\end{rhobox}







\subsection{Second-Order Regular Variation}

A new assumption is that the response variable $Y$ is heavy-tailed to the second order. Specifically, the tail quantile function $U(t) := F^-(1-1/t)$ belongs to the class of second-order regularly-varying functions:

\begin{definition}[Second-Order Regular Variation]
The function $U$ belongs to $2\RV_{\gamma,\rho}(+\infty)$ if there exist $\gamma \in (0,1)$, $\rho \le 0$ and an auxiliary function $A$ ultimately of constant sign with $A(t) \to 0$ as $t \to +\infty$ such that:
\begin{align}
\label{hyp:2rv}
\lim_{t \to +\infty} \frac{1}{A(t)}\Big( \frac{U(ty)}{U(t)} - y^{\gamma} \Big) = y^{\gamma} H_\rho(y) := y^{\gamma}\int_1^y u^{\rho-1}
\end{align}
\end{definition}

\paragraph{A variation.}

Although the original paper does not state it explicitly, this definition immediately yields that $A$ is of the form $A(t) = t^{\rho}$ which is important to derive convergence scaling law in \eqref{eq:convergence_scaling_law}.



\begin{rhobox}{Interpretation of $\rho$ 2nd-order RV}

\textbf{Why is Second-Order Regular Variation (2RV) necessary?}

The first-order regular variation ($\RV_\gamma$) tells us that the distribution ``resembles'' a Pareto distribution asymptotically, but in finite samples, we are never truly ``at infinity.'' The distribution is:
\[
\text{True Distribution} = \text{Pareto} + \text{Error}
\]

The 2RV quantifies this error through the auxiliary function $A(t)$. This becomes crucial when choosing $k$:

\begin{itemize}
    \item If $k$ is very small (deep in the tail), the approximation error is small (close to the limit), but the variance is huge (few points).
    \item If $k$ is larger (to stabilize variance), we move away from the extreme tail where the distribution deviates from pure Pareto. The approximation error grows.
\end{itemize}

The 2RV tells us \textit{how fast} this error grows. The condition $\sqrt{k}A(n/k) = O(1)$ balances:
\begin{itemize}
    \item $\sqrt{k}$: The statistical variance (noise)
    \item $A(n/k)$: The model bias (error quantified by 2RV)
\end{itemize}

This condition says: ``You may increase $k$ as long as your model error ($A$) remains smaller than your statistical noise ($\sqrt{k}$).''

In summary: 1RV ($\gamma$) gives the \textit{direction} of the tail (the slope), while 2RV ($\rho$) gives the \textit{straightness} of the tail (is it a perfect straight line in log-log scale, or is it curved?). 
If $\rho$ is very negative, the tail is almost straight, making estimation easier. If $\rho$ is close to 0, convergence is very slow.

\end{rhobox}





\subsection{Signal Dominance over Noise}
The tail of $g(Y)$ (signal channel) must be heavier than the tail of the noise $\varepsilon$. 
In practical terms, this ensures that among the largest observed $Y$ values, the associated signal $g(Y)\beta$ is
 much more extreme than the noise, so the extreme behavior is informative about $\beta$.

\textbf{Why necessary?} If the noise's tail is too fat, its fluctuations swamp the regression signal in the extremes, m
aking recovery impossible even with infinite data. \emph{If $\kappa$ increases (sharper $g$ growth), you can choose larger $k$ and 
variance decreases.}

That is why the model is only identifiable if the signal is dominant over the noise, under the inequality 
\begin{align}
\label{eq:signal_dominance}
q\kappa\gamma > 1.
\end{align}

We call this condition \textbf{signal dominance} for \textbf{identifiability}.
\end{rhobox}







\subsection{What do we estimate to ensure consistency ?}

To ensure that we are into the consistency inequality taht allows signal recovery, we need to estimate both $\gamma$ and $\rho$.
$\kappa$ cannot be estimated from the data.



so there is no best tau choice a priori, we need to construct estimator for a lot of different taus and find an interval 
that contains some stationarity in the beta estimate. We call this procedure \textbf{tau tuning} to satisfy the \textbf{integrability condition}.




\subsubsection{Choice of $k$}

When testing a particular tau, we need to choose a number of extreme observations $k$ to minimize the bias-variance tradeoff.
We call this procedure \textbf{k tuning} for \textbf{convergence}.

\begin{rhobox}{Optimal Choice of $k$}
Following the classical extreme-value theory framework (see Haan2007~\cite[Equation~(3.2.10)]), the optimal choice of $k$ balances bias (governed by 2RV, i.e., $A(n/k)$) and variance (proportional to $k^{-1/2}$):
\begin{quote}
\[
\mathrm{error}(n, k) \approx C_1\,A(n/k) + C_2\,k^{-1/2}
\]
\end{quote}
where $A(n/k)$ controls bias and $k^{-1/2}$ controls the noise variance.

Solving for the optimal tradeoff yields:
\[
k_n \sim c\, n^{-2\rho/(1-2\rho)}
\]
for some $c > 0$ (constant) and $\rho < 0$.
\end{rhobox}
Once you have chosen proper tau, estimated rho and gamma, you have chosen the number of observations to use, you have chosen k.

It remains that you have performed your FEPLS estimation, which convergence guarantee do you have ?





\paragraph{Threshold choice and growth of $y$.}

For any threshold $y \ge 0$, the FEPLS estimator is defined as
\[
\hat\beta_\varphi(y)
  := \frac{\hat v_\varphi(y)}{\|\hat v_\varphi(y)\|}, \qquad
\hat v_\varphi(y)
  = \frac{1}{n} \sum_{i=1}^n X_i \,\varphi(Y_i)\,\mathbf{1}_{\{Y_i \ge y\}}.
\]
Thus, $\hat\beta_\varphi(y)$ is a weighted combination of the $X_i$ whose $Y_i$ are in the tail, with weights set by the test function $\varphi$.

For consistency results, we consider a sequence of (deterministic) thresholds $(y_{n,k})$ such that
\[
y_{n,k} \sim U\!\left(\frac{n}{k}\right),\qquad n\to\infty,
\]
where $U$ is the tail quantile function of $Y$. In practice, $y_{n,k}$ is approximated by the $(n-k+1)$-th order statistic $Y_{n-k+1,n}$:
\[
Y_{n-k+1,n} \approx y_{n,k} \sim U\!\left(\frac{n}{k}\right)
\quad\text{as}\ n\to\infty.
\]

If the tail of $Y$ is regularly varying, i.e., $\bar F(y) = \mathbb{P}(Y>y) \in \mathrm{RV}_{-1/\gamma}(+\infty)$, then
\[
U(t) \sim C\, t^\gamma \quad(t\to\infty)
\]
for some $C>0$; for example, in the standard Pareto, $U(t)\sim t^\gamma$. For $t=n/k$,
\[
y_{n,k} \sim C\left(\frac{n}{k}\right)^{\gamma}.
\]
We consider intermediate sequences $k = k_n \to \infty$ with $k_n/n \to 0$ so $n/k_n \to \infty$, hence
\[
y_{n,k_n} \sim C\left(\frac{n}{k_n}\right)^{\gamma}
\to +\infty\quad\text{as }n\to\infty.
\]
So, the threshold $y$ grows with $n$ and $k_n$:
\[
y = Y_{n-k+1,n} \approx U\!\left(\frac{n}{k}\right)\to+\infty
\]
and more and more extreme observations are used as $n$ increases.



\section{Consistency Results}

The FEPLS estimator is
\begin{align}
\label{eq:estimator}
\hat{\beta}_\varphi(y) := \frac{\hat{v}_\varphi(y)}{\|\hat{v}_\varphi(y)\|}, \quad \text{where} \quad \hat{v}_\varphi(y) = \frac{1}{n} \sum_{i=1}^n X_i \varphi(Y_i) \mathbf{1}_{\{Y_i \ge y\}}.
\end{align}

\begin{theorem}[Consistency of FEPLS Estimator]
\label{thm:consistency}
Under the previous assumptions, the FEPLS estimator is consistent:
\begin{align}
\|\hat{\beta}_\varphi(Y_{n-k+1,n}) - \beta\| = O_{\p}(\delta_{n,k}) \xrightarrow[n \to +\infty]{} 0,
\end{align}
where the convergence rate is
\[
\delta_{n,k} := \left( g(y_{n,k}) \left(\frac{k}{n}\right)^{1/q} \right)^{-1}
\]
with $g$ and $q$ as in Section~\ref{sec:scalinglaw}. When $k \sim c\, n^{-2\rho/(1-2\rho)}$ for $\rho<0$ and $c>0$, and the bias condition $\sqrt{k}A(n/k)=O(1)$ holds, the rate satisfies
\begin{align}
\label{eq:convergence_rate_exact}
\delta_{n,k} = O\left( n^{(1/q-\gamma\kappa)/(1-2\rho)} \right).
\end{align}

Consistency requires (i) signal dominance $q\kappa\gamma > 1$ (signal tail heavier than noise) and (ii) convergence stability $2(\kappa+\tau)\gamma < 1$ (finite variance).
\end{theorem}

The rate $\delta_{n,k}$ depends on the tail of $Y$ (via $\gamma$ and $\rho$), noise integrability $q$, and the link function $g$ (via $\kappa$). See \cite{girard2023} for detailed proof and results.





From now we are all set to perform extensive empirical validation of the FEPLS estimator.



\section{Empirical Validation}
All our reproducible experiments are available in this \href{https://github.com/janisaiad/FEPLS}{repository}.

\subsection{Data Description}

\paragraph{OHLC data:}
We use one year of mid-frequency (5-minute) open-high-low-close (OHLC) market data for Hungary and Poland, sourced from the open and widely-used financial data aggregator \href{https://stooq.com/db/h/}{Stooq}. The dataset covers both the Hungarian (Budapest Stock Exchange) and Polish (Warsaw Stock Exchange) stock indices.

\paragraph{Data Characteristics:}
\begin{itemize}
    \item \textbf{Frequency:} 5-minute bars (OHLC)
    \item \textbf{Markets:} Hungary (\texttt{BUX}), Poland (\texttt{WIG20})
    \item \textbf{Size:} Approximately 50 megabytes compressed for 1 year (per market)
    \item \textbf{Coverage:} Full calendar year (about 50,000--60,000 bars per market)
    \item \textbf{Format:} CSV, standardized with columns for open, high, low, close, and volume
    \item \textbf{Source:} Freely available and reproducible from \href{https://stooq.com/db/h/}{https://stooq.com/db/h/}
\end{itemize}

The analysis uses X as the daily return curve from a stock A and Y as the next day's max return from a stock B.
So we reproduce the analysis from the paper using open-source data.


\paragraph{Tick-by-tick data:}

For the most granular analysis, we purchased 3 months of nanosecond-resolution tick-by-tick data from Databento, 
covering all limit order book events (including every limit order, cancelation, and execution) for p
rominent NASDAQ stocks: Google, Apple, American Airlines (AAL), Amazon, and Microsoft. 
This dataset, considered extremely high quality, cost approximately \$1000 and represents a gold standard for empirical financial 
microstructure research.

\begin{itemize}
    \item \textbf{Frequency:} Nanosecond-scale tick-by-tick (full limit order book)
    \item \textbf{Markets:} NASDAQ (Google, Apple, American Airlines (AAL), Amazon, Microsoft)
    \item \textbf{Duration:} 3 months
    \item \textbf{Events:} All limit order submissions, cancellations, executions (Level 3 order book data)
    \item \textbf{Size/Cost:} Approximately \$1,000 for the full period via Databento
    \item \textbf{Quality:} Highest market microstructure fidelity available
    \item \textbf{Source:} Commercially obtained from \href{https://databento.com/}{Databento} from previous work.
\end{itemize}


\subsection{Statistical Workflow for Empirical Validation}

We adhere to a transparent, reproducible workflow for FEPLS model validation using both Stooq (5-minute OHLC) and Databento (tick) data. The procedure is as follows:

\begin{enumerate}
    \item \textbf{Data Preparation:} Download raw data (Stooq or Databento). Select target stocks and extract daily return curves $X$ (covariate) and the scalar $Y$ (next-day maximum return), possibly from different stocks to capture cross-dependence.
    \item \textbf{Cleaning and Log Return Construction:} Compute 5-minute (or tick) log-returns, handle missing values (e.g., using linear interpolation or LOCF), and align data to uniform time grids.
    \item \textbf{Dependence Check:} Assess temporal dependence (volatility clustering) in extremes via autocorrelation plots; optionally thin the data by taking every $n$th day to reduce dependence.
    \item \textbf{Heavy-Tail Verification:} Plot the Hill estimator for $Y$ to confirm a heavy-tailed regime appropriate for FEPLS. Proceed only if the estimated tail index $\gamma$ is positive.
    \item \textbf{Correlation Check (Optional):} Compute canonical or ordinary correlation between projections $\langle X, \beta \rangle$ and $Y$ to confirm a tractable signal for dimension reduction.
    \item \textbf{Train/Test Chronological Split:} Split the data into training and testing periods. \textbf{ $\tau$ tuning} is carried out on the training set.
    \item \textbf{Parameter Calibration:}
    \begin{enumerate}
        \item Select a conservative test function parameter $\tau$ (e.g., $\tau = -1$).
        \item Optimize the number of extremes $k$ via a grid search: for each $k$, estimate $\hat{\beta}_{\tau, k}$ on the training set and maximize the in-extremes correlation between projections and $Y$.
    \end{enumerate}
    \item \textbf{Validation:} 
    \begin{itemize}
        \item Apply the fitted $\hat{\beta}$ to the test set.
        \item Project new curves $X^{\text{test}}$ onto $\hat{\beta}$ to obtain scalar scores.
        \item Estimate conditional Value-at-Risk (VaR) via quantile regression or local smoothing.
        \item Assess visual coverage (exceedance) and independence of VaR violations.
    \end{itemize}
\end{enumerate}

This workflow ensures both statistical rigor and full reproducibility for all empirical FEPLS validation results.













\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/4ig_akko_tau_comparison.png}
    \caption{Empirical validation of FEPLS consistency: comparison across different $\tau$ values showing hypothesis verification, correlation analysis, and optimal threshold selection for the 4IG $\to$ AKKO pair.}
    \label{fig:consistency_validation}
    \end{figure}
    
    
    
































\section{Empirical Analysis: 4IG and AKKO Stock Pairs}

\subsection{Data Description}

We analyze the relationship between pairs of stocks from the Hungarian market, focusing on the 4IG and AKKO stocks. The data consists of:
\begin{itemize}
    \item Functional covariates $X_i$: intraday log-return curves for one stock
    \item Scalar responses $Y_i$: daily maximum log-returns for another stock
    \item The analysis is performed in both directions: 4IG $\to$ AKKO and AKKO $\to$ 4IG
\end{itemize}

The dataset is split into training (80\%) and testing (20\%) sets to assess the predictive performance of the FEPLS method.

\subsection{Methodology}

For each pair, we:
\begin{enumerate}
    \item Estimate the tail index $\gamma$ and the link function parameter $\kappa$ on the training set
    \item Test multiple values of $\tau \in \{-3.0, -2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0\}$
    \item For each $\tau$, compute the FEPLS direction $\hat{\beta}_\varphi$ and verify the hypothesis condition: $0 < 2(\kappa + \tau)\gamma < 1$
    \item Select the optimal threshold $k$ using the sharpness criterion on the correlation curve
    \item Evaluate the correlation between projections and responses on both training and test sets
    \item Generate comprehensive visualizations
\end{enumerate}

\subsection{Results for 4IG $\to$ AKKO}

Figure~\ref{fig:4ig_akko_comparison} shows the comparison across different $\tau$ values for the 4IG $\to$ AKKO pair. The analysis reveals:

\begin{itemize}
    \item Several $\tau$ values satisfy the theoretical hypothesis condition
    \item The train and test correlations vary with $\tau$, indicating the sensitivity of the method to this parameter
    \item The optimal threshold $k$ (number of exceedances) is selected adaptively for each $\tau$
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/4ig_akko_tau_comparison.png}
\caption{Comparison of FEPLS results across different $\tau$ values for the 4IG $\to$ AKKO pair. Top left: Hypothesis value $2(\kappa + \tau)\gamma$ vs $\tau$ (green = valid, red = invalid). Top right: Train and test correlations vs $\tau$. Bottom left: Optimal threshold $k$ vs $\tau$. Bottom right: Summary table with all results.}
\label{fig:4ig_akko_comparison}
\end{figure}

Figure~\ref{fig:4ig_akko_tau0} presents a detailed analysis for $\tau = 0.0$, showing:
\begin{itemize}
    \item The tail correlation curve and selected threshold
    \item The Hill plot for tail index estimation
    \item The exponential QQ plot for goodness-of-fit assessment
    \item The estimated FEPLS direction $\hat{\beta}(t)$
    \item The conditional quantile plot with scatter of extreme vs non-extreme observations
    \item The hypothesis verification summary
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/4ig_akko_tau_0.0.png}
\caption{Detailed FEPLS analysis for 4IG $\to$ AKKO with $\tau = 0.0$. The plot shows correlation curves, Hill estimator, QQ plot, estimated direction, conditional quantiles, and hypothesis verification.}
\label{fig:4ig_akko_tau0}
\end{figure}

\subsection{Results for AKKO $\to$ 4IG}

The reverse direction (AKKO $\to$ 4IG) is analyzed in Figure~\ref{fig:akko_4ig_comparison}. This analysis provides insights into the asymmetric nature of the relationship between these two stocks.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/akko_4ig_tau_comparison.png}
\caption{Comparison of FEPLS results across different $\tau$ values for the AKKO $\to$ 4IG pair.}
\label{fig:akko_4ig_comparison}
\end{figure}

Figure~\ref{fig:akko_4ig_tau-1} shows the detailed analysis for $\tau = -1.0$ in the AKKO $\to$ 4IG direction.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/akko_4ig_tau_-1.0.png}
\caption{Detailed FEPLS analysis for AKKO $\to$ 4IG with $\tau = -1.0$.}
\label{fig:akko_4ig_tau-1}
\end{figure}

\subsection{Additional Analysis for Different $\tau$ Values}

To illustrate the sensitivity to the parameter $\tau$, we present additional results for different values. Figure~\ref{fig:4ig_akko_tau-05} shows the analysis for $\tau = -0.5$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/4ig_akko_tau_-0.5.png}
\caption{Detailed FEPLS analysis for 4IG $\to$ AKKO with $\tau = -0.5$.}
\label{fig:4ig_akko_tau-05}
\end{figure}





























\section{Empirical Analysis: Subsampling-Based Approach on High-Frequency Data}

\subsection{Data Description and Methodology}

We extend the FEPLS analysis to high-frequency financial data using a subsampling-based approach. This analysis focuses on AAPL stock data and employs a fundamentally different data structure compared to the previous analysis:

\begin{itemize}
    \item \textbf{Functional covariates $X_i$}: Each $X_i$ consists of $d$ consecutive subsampled mid prices, i.e., $X_i = [x_i, x_{i+1}, \ldots, x_{i+d-1}]$ where $x_j$ represents the mid price at time $j$ after subsampling at a given time interval.
    \item \textbf{Scalar responses $Y_i$}: Each $Y_i$ is the maximum of the next $k$ subsampled prices, i.e., $Y_i = \max(x_{i+d}, x_{i+d+1}, \ldots, x_{i+d+k-1})$.
    \item \textbf{Objective}: Predict extreme future price movements (maxima over a horizon $k$) based on recent price patterns (window of size $d$).
\end{itemize}

This approach allows us to study how different temporal resolutions and prediction horizons affect the FEPLS method's performance. The analysis tests multiple configurations:

\begin{itemize}
    \item \textbf{Subsampling intervals}: 5 minutes, 1 minute, 5 seconds, 1 second, 500ms, 100ms, 50ms, 10ms, 5ms, 1ms
    \item \textbf{Dimensions $d$}: 10, 20, 50, 100 (length of the input window)
    \item \textbf{Prediction horizons $k$}: 5, 10, 20, 50 (number of future prices to consider for the maximum)
    \item \textbf{Tau values}: $\tau \in \{-3.0, -2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0\}$
\end{itemize}

The data is processed day by day, with batches created across multiple trading days to ensure sufficient sample size (targeting 100-5000 batches per configuration).

\subsection{Key Differences from Previous Analysis}

This subsampling-based approach differs from the 4IG/AKKO analysis in several important ways:

\begin{enumerate}
    \item \textbf{Data source}: Uses high-frequency tick-by-tick data (subsampled) rather than 5-minute aggregated data
    \item \textbf{Functional structure}: The functional covariate $X$ represents a sliding window of consecutive prices rather than intraday return curves
    \item \textbf{Response variable}: Predicts the maximum of future prices rather than daily maximum returns
    \item \textbf{Temporal resolution}: Explores multiple time scales from milliseconds to minutes
    \item \textbf{Univariate focus}: Analyzes a single stock (AAPL) with self-prediction rather than cross-asset relationships
\end{enumerate}

\subsection{Results Overview}

The comprehensive analysis across all configurations generates results for each combination of subsampling interval, dimension $d$, and prediction horizon $k$. For each configuration, we:

\begin{enumerate}
    \item Estimate the tail index $\gamma$ and link function parameter $\kappa$
    \item Test all $\tau$ values and verify the hypothesis condition $0 < 2(\kappa + \tau)\gamma < 1$
    \item Compute FEPLS directions $\hat{\beta}_\varphi$ for each valid $\tau$
    \item Select optimal thresholds $k$ using the sharpness criterion
    \item Generate comprehensive visualizations including correlation curves, Hill plots, QQ plots, and conditional quantile estimates
\end{enumerate}

\subsection{Example Results: Fine Time Scale Analysis}

Figure~\ref{fig:aapl_subsampling_example} presents an example of the analysis for a fine time scale configuration (1ms subsampling, $d=20$, $k=10$). This configuration examines very short-term price patterns and their relationship to immediate future extremes.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/AAPL_1ms_d20_k10_tau_comparison.png}
\caption{Tau comparison for AAPL with 1ms subsampling, dimension $d=20$, and prediction horizon $k=10$. The plot shows hypothesis verification, correlation analysis, and optimal threshold selection across different $\tau$ values.}
\label{fig:aapl_subsampling_example}
\end{figure}

\subsection{Example Results: Coarse Time Scale Analysis}

Figure~\ref{fig:aapl_coarse_example} shows results for a coarser time scale (1 second subsampling, $d=50$, $k=20$), which captures longer-term patterns in price movements.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/AAPL_1sec_d50_k20_tau_comparison.png}
\caption{Tau comparison for AAPL with 1 second subsampling, dimension $d=50$, and prediction horizon $k=20$.}
\label{fig:aapl_coarse_example}
\end{figure}

\subsection{Detailed Analysis for Selected Configurations}

For selected configurations, we provide detailed analyses similar to the 4IG/AKKO study. Figure~\ref{fig:aapl_detailed_example} shows a comprehensive analysis for a specific $\tau$ value, including:

\begin{itemize}
    \item Tail correlation curve with optimal threshold selection
    \item Hill plot for tail index estimation
    \item Exponential QQ plot for goodness-of-fit assessment
    \item Estimated FEPLS direction $\hat{\beta}(t)$ showing which time points in the window are most predictive
    \item Conditional quantile plot with scatter of extreme vs non-extreme observations
    \item Hypothesis verification summary
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.3\textwidth]{figures/AAPL_100ms_d20_k10_tau_0.0.png}
\caption{Detailed FEPLS analysis for AAPL with 100ms subsampling, $d=20$, $k=10$, and $\tau = 0.0$. The plot shows all diagnostic components including correlation curves, Hill estimator, QQ plot, estimated direction, conditional quantiles, and hypothesis verification.}
\label{fig:aapl_detailed_example}
\end{figure}

\subsection{Insights from Multi-Scale Analysis}

The multi-scale analysis reveals several important patterns:

\begin{itemize}
    \item \textbf{Time scale effects}: The optimal $\tau$ values and hypothesis validity vary with the subsampling interval, suggesting that different time scales require different tuning parameters.
    
    \item \textbf{Dimension effects}: Larger dimensions $d$ (longer input windows) may capture more complex patterns but require more data to estimate reliably.
    
    \item \textbf{Prediction horizon effects}: The relationship between input patterns and future extremes depends on the prediction horizon $k$, with shorter horizons potentially showing stronger relationships.
    
    \item \textbf{Hypothesis validity}: The condition $0 < 2(\kappa + \tau)\gamma < 1$ is satisfied for different $\tau$ ranges depending on the configuration, highlighting the importance of parameter selection.
\end{itemize}

\subsection{Comparison Across Configurations}

The analysis generates a comprehensive summary table (saved as \texttt{analysis\_summary.csv}) that tracks:

\begin{itemize}
    \item Configuration parameters (interval, $d$, $k$)
    \item Number of batches successfully created
    \item Number of valid $\tau$ values for each configuration
    \item Status of each analysis (completed, insufficient data, or error)
\end{itemize}

This summary allows for systematic comparison of FEPLS performance across different temporal resolutions and prediction settings, providing insights into the method's robustness and applicability to high-frequency financial data.

\section{Discussion}

\subsection{Key Findings}

The empirical analysis reveals several important insights from both the cross-asset analysis (4IG/AKKO) and the subsampling-based high-frequency analysis (AAPL):

\begin{enumerate}
    \item \textbf{Hypothesis Verification}: Not all values of $\tau$ satisfy the theoretical bounds $0 < 2(\kappa + \tau)\gamma < 1$. This is expected and highlights the importance of verifying assumptions before applying the method. The valid range of $\tau$ values varies across different data structures and time scales.
    
    \item \textbf{Parameter Sensitivity}: The choice of $\tau$ significantly affects both the estimated FEPLS direction and the predictive performance. This suggests that careful selection of $\tau$ is crucial in practice. The subsampling analysis shows that optimal $\tau$ values may depend on the temporal resolution of the data.
    
    \item \textbf{Asymmetry}: The relationship between 4IG and AKKO appears asymmetric, as evidenced by different results in the two directions (4IG $\to$ AKKO vs AKKO $\to$ 4IG). This suggests that lead-lag relationships in financial markets are directional.
    
    \item \textbf{Generalization}: The test set correlations provide evidence of the method's ability to generalize, though the performance varies with $\tau$. The subsampling approach demonstrates that FEPLS can be applied across different time scales, from milliseconds to minutes.
    
    \item \textbf{Time Scale Effects}: The subsampling analysis reveals that the relationship between input patterns and future extremes depends on the temporal resolution. Fine time scales (milliseconds) may capture different patterns than coarse scales (seconds or minutes), requiring different tuning parameters.
    
    \item \textbf{Window Size and Prediction Horizon}: The dimension $d$ (input window size) and prediction horizon $k$ significantly affect the analysis. Larger windows may capture more complex patterns but require more data, while the optimal prediction horizon depends on the specific application and time scale.
\end{enumerate}

\subsection{Interpretation of the FEPLS Direction}

The estimated FEPLS direction $\hat{\beta}(t)$ represents the functional pattern in the intraday returns that is most predictive of extreme daily maximum returns. The shape of $\hat{\beta}(t)$ reveals:
\begin{itemize}
    \item Which parts of the trading day are most informative for predicting extremes
    \item The relative importance of different time periods within the day
    \item Potential lead-lag relationships between the two assets
\end{itemize}

\subsection{Practical Considerations}

\begin{itemize}
    \item \textbf{Threshold Selection}: The adaptive selection of $k$ using the sharpness criterion appears to work well in practice, though it may require tuning for different datasets.
    
    \item \textbf{Computational Efficiency}: The method is computationally tractable even for high-dimensional functional data, making it suitable for real-time applications.
    
    \item \textbf{Robustness}: The method shows reasonable robustness to the choice of $\tau$ within the valid range, though optimal performance may require cross-validation.
\end{itemize}

\section{Why FEPLS Works: Intuition Behind the Conditions}

\subsection{Understanding the Two Key Inequalities}

The consistency of FEPLS relies on two fundamental inequalities that have clear intuitive interpretations:

\subsubsection{Condition 1: Signal Dominance ($q\kappa\gamma > 1$)}

\textbf{The Question:} In extreme events, what causes large values of $X$? Is it the fundamental mechanism ($Y$ via $g$) or just random ``bad luck'' (the noise $\varepsilon$)?

\textbf{The Answer:} The condition $q\kappa\gamma > 1$ ensures that the signal dominates the noise.

\textbf{The Mechanics:}
\begin{itemize}
    \item $\kappa\gamma$ is the tail index of the signal $g(Y)$.
    \item $1/q$ is (roughly) the tail index of the noise $\|\varepsilon\|$.
    \item The condition literally says: \textbf{Tail of Signal $>$ Tail of Noise}.
\end{itemize}

\textbf{Consequence:} If this condition is not satisfied, when you look at the largest $X$ values, you capture only random noise. Your $\hat{\beta}$ would point in a random direction, and the method would fail.

\textbf{In Practice:} This is why we need to verify that the relationship between $X$ and $Y$ is strong enough in the tail. If the noise is too heavy, FEPLS cannot extract meaningful information.

\subsubsection{Condition 2: Convergence Stability ($2(\kappa + \tau)\gamma < 1$)}

\textbf{The Question:} Why does the estimator converge? What prevents it from being unstable?

\textbf{The Answer:} The condition $2(\kappa + \tau)\gamma < 1$ ensures that the variance of the estimator remains finite.

\textbf{The Mechanics:}
\begin{itemize}
    \item The estimator computes a sum of terms that look like $X \cdot \varphi(Y)$.
    \item Since $X \approx Y^\kappa$ and $\varphi(Y) \approx Y^\tau$, the term is of order $Y^{\kappa+\tau}$.
    \item To compute a variance (squared standard deviation), we square this: $Y^{2(\kappa+\tau)}$.
    \item For the expectation of this square to be finite (integral converges), the probability tail of $Y$ (driven by $\gamma$) must decay faster than this term explodes.
\end{itemize}

\textbf{Consequence:} If this condition is not satisfied, the integral diverges. Numerically, your $\hat{\beta}$ would be unstable: adding a single new extreme datum could completely change the direction of the vector.

\textbf{The Role of $\tau$:} This is where your choice of $\tau$ (negative) acts as a brake to satisfy the inequality. If the natural link $g$ (parameter $\kappa$) is too explosive, you choose a negative $\tau$ to ``calm'' the integral.

\subsection{Why Second-Order Regular Variation is Necessary}

The first-order regular variation ($\RV_\gamma$) tells us that the distribution ``resembles'' a Pareto distribution asymptotically. But in finite samples, we are never truly ``at infinity.'' The distribution is:
\[
\text{True Distribution} = \text{Pareto} + \text{Error}
\]

The second-order regular variation quantifies this error through the auxiliary function $A(t)$. This becomes crucial when choosing $k$:

\begin{itemize}
    \item If $k$ is very small (deep in the tail), the approximation error is small (close to the limit), but the variance is huge (few points).
    \item If $k$ is larger (to stabilize variance), we move away from the extreme tail where the distribution deviates from pure Pareto. The approximation error grows.
\end{itemize}

The 2RV tells us \textit{how fast} this error grows. The condition $\sqrt{k}A(n/k) = O(1)$ balances:
\begin{itemize}
    \item $\sqrt{k}$: The statistical variance (noise)
    \item $A(n/k)$: The model bias (error quantified by 2RV)
\end{itemize}

This condition says: ``You may increase $k$ as long as your model error ($A$) remains smaller than your statistical noise ($\sqrt{k}$).''

\textbf{Summary:}
\begin{itemize}
    \item 1RV ($\gamma$) = The \textit{direction} of the tail (the slope in log-log scale)
    \item 2RV ($\rho$) = The \textit{straightness} of the tail (is it a perfect straight line, or is it curved?)
    \item If $\rho$ is very negative, the tail is almost straight, making estimation easier
    \item If $\rho$ is close to 0, convergence is very slow
\end{itemize}

\subsection{Why the Method Might Fail: Common Pitfalls}

Understanding when and why FEPLS might not work is crucial for practical applications:

\begin{enumerate}
    \item \textbf{Violation of Signal Dominance:} If $q\kappa\gamma \le 1$, the noise is too heavy. The method will find random directions that don't generalize.
    
    \item \textbf{Violation of Convergence Condition:} If $2(\kappa + \tau)\gamma \ge 1$, the estimator is unstable. Small changes in data lead to large changes in $\hat{\beta}$.
    
    \item \textbf{Insufficient Tail Data:} If $k$ is too small, there aren't enough extreme observations to reliably estimate the direction.
    
    \item \textbf{Non-Heavy-Tailed Distribution:} If $\gamma$ is too small or negative, the distribution is not in the Fréchet domain. FEPLS is not designed for this case.
    
    \item \textbf{Weak Relationship:} If there's no meaningful relationship between $X$ and $Y$ in the tail, the method will find spurious correlations.
    
    \item \textbf{Serial Dependence:} If observations are strongly dependent (e.g., volatility clustering), the i.i.d. assumptions are violated, and the theoretical guarantees don't hold.
\end{enumerate}




















\section*{Acknowledgments}

We thank Charles-Albert Lehalle for providing the Databento data.

\onecolumn
\section{Appendix}

\subsection{Comparison between PCA and PLS}

\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \footnotesize
    \begin{tabular}{p{3.5cm} p{4.5cm} p{4.5cm}}
    \toprule
     & \textbf{PCA (Unsupervised)} & \textbf{PLS (Supervised)} \\
    \midrule
    \textbf{Uses response variable?} 
        & No: PCA operates solely on the covariates $X$, ignoring any relationship with a response variable $Y$.
        & Yes: PLS explicitly incorporates the response $Y$ in the optimization. \\
    \textbf{Objective} 
        & Find directions that maximize the \emph{variance} of the projected data, i.e., $\argmax_{\|w\|=1} \Var(\langle w, X \rangle)$.
        & Find directions that maximize the \emph{covariance} between the projection and response, i.e., $\argmax_{\|w\|=1} \Cov(\langle w, X \rangle, Y)$.\\
    \textbf{Interpretation} 
        & The first principal component captures the direction of maximum variance in $X$, regardless of its relevance to predicting $Y$.
        & The PLS direction captures the direction in $X$ most correlated with $Y$, making it directly relevant to prediction. \\
    \textbf{Strengths/Limitations} 
        & Limitation: In regression contexts, high-variance directions can be orthogonal to the relationship with $Y$, so PCA may prioritize irrelevant directions.
        & Advantage: By focusing on covariance with $Y$, PLS finds directions that are both informative about $X$ and predictive of $Y$. \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between PCA (Principal Component Analysis) and PLS (Partial Least Squares).}
    \label{tab:pca_pls}
    \end{table}
    
    \begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \footnotesize
    \begin{tabular}{p{2.5cm} p{5cm} p{2.5cm} p{5cm}}
    \toprule
    \multicolumn{2}{c}{\textbf{Column 1}} & \multicolumn{2}{c}{\textbf{Column 2}} \\
    \cmidrule(lr){1-2} \cmidrule(lr){3-4}
    \textbf{Symbol} & \textbf{Explanation} & \textbf{Symbol} & \textbf{Explanation} \\
    \midrule
    \multicolumn{4}{l}{\textbf{Regular Variation \& Auxiliary Functions}} \\
    $\RV_\tau(+\infty)$ & Regularly-varying (index $\tau$) & $2\RV_{\gamma, \rho}(+\infty)$ & Second-order RV \\
    $H_\rho(y)$ & $y^{\gamma}\int_1^y u^{\rho-1}\mathrm{d}u$ & $A(t)$ & Auxiliary function, $A(t) \to 0$ \\
    \midrule
    \multicolumn{4}{l}{\textbf{Model and FEPLS Notation}} \\
    $X = g(Y)\beta + \varepsilon$ & Inverse regression model & $\beta \in H$, $\zp{\beta}=1$ & Index vector (unit norm) \\
    $g \in \RV_\kappa(+\infty)$ & Link function (index $\kappa$) & $\varepsilon: \Omega \to H$ & Noise term \\
    $\varphi \in \RV_\tau(+\infty)$ & Test function (index $\tau$) & $w_\varphi(y)$ & Theoretical FEPLS direction \\
    $\hat{\beta}_\varphi(y)$ & Empirical FEPLS estimator & & \\
    \midrule
    \multicolumn{4}{l}{\textbf{Key Parameters}} \\
    $\gamma \in (0,1)$ & Tail index of $Y$ & $\rho \le 0$ & Second-order parameter \\
    $\kappa > 0$ & Link/model function index & $\tau \in \R$ & Test function index \\
    $k=k_n$ & Intermediate sequence & $y_{n,k} \sim U(n/k)$ & High threshold \\
    $\delta_{n,k}$ & $(g(y_{n,k})(k/n)^{1/q})^{-1}$ & $q > 2$ & Noise integrability order \\
    \bottomrule
    \end{tabular}
    \caption{Summary of main notation used in the FEPLS framework.}
    \label{tab:notation}
    \end{table}

\twocolumn

% --- Bibliography ---
\begin{thebibliography}{9}

\bibitem{girard2023}
Girard, S., and Pakzad, A. 2023.
\textit{Functional Extreme Partial Least Squares}.
Journal of Multivariate Analysis.

\bibitem{peyre2019}
Peyré, G., and Cuturi, M. 2019.
\textit{Computational optimal transport: With applications to data science}.
Foundations and Trends® in Machine Learning, 11(5-6), 355--607.

\bibitem{sejourne2019}
Séjourné, T., Feydy, J., Vialard, F. X., Trouvé, A., and Peyré, G. 2019.
\textit{Sinkhorn divergences for unbalanced optimal transport}.
arXiv preprint arXiv:1910.12958.

\bibitem{hill1975}
Hill, B. M. 1975.
\textit{A simple general approach to inference about the tail of a distribution}.
The Annals of Statistics, 3(5), 1163--1174.

\bibitem{de2007}
de Haan, L., and Ferreira, A. 2007.
\textit{Extreme value theory: an introduction}.
Springer Science \& Business Media.

\bibitem{resnick2007}
Resnick, S. I. 2007.
\textit{Heavy-tail phenomena: probabilistic and statistical modeling}.
Springer Science \& Business Media.

\bibitem{embrechts2013}
Embrechts, P., Klüppelberg, C., and Mikosch, T. 2013.
\textit{Modelling extremal events: for insurance and finance}.
Springer Science \& Business Media.

\bibitem{ramsay2005}
Ramsay, J. O., and Silverman, B. W. 2005.
\textit{Functional data analysis}.
Springer Science \& Business Media.

\bibitem{horvath2012}
Horváth, L., and Kokoszka, P. 2012.
\textit{Inference for functional data with applications}.
Springer Science \& Business Media.

\end{thebibliography}

\end{document}

