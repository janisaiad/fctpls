\documentclass[10pt]{article}

% --- Packages ---
\usepackage[english]{babel}
\let\Bbbk\relax
\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tcolorbox}
\tcbuselibrary{theorems,breakable}
\usepackage{hyperref}
\usepackage{float}

% ICML-style formatting
\usepackage[margin=0.75in,columnsep=0.25in]{geometry}
\usepackage{microtype}  % Better typography
\usepackage{mathptmx}  % Times font with math support for ICML style

% Column spacing for two-column layout
\setlength{\columnsep}{0.25in}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5\baselineskip}

% --- Macros ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\RV}{\text{RV}}
\newcommand{\zp}[1]{\|#1\|}
\newcommand{\f}[1]{\frac{#1}}
\newcommand{\ff}[1]{\frac{1}{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

% Theorem environments (ICML style)
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Section formatting (ICML style)
\makeatletter
\renewcommand\section{\@startsection {section}{1}{\z@}%
                                   {-3.5ex \@plus -1ex \@minus -.2ex}%
                                   {2.3ex \@plus.2ex}%
                                   {\normalfont\large\bfseries}}
\renewcommand\subsection{\@startsection{subsection}{2}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                                     {-3.25ex\@plus -1ex \@minus -.2ex}%
                                     {1.5ex \@plus .2ex}%
                                     {\normalfont\normalsize\bfseries}}
\makeatother

% Custom boxes for explanations (adapted for two-column, ICML style)
\newtcolorbox{explanationbox}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    title=#1,
    fonttitle=\bfseries\small,
    fontupper=\small,
    breakable,
    before skip=0.3\baselineskip,
    after skip=0.3\baselineskip,
    left=2pt,
    right=2pt,
    top=2pt,
    bottom=2pt
}

\newtcolorbox{boundbox}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    title=#1,
    fonttitle=\bfseries\small,
    fontupper=\small,
    breakable,
    before skip=0.3\baselineskip,
    after skip=0.3\baselineskip,
    left=2pt,
    right=2pt,
    top=2pt,
    bottom=2pt
}

\newtcolorbox{rhobox}[1]{
    colback=orange!5!white,
    colframe=orange!75!black,
    title=#1,
    fonttitle=\bfseries\small,
    fontupper=\small,
    breakable,
    before skip=0.3\baselineskip,
    after skip=0.3\baselineskip,
    left=2pt,
    right=2pt,
    top=2pt,
    bottom=2pt
}

% --- Metadata ---
\title{Functional Extreme Partial Least Squares:\\Unraveling the Intuition and Empirical Validation}

\author{
\small
Janis Aiad$^1$ \quad Simon Elis$^1$ \\
$^1$Master MVA - Statistical Learning with Extreme Values\\
ENS Paris Saclay - Ecole Polytechnique - ENS ULM, Paris, France\\
\href{https://github.com/janisaiad/fctpls}{\texttt{github.com/janisaiad/fctpls}}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
\small
We present an in-depth study of the Functional Extreme Partial Least Squares (FEPLS) framework, a method designed to identify predictive features in high-dimensional or functional data associated with rare, extreme events. Our work applies the FEPLS approach to financial time series, with a particular focus on understanding how patterns in one asset's intraday returns anticipate large, infrequent moves in another. We empirically evaluate FEPLS in two settings: (1) cross-asset analysis using medium-frequency stock data, and (2) high-frequency financial data via a subsampling methodology. The results provide new insights into the strengths and limitations of FEPLS, practical guidelines for its calibration, and its applicability across a range of financial regimes.
\end{abstract}

\section{Introduction}

Extreme events—rare, significant deviations from normal behavior—play a crucial role in finance, signal processing, risk management, and many scientific fields. The way such extremes manifest in data, especially when those data are high-dimensional or functional (such as entire intraday price curves), presents substantial challenges both for modeling and for prediction. Conventional dimension reduction tools like Principal Component Analysis (PCA) or classical Partial Least Squares (PLS) are effective for understanding average behavior but may fail to capture the patterns associated specifically with rare, high-impact events.

Functional Extreme Partial Least Squares (FEPLS) is a methodology developed to address precisely this challenge. It extends the classical PLS approach into the domain of extremes, targeting the features of high-dimensional or infinite-dimensional covariates that are most informative for explaining or predicting the occurrence of extreme events in a response variable. 

The central research question motivating this work is the following:
\begin{center}
    \emph{What is the most likely shape of the covariate $X$ when the response $Y$ is extreme?}
\end{center}
Answering this question, for instance, can reveal which specific features of a price curve tend to occur in the lead-up to major market moves—a task of paramount importance in financial engineering, but also relevant in fields like signal processing and detection theory, where characterization of rare patterns is required.

In this report, we offer both a theoretical and comprehensive empirical study of the FEPLS framework. We begin by unpacking the intuition and mathematical underpinnings of FEPLS, clarifying how it focuses on conditional extremes, and describing the signal-to-noise assumptions required for consistent estimation. Our goal is to provide a clear narrative that connects the theory to practical considerations.

To bridge theory and practice, we conduct extensive experiments on open financial datasets. These analyses illuminate the strengths and limitations of FEPLS in realistic, noisy, heavy-tailed environments. We also move beyond the day-to-day context of classical financial studies, exploring high-frequency data using a subsampling-based methodology. This allows us to probe FEPLS performance and the validity of its foundational assumptions across a variety of time scales and predictive settings.

Our contributions in this work include:
\begin{itemize}
    \item A concise evaluation of FEPLS's statistical assumptions and estimator stability in real-world data, along with practical guidance on parameter calibration and computational considerations, culminating in the first empirical analysis of FEPLS applied to high-frequency financial data across milliseconds to minutes.
    \item A detailed empirical evaluation of FEPLS under challenging market noise, providing clear illustrations of when and where the method succeeds or faces limitations.
\end{itemize}

Through this combination of theoretical insight, methodological clarification, and practical experimentation, we aim to both advance the understanding of FEPLS and provide a resource for practitioners seeking to apply it in domains where extremes govern the risk landscape.


















\section{Intuitive Understanding of the FEPLS Framework}

\subsection{Difference Between PLS and PCA}

The fundamental distinction between Partial Least Squares (PLS) and Principal Component Analysis (PCA) lies in their use of supervision. See Table~\ref{tab:pca_pls} in the appendix for a detailed comparison.

\textbf{FEPLS (PLS for extremes):}
FEPLS extends this supervised approach to the extreme regime. The fundamental idea is to find a \textbf{single direction} in the infinite-dimensional space of functional covariates that best explains extreme events. Think of it as finding the ``recipe'' for extreme market movements: if you could only look at one linear combination of all the intraday price movements, which combination would best predict tomorrow's crash?

\subsection{The Mathematical Intuition: From Covariance to Direction}

The optimization problem \eqref{eq:fepls} asks: ``Which direction $w$ maximizes the covariance between the projection $\langle w, X \rangle$ and $Y$, \textit{when we only look at extreme values of $Y$}?''

This is different from standard PLS because:
\begin{itemize}
    \item Standard PLS: Maximizes covariance over all data points (average behavior).
    \item FEPLS: Maximizes covariance \textit{conditionally} on $Y \ge y$ (tail behavior).
\end{itemize}

The key insight is that by conditioning on extremes, we focus the optimization on the regime that matters most for risk prediction.
















\section{Notation}

\subsection{Notation}

A summary of the main notation used in the FEPLS framework is provided in Table~\ref{tab:notation} in the appendix.








\section{Theoretical Framework}
\label{sec:theoretical_framework}

\subsection{The FEPLS Problem}

Suppose we observe data pairs $(Y, X)$, where $Y$ is a real-valued variable we care about (for example, the size of a market move), and $X$ contains other observable information (for example, a time series or a function we record alongside $Y$). The central question is: \textbf{Given $X$ and $Y$, can we find a summary of $X$ that best explains the most extreme values of $Y$?}

FEPLS answers this by searching for a direction $w$ in the space of possible summaries of $X$ that is most strongly related to the largest values of $Y$. Formally, for a high threshold $y$, we solve:
\begin{align}
\label{eq:fepls}
w(y) := \argmax_{\zp{w} = 1} \Cov\big( \langle w, X \rangle, Y \mid Y \ge y\big), \quad y \in \R,
\end{align}
where $\langle w, X \rangle$ simply means forming a linear combination of $X$ using $w$ (for instance, by integrating $w(t) X(t)$ if $X$ is a function of $t$), and the $\Cov$ is computed only among the cases where $Y$ exceeds $y$---that is, focusing only on the extremes.


\subsection{Inverse Model: $g$ (Model) vs.\ $\varphi$ (Method)}

The paper \textbf{suppose} the inverse model
\begin{align}
\label{eq:single_index_model}
X = g(Y)\beta + \varepsilon, \quad \beta \in H, \quad \zp{\beta} = 1,
\end{align}
where $g \in \RV_\kappa(+\infty)$ ($\kappa > 0$) is an unknown link function and $\varepsilon: \Omega \to H$ is noise. When $Y$ is extreme, $g(Y)$ tends to dominate $X$, making the signal $g(Y)\beta$ far larger than the noise.

\begin{explanationbox}{What’s fixed and what’s chosen?}
\begin{itemize}
    \item \textbf{$g$ ($\kappa$):} Describes the underlying, real-world relationship between $Y$ and $X$ (imposed by the data-generating process; not under our control). Example: $g(y) = y^{0.5}$.
    \item \textbf{$\varphi$ ($\tau$):} A user-chosen, ``test'' function (tuning parameter); not part of nature, but introduced in the estimator $\hat{\beta}_\varphi$ to optimize statistical performance for extremes.
\end{itemize}
\end{explanationbox}


\begin{rhobox}{Moment Condition and Tuning of $\tau$}\label{rhobox:tau_tuning}
The first key condition, $0 < 2(\kappa+\tau)\gamma < 1$, ensures moments exist for reliable estimation. If $g$ (large $\kappa$) grows too fast, sums like $\sum X_i$ may diverge; picking a decreasing $\varphi$ (negative $\tau$) counteracts this, keeping sums finite.

The estimator uses weights $\varphi(Y_i)$ for extremes, e.g.
\[
\sum X_i\, \varphi(Y_i)\, \mathbf{1}_{\{Y_i \ge y\}}
\]
so tuning $\tau$ adjusts influence: big $\tau$ (positive) focuses on the deepest extremes (low bias, high variance); small or negative $\tau$ spreads weight (better stability).
\end{rhobox}







\subsection{Second-Order Regular Variation}

A new assumption is that the response variable $Y$ is heavy-tailed to the second order. Specifically, the tail quantile function $U(t) := F^-(1-1/t)$ belongs to the class of second-order regularly-varying functions:

\begin{definition}[Second-Order Regular Variation]
The function $U$ belongs to $2\RV_{\gamma,\rho}(+\infty)$ if there exist $\gamma \in (0,1)$, $\rho \le 0$ and an auxiliary function $A$ ultimately of constant sign with $A(t) \to 0$ as $t \to +\infty$ such that:
\begin{align}
\label{hyp:2rv}
\lim_{t \to +\infty} \frac{1}{A(t)}\Big( \frac{U(ty)}{U(t)} - y^{\gamma} \Big) = y^{\gamma} H_\rho(y) := y^{\gamma}\int_1^y u^{\rho-1}
\end{align}
\end{definition}

\paragraph{A variation.}

Although the original paper does not state it explicitly, this definition immediately yields that $A$ is of the form $A(t) = t^{\rho}$ which is important to derive the convergence scaling law in \eqref{eq:convergence_rate_exact}.



\begin{rhobox}{Interpretation of $\rho$ 2nd-order RV}

\textbf{Why is Second-Order Regular Variation (2RV) necessary?}

The first-order regular variation ($\RV_\gamma$) tells us that the distribution ``resembles'' a Pareto distribution asymptotically, but in finite samples, we are never truly ``at infinity.'' The distribution is:
\[
\text{True Distribution} = \text{Pareto} + \text{Error}
\]

The 2RV quantifies this error through the auxiliary function $A(t)=t^{\rho}$. This becomes crucial when choosing $k$:

\begin{itemize}
    \item If $k$ is very small (deep in the tail), the approximation error is small (close to the limit), but the variance is huge (few points).
    \item If $k$ is larger (to stabilize variance), we move away from the extreme tail where the distribution deviates from pure Pareto. The approximation error grows.
\end{itemize}

The 2RV tells us \textit{how fast} this error grows. The condition $\sqrt{k}A(n/k) = O(1)$ balances:
\begin{itemize}
    \item $\sqrt{k}$: The statistical variance (noise)
    \item $A(n/k)$: The model bias (error quantified by 2RV)
\end{itemize}

This condition says: ``You may increase $k$ as long as your model error ($A$) remains smaller than your statistical noise ($\sqrt{k}$).''

In summary: 1RV ($\gamma$) gives the \textit{direction} of the tail (the slope), while 2RV ($\rho$) gives the \textit{straightness} of the tail (is it a perfect straight line in log-log scale, or is it curved?). 
If $\rho$ is very negative, the tail is almost straight, making estimation easier. If $\rho$ is close to 0, convergence is very slow.

\end{rhobox}





\subsection{Signal Dominance over Noise}
\begin{rhobox}{Signal Dominance over Noise}\label{rhobox:signal_dominance}
The tail of $g(Y)$ (signal channel) must be heavier than the tail of the noise $\varepsilon$. 
In practical terms, this ensures that among the largest observed $Y$ values, the associated signal $g(Y)\beta$ is
 much more extreme than the noise, so the extreme behavior is informative about $\beta$.

\textbf{Why necessary?} If the noise's tail is too fat, its fluctuations swamp the regression signal in the extremes, making recovery
impossible even with infinite data. \emph{If $\kappa$ increases (sharper $g$ growth), you can choose larger $k$ and 
variance decreases.}

That is why the model is only identifiable if the signal is dominant over the noise, under the inequality 
\begin{align}
\label{eq:signal_dominance}
q\kappa\gamma > 1.
\end{align}

We call this condition \textbf{signal dominance} for \textbf{identifiability}.
\end{rhobox}







\subsection{What do we estimate to ensure consistency ?}

To ensure that we are into the consistency inequality taht allows signal recovery, we need to estimate both $\gamma$ and $\rho$.
$\kappa$ cannot be estimated from the data.



so there is no best $\tau$ choice a priori, we need to construct estimator for a lot of different $\tau$ values and find an interval 
that contains some stationarity in the beta estimate. We call this procedure \textbf{$\tau$ tuning} to satisfy the \textbf{integrability condition}.




\subsubsection{Choice of $k$}

When testing a particular $\tau$, we need to choose a number of extreme observations $k$ to minimize the bias-variance tradeoff.
We call this procedure \textbf{k tuning} for \textbf{convergence}.

\begin{rhobox}{Optimal Choice of $k$}
Following the classical extreme-value theory framework (see \cite{de2007}, Equation~3.2.10), the optimal choice of $k$ balances bias (governed by 2RV, i.e., $A(n/k)$) and variance (proportional to $k^{-1/2}$):
\begin{quote}
\[
\mathrm{error}(n, k) \approx C_1\,A(n/k) + C_2\,k^{-1/2}
\]
\end{quote}
where $A(n/k)$ controls bias and $k^{-1/2}$ controls the noise variance.

Solving for the optimal tradeoff yields:
\[
k_n \sim c\, n^{-2\rho/(1-2\rho)}
\]
for some $c > 0$ (constant) and $\rho < 0$.
\end{rhobox}
Once you have chosen proper $\tau$, estimated $\rho$ and $\gamma$, you have chosen the number of observations to use, you have chosen $k$.

It remains that you have performed your FEPLS estimation, which convergence guarantee do you have ?





\subsection{Threshold choice and growth of $y$}

\begin{explanationbox}{Growth of the Threshold and Intermediate Sequences}
For any threshold $y \ge 0$, the FEPLS estimator is defined as
\[
\hat\beta_\varphi(y)
  := \frac{\hat v_\varphi(y)}{\|\hat v_\varphi(y)\|}, \qquad
\hat v_\varphi(y)
  = \frac{1}{n} \sum_{i=1}^n X_i \,\varphi(Y_i)\,\mathbf{1}_{\{Y_i \ge y\}}.
\]
Thus, $\hat\beta_\varphi(y)$ is a weighted combination of the $X_i$ whose $Y_i$ are in the tail, with weights set by the test function $\varphi$.

For consistency results, we consider a sequence of (deterministic) thresholds $(y_{n,k})$ such that
\[
y_{n,k} \sim U\!\left(\frac{n}{k}\right),\qquad n\to\infty,
\]
where $U$ is the tail quantile function of $Y$. In practice, $y_{n,k}$ is approximated by the $(n-k+1)$-th order statistic $Y_{n-k+1,n}$:
\[
Y_{n-k+1,n} \approx y_{n,k} \sim U\!\left(\frac{n}{k}\right)
\quad\text{as}\ n\to\infty.
\]

If the tail of $Y$ is regularly varying, i.e., $\bar F(y) = \mathbb{P}(Y>y) \in \mathrm{RV}_{-1/\gamma}(+\infty)$, then
\[
U(t) \sim C\, t^\gamma \quad(t\to\infty)
\]
for some $C>0$; for example, in the standard Pareto, $U(t)\sim t^\gamma$. For $t=n/k$,
\[
y_{n,k} \sim C\left(\frac{n}{k}\right)^{\gamma}.
\]
We consider intermediate sequences $k = k_n \to \infty$ with $k_n/n \to 0$ so $n/k_n \to \infty$, hence
\[
y_{n,k_n} \sim C\left(\frac{n}{k_n}\right)^{\gamma}
\to +\infty\quad\text{as }n\to\infty.
\]
So, the threshold $y$ grows with $n$ and $k_n$:
\[
y = Y_{n-k+1,n} \approx U\!\left(\frac{n}{k}\right)\to+\infty
\]
and more and more extreme observations are used as $n$ increases.
\end{explanationbox}



\subsection{Consistency Results}

The FEPLS estimator is
\begin{align}
\label{eq:estimator}
\hat{\beta}_\varphi(y) := \frac{\hat{v}_\varphi(y)}{\|\hat{v}_\varphi(y)\|}, \quad \text{where} \quad \hat{v}_\varphi(y) = \frac{1}{n} \sum_{i=1}^n X_i \varphi(Y_i) \mathbf{1}_{\{Y_i \ge y\}}.
\end{align}

\begin{boundbox}{Consistency of FEPLS Estimator (Theorem~\ref{thm:consistency})}
\begin{theorem}[Consistency of FEPLS Estimator]
\label{thm:consistency}
Under the previous assumptions, the FEPLS estimator is consistent:
\begin{align}
\|\hat{\beta}_\varphi(Y_{n-k+1,n}) - \beta\| = O_{\p}(\delta_{n,k}) \xrightarrow[n \to +\infty]{} 0,
\end{align}
where the convergence rate is
\[
\delta_{n,k} := \left( g(y_{n,k}) \left(\frac{k}{n}\right)^{1/q} \right)^{-1}
\]
with $g$ and $q$ as defined in the model assumptions. When $k \sim c\, n^{-2\rho/(1-2\rho)}$ for $\rho<0$ and $c>0$, and the bias condition $\sqrt{k}A(n/k)=O(1)$ holds, the rate satisfies
\begin{align}
\label{eq:convergence_rate_exact}
\delta_{n,k} = O\left( n^{(1/q-\gamma\kappa)/(1-2\rho)} \right).
\end{align}

Consistency requires (i) signal dominance $q\kappa\gamma > 1$ (signal tail heavier than noise) and (ii) convergence stability $2(\kappa+\tau)\gamma < 1$ (finite variance).
\end{theorem}
\end{boundbox}

The rate $\delta_{n,k}$ depends on the tail of $Y$ (via $\gamma$ and $\rho$), noise integrability $q$, and the link function $g$ (via $\kappa$). See \cite{girard2023} for detailed proof and results.





From now we are all set to perform extensive empirical validation of the FEPLS estimator.

\subsection*{Summary of Main Assumptions and Scaling Laws}
\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}
\footnotesize
\begin{tabular}{p{3.2cm} p{4.0cm} p{4.2cm} p{3.8cm}}
\toprule
\textbf{Name} & \textbf{Condition / Regime} & \textbf{Role} & \textbf{Main Scaling / Consequence} \\
\midrule
Tail heaviness & $\bar F(y) \in \RV_{-1/\gamma}(+\infty)$, $\gamma \in (0,1)$ & Heavy-tailed response $Y$ (first-order RV) & Tail quantile $U(t) \sim C t^\gamma$, threshold $y_{n,k} \sim C (n/k)^\gamma$ \\
\midrule
Second-order RV & $U \in 2\RV_{\gamma,\rho}(+\infty)$, $\rho \le 0$, $A(t)=t^{\rho}$ & Controls bias of Pareto approximation (curvature of the tail) & Bias term $A(n/k)$, choice of $k$ via $\sqrt{k}A(n/k)=O(1)$ \\
\midrule
Moment / integrability & $0 < 2(\kappa+\tau)\gamma < 1$ & Ensures existence of moments for FEPLS statistics; governs valid $\tau$ range & Admissible $(\kappa,\tau,\gamma)$ region for stable estimation \\
\midrule
Signal dominance & $q\kappa\gamma > 1$ & Signal tail heavier than noise; guarantees identifiability & FEPLS direction $\beta$ recoverable from extremes; noise cannot dominate \\
\midrule
Intermediate sequence & $k_n \to \infty$, $k_n/n \to 0$ & Balances number of extremes and sample size & Threshold $y_{n,k_n} \sim C (n/k_n)^\gamma \to +\infty$; more and more extreme $Y$'s used \\
\midrule
Optimal $k_n$ & $k_n \sim c\, n^{-2\rho/(1-2\rho)}$ for $\rho<0$ & Explicit bias-variance tradeoff using 2RV & Minimizes asymptotic error $\mathrm{error}(n,k)$; feeds into optimal convergence rate \\
\midrule
Convergence rate & $\delta_{n,k} = \big(g(y_{n,k})(k/n)^{1/q}\big)^{-1}$ & Speed of convergence of $\hat{\beta}_\varphi$ to $\beta$ & For optimal $k_n$: $\delta_{n,k} = O\big(n^{(1/q-\gamma\kappa)/(1-2\rho)}\big)$ \\
\bottomrule
\end{tabular}
\caption{Summary of the main assumptions, regimes, and resulting scaling laws underlying the FEPLS consistency and convergence analysis.}
\label{tab:assumptions_scaling}
\end{table}



\newpage



\section{Empirical Validation}
All our reproducible experiments are available in this \href{https://github.com/janisaiad/fctpls}{repository}.

\subsection{Data Description}

\paragraph{OHLC data:}
We use one year of mid-frequency (5-minute) open-high-low-close (OHLC) market data for Hungary and Poland, sourced from the open and widely-used financial data aggregator \href{https://stooq.com/db/h/}{Stooq}. The dataset covers both the Hungarian (Budapest Stock Exchange) and Polish (Warsaw Stock Exchange) stock indices.

\paragraph{Data Characteristics:}
    \begin{itemize}
    \item \textbf{Frequency:} 5-minute bars (OHLC)
    \item \textbf{Markets:} Hungary (\texttt{BUX}) 50 stocks, Poland (\texttt{WIG20}) 100 stocks
    \item \textbf{Size:} Approximately 50 megabytes compressed for 1 year (per market)
    \item \textbf{Coverage:} Full calendar year (about 50,000--60,000 bars per market)
    \item \textbf{Format:} CSV, standardized with columns for open, high, low, close, and volume
    \item \textbf{Source:} Freely available and reproducible from \href{https://stooq.com/db/h/}{https://stooq.com/db/h/}
    \end{itemize}
    
The analysis uses X as the daily return curve from a stock A and Y as the next day's max return from a stock B.
So we reproduce the analysis from the paper using open-source data.


\paragraph{Tick-by-tick data:}

For the most granular analysis, we purchased 3 months of nanosecond-resolution tick-by-tick data from Databento, 
covering all limit order book events (including every limit order, cancelation, and execution) for p
rominent NASDAQ stocks: Google, Apple, American Airlines (AAL), Amazon, and Microsoft. 
This dataset, considered extremely high quality, cost approximately \$1000 and represents a gold standard for empirical financial 
microstructure research.

\begin{itemize}
    \item \textbf{Frequency:} Nanosecond-scale tick-by-tick (full limit order book)
    \item \textbf{Markets:} NASDAQ (Google, Apple, American Airlines (AAL), Amazon, Microsoft)
    \item \textbf{Duration:} 3 months
    \item \textbf{Events:} All limit order submissions, cancellations, executions (Level 3 order book data)
    \item \textbf{Size/Cost:} Approximately \$1,000 for the full period via Databento
    \item \textbf{Quality:} Highest market microstructure fidelity available
    \item \textbf{Source:} Commercially obtained from \href{https://databento.com/}{Databento} from previous work.
\end{itemize}















\subsection{Statistical Workflow for Empirical Validation}

We adhere to a transparent, reproducible workflow for FEPLS model validation using both Stooq (5-minute OHLC) and Databento (tick) data. The procedure is as follows:

\begin{enumerate}
    \item \textbf{Data Preparation:} Download raw data (Stooq or Databento). Select target stocks and extract daily return curves $X$ (covariate) and the scalar $Y$ (next-day maximum return), possibly from different stocks to capture cross-dependence.
    \item \textbf{Cleaning and Log Return Construction:} Compute 5-minute (or tick) log-returns, handle missing values (e.g., using linear interpolation or LOCF), and align data to uniform time grids.
    \item \textbf{Dependence Check:} Assess temporal dependence (volatility clustering) in extremes via autocorrelation plots; optionally thin the data by taking every $n$th day to reduce dependence.
    \item \textbf{Heavy-Tail Verification:} Plot the Hill estimator for $Y$ to confirm a heavy-tailed regime appropriate for FEPLS. Proceed only if the estimated tail index $\gamma$ is positive. If applicable for sufficient data, we estimate $\rho$ as well.
    \item \textbf{Correlation Check (Optional):} Compute canonical or ordinary correlation between projections $\langle X, \beta \rangle$ and $Y$ to confirm a tractable signal for dimension reduction.
    \item \textbf{Train/Test Chronological Split:} Split the data into training and testing periods. \textbf{ $\tau$ tuning} is carried out on the training set.
    \item \textbf{Parameter Calibration:}
    \begin{enumerate}
        \item Select a conservative test function parameter $\tau$ (e.g., $\tau = -1$).
        \item Optimize the number of extremes $k$ via a grid search: for each $k$, estimate $\hat{\beta}_{\tau, k}$ on the training set and maximize the in-extremes correlation between projections and $Y$.
    \end{enumerate}
    \item \textbf{Validation:} 
\begin{itemize}
        \item Apply the fitted $\hat{\beta}$ to the test set.
        \item Project new curves $X^{\text{test}}$ onto $\hat{\beta}$ to obtain scalar scores.
        \item Estimate conditional Value-at-Risk (VaR) via quantile regression or local smoothing.
        \item Assess visual coverage (exceedance) and independence of VaR violations.
\end{itemize}
\end{enumerate}

This workflow ensures both statistical rigor and full reproducibility for all empirical FEPLS validation results.

\section{Empirical Results}

We present a comprehensive empirical validation of FEPLS on Hungarian stock market data. The analysis focuses on cross-asset relationships, examining how intraday return curves from one stock predict extreme returns in another. We analyze multiple stock pairs, with detailed results for the 4IG $\to$ AKKO pair shown in Figure~\ref{fig:consistency_validation}. Additional results for 4IG paired with other stocks (Appeninn, Autowallis, BIF, CIG Pannonia, Dunahouse, GSPark, Richter, Alteo) are provided in the appendix.


    

\subsection{Comments}


\paragraph{Comments on the low frequency data:}
In contrast to the high-frequency setting, the low-frequency analysis (daily data) shows excellent Hill estimator performance, indicating proper heavy-tail behavior. However, we work with a much smaller sample size, with $k < 10$ extreme observations. In this regime of very noisy data with limited samples, we observe that the $\tau$ dependency becomes crucial: the choice of test function parameter $\tau$ significantly impacts the FEPLS estimates and their stability.

Our empirical findings reveal an important structural pattern: we can identify distinct stock groups based on their sensitivity to end-of-day volatility. Some stock pairs exhibit a clear relationship where end-of-day volatility drives the maximum return of the next day's stock movement, while others remain insensitive to this effect. 



\begin{figure}[htbp]
    \centering
        \includegraphics[width=0.9\textwidth]{figures/4ig_akko_hill_qq_tail.png}
        \caption{Empirical validation of FEPLS consistency: diagnostic plots for the 4IG $\to$ AKKO pair.}
        \label{fig:consistency_validation}
    \end{figure}







\paragraph{Comments on the high frequency data:}
We present results from high-frequency analysis using 5-minute intraday data for AAPL. In this setup, $X$ represents the 5-minute window of returns, and $Y$ is the maximum return of the next 5-minute window for the same stock. With $k$ ranging from 0 to 800, we have access to a large amount of data. 

However, the high-frequency nature of the data (5-minute intervals) presents challenges: the Hill estimator is not well estimated due to the tick-by-tick microstructure effects, and the Q-Q plot shows poor fit. Despite these limitations, we observe a clear increasing relationship between the FEPLS projections and extreme responses, as shown in Figure~\ref{fig:aapl_hf}. This increasing curve provides strong evidence that mid-frequency price movements are primarily trend-driven, where current 5-minute patterns predict subsequent extreme moves in the same direction.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{figures/AAPL_5min_d10_k5_tau_-0.5.png}
\caption{FEPLS analysis for AAPL using 5-minute high-frequency data. The figure shows the relationship between 5-minute return windows ($X$) and the maximum return of the next 5-minute window ($Y$). Despite challenges with Hill estimation and Q-Q plot fit due to microstructure effects, the increasing relationship clearly indicates trend-driven behavior at mid-frequencies.}
\label{fig:aapl_hf}
\end{figure}








\section*{Acknowledgments}

We thank Charles-Albert Lehalle for providing the expensive high-frequency data.





% --- Bibliography ---
\begin{thebibliography}{9}

\bibitem{girard2023}
Girard, S., and Pakzad, A. 2023.
\textit{Functional Extreme Partial Least Squares}.
Journal of Multivariate Analysis.

\bibitem{de2007}
de Haan, L., and Ferreira, A. 2007.
\textit{Extreme value theory: an introduction}.
Springer Science \& Business Media.

\end{thebibliography}


\newpage
\section{Appendix}

\subsection{Comparison between PCA and PLS}

\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \footnotesize
    \begin{tabular}{p{3.5cm} p{4.5cm} p{4.5cm}}
    \toprule
     & \textbf{PCA (Unsupervised)} & \textbf{PLS (Supervised)} \\
    \midrule
    \textbf{Uses response variable?} 
        & No: PCA operates solely on the covariates $X$, ignoring any relationship with a response variable $Y$.
        & Yes: PLS explicitly incorporates the response $Y$ in the optimization. \\
    \textbf{Objective} 
        & Find directions that maximize the \emph{variance} of the projected data, i.e., $\argmax_{\|w\|=1} \Var(\langle w, X \rangle)$.
        & Find directions that maximize the \emph{covariance} between the projection and response, i.e., $\argmax_{\|w\|=1} \Cov(\langle w, X \rangle, Y)$.\\
    \textbf{Interpretation} 
        & The first principal component captures the direction of maximum variance in $X$, regardless of its relevance to predicting $Y$.
        & The PLS direction captures the direction in $X$ most correlated with $Y$, making it directly relevant to prediction. \\
    \textbf{Strengths/Limitations} 
        & Limitation: In regression contexts, high-variance directions can be orthogonal to the relationship with $Y$, so PCA may prioritize irrelevant directions.
        & Advantage: By focusing on covariance with $Y$, PLS finds directions that are both informative about $X$ and predictive of $Y$. \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between PCA (Principal Component Analysis) and PLS (Partial Least Squares).}
    \label{tab:pca_pls}
    \end{table}
    
\subsection{Additional Empirical Results: 4IG Stock Pairings}

This section presents detailed FEPLS analysis results for 4IG paired with several other stocks from the Hungarian market. 
We selected this collection of stocks to illustrate both consistently estimated and non-consistently estimated cases, providing a realistic picture of what can be expected when applying the FEPLS framework to real financial data without curated experimental design.

Additional figures for all stock pairs are available in the figures directory:
\begin{itemize}
    \item Hill, Q-Q, and tail plots (files with \texttt{hill\_qq\_tail} in the name) for diagnostic assessment
    \item Conditional quantile plots (files with \texttt{conditional\_quantile} in the name) showing the relationship between projections and extreme responses
    \item Beta comparison plots (4 digits in filename, e.g., \texttt{4ig\_akko\_tau\_-2.0\_-0.5.png}) for $\hat{\beta}$ across different $\tau$ pairs
\end{itemize}

\onecolumn

\begin{figure*}[htbp]
\centering
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_bif_hill_qq_tail.png}
\caption{4IG $\to$ BIF}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_cigpannonia_hill_qq_tail.png}
\caption{4IG $\to$ CIG Pannonia}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_delta_hill_qq_tail.png}
\caption{4IG $\to$ Delta}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_dunahouse_hill_qq_tail.png}
\caption{4IG $\to$ Dunahouse}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_esense_hill_qq_tail.png}
\caption{4IG $\to$ eSense}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_granit_hill_qq_tail.png}
\caption{4IG $\to$ Granit}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_gspark_hill_qq_tail.png}
\caption{4IG $\to$ GSPark}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_masterplast_hill_qq_tail.png}
\caption{4IG $\to$ Masterplast}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_mol_hill_qq_tail.png}
\caption{4IG $\to$ MOL}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_mtelekom_hill_qq_tail.png}
\caption{4IG $\to$ MTelekom}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_opus_hill_qq_tail.png}
\caption{4IG $\to$ Opus}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_otp_hill_qq_tail.png}
\caption{4IG $\to$ OTP}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_pannergy_hill_qq_tail.png}
\caption{4IG $\to$ Pannergy}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_raba_hill_qq_tail.png}
\caption{4IG $\to$ Raba}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_richter_hill_qq_tail.png}
\caption{4IG $\to$ Richter}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_vertikal_hill_qq_tail.png}
\caption{4IG $\to$ Vertikal}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_waberers_hill_qq_tail.png}
\caption{4IG $\to$ Waberers}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_zwack_hill_qq_tail.png}
\caption{4IG $\to$ Zwack}
\end{subfigure}

\caption{Hill, Q-Q, and tail plots for FEPLS analysis of 4IG paired with various stocks from the Hungarian market. Each plot shows exceedance analysis, Hill plot for tail index estimation, and Q-Q plot for goodness-of-fit assessment.}
\label{fig:hill_qq_tail_grid}
\end{figure*}

\subsection{Conditional Quantile Plots}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/4ig_akko_conditional_quantile.png}
\caption{Conditional quantile plot for 4IG $\to$ AKKO.}
\label{fig:4ig_akko_conditional}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/4ig_appeninn_conditional_quantile.png}
\caption{Conditional quantile plot for 4IG $\to$ Appeninn.}
\label{fig:4ig_appeninn_conditional}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/4ig_autowallis_conditional_quantile.png}
\caption{Conditional quantile plot for 4IG $\to$ Autowallis.}
\label{fig:4ig_autowallis_conditional}
\end{figure}

\subsection{Beta Comparison Plots}

To visualize how the estimated FEPLS direction $\hat{\beta}$ changes across different pairs of $\tau$ values, we provide beta comparison plots below for 4IG $\to$ AKKO. Each plot corresponds to a specific pair of $(\tau_1, \tau_2)$ values. This enables us to assess the stability and sensitivity of the FEPLS estimator under varying test function parameters.

\onecolumn

\begin{figure*}[htbp]
\centering
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-2.0_-0.5.png}
\caption{$(\tau_1, \tau_2) = (-2.0, -0.5)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-2.0_-1.0.png}
\caption{$(\tau_1, \tau_2) = (-2.0, -1.0)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-2.0_-3.0.png}
\caption{$(\tau_1, \tau_2) = (-2.0, -3.0)$}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_-0.5.png}
\caption{$(\tau_1, \tau_2) = (-3.0, -0.5)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_-1.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, -1.0)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_-2.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, -2.0)$}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_-3.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, -3.0)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_0.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, 0.0)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_0.5.png}
\caption{$(\tau_1, \tau_2) = (-3.0, 0.5)$}
\end{subfigure}

\vspace{0.3cm}

\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_1.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, 1.0)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_2.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, 2.0)$}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
\centering
\includegraphics[width=\textwidth]{figures/4ig_akko_tau_-3.0_3.0.png}
\caption{$(\tau_1, \tau_2) = (-3.0, 3.0)$}
\end{subfigure}

\caption{Beta comparison plots for 4IG $\to$ AKKO showing how the estimated FEPLS direction $\hat{\beta}$ varies across different pairs of $\tau$ values.}
\label{fig:beta_comparison_grid}
\end{figure*}



\end{document}

