\documentclass[11pt,twocolumn]{article}

% --- Packages ---
\usepackage[english]{babel}
\let\Bbbk\relax
\usepackage{amsmath, amsthm, amssymb}
\usepackage{newtxtext,newtxmath}  % Formal Times-like font
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tcolorbox}
\tcbuselibrary{theorems,breakable}
\usepackage{hyperref}
\usepackage[margin=0.3in]{geometry}
\usepackage{float}

% Column spacing for two-column layout
\setlength{\columnsep}{0.5in}  % Increase space between columns

% --- Macros ---
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\RV}{\text{RV}}
\newcommand{\zp}[1]{\|#1\|}
\newcommand{\f}[1]{\frac{#1}}
\newcommand{\ff}[1]{\frac{1}{#1}}
\DeclareMathOperator*{\argmax}{arg\,max}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

% Custom boxes for explanations (adapted for two-column)
\newtcolorbox{explanationbox}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    title=#1,
    fonttitle=\bfseries,
    breakable,
    before skip=0.5\baselineskip,
    after skip=0.5\baselineskip
}

\newtcolorbox{boundbox}[1]{
    colback=green!5!white,
    colframe=green!75!black,
    title=#1,
    fonttitle=\bfseries,
    breakable,
    before skip=0.5\baselineskip,
    after skip=0.5\baselineskip
}

\newtcolorbox{rhobox}[1]{
    colback=orange!5!white,
    colframe=orange!75!black,
    title=#1,
    fonttitle=\bfseries,
    breakable,
    before skip=0.5\baselineskip,
    after skip=0.5\baselineskip
}

% --- Metadata ---
\title{Functional Extreme Partial Least Squares:\\Unraveling the Intuition and Empirical Validation}

\author{Janis Aiad and Simon Elis\\
Master MVA - Statistical Learning with Extreme Values\\
ENS Paris Saclay, Paris, France
\href{GitHub}{https://github.com/janisaiad/FEPLS}}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We present an in-depth study of the Functional Extreme Partial Least Squares (FEPLS) framework, a method designed to identify predictive features in high-dimensional or functional data associated with rare, extreme events. Our work applies the FEPLS approach to financial time series, with a particular focus on understanding how patterns in one asset's intraday returns anticipate large, infrequent moves in another. We empirically evaluate FEPLS in two settings: (1) cross-asset analysis using medium-frequency stock data, and (2) high-frequency financial data via a subsampling methodology. The results provide new insights into the strengths and limitations of FEPLS, practical guidelines for its calibration, and its applicability across a range of financial regimes.
\end{abstract}

\section{Introduction}

Extreme events—rare, significant deviations from normal behavior—play a crucial role in finance, signal processing, risk management, and many scientific fields. The way such extremes manifest in data, especially when those data are high-dimensional or functional (such as entire intraday price curves), presents substantial challenges both for modeling and for prediction. Conventional dimension reduction tools like Principal Component Analysis (PCA) or classical Partial Least Squares (PLS) are effective for understanding average behavior but may fail to capture the patterns associated specifically with rare, high-impact events.

Functional Extreme Partial Least Squares (FEPLS) is a methodology developed to address precisely this challenge. It extends the classical PLS approach into the domain of extremes, targeting the features of high-dimensional or infinite-dimensional covariates that are most informative for explaining or predicting the occurrence of extreme events in a response variable. 

The central research question motivating this work is the following:
\begin{center}
    \emph{What is the most likely shape of the covariate $X$ when the response $Y$ is extreme?}
\end{center}
Answering this question, for instance, can reveal which specific features of a price curve tend to occur in the lead-up to major market moves—a task of paramount importance in financial engineering, but also relevant in fields like signal processing and detection theory, where characterization of rare patterns is required.

In this report, we offer both a theoretical and comprehensive empirical study of the FEPLS framework. We begin by unpacking the intuition and mathematical underpinnings of FEPLS, clarifying how it focuses on conditional extremes, and describing the signal-to-noise assumptions required for consistent estimation. Our goal is to provide a clear narrative that connects the theory to practical considerations.

To bridge theory and practice, we conduct extensive experiments on open financial datasets. These analyses illuminate the strengths and limitations of FEPLS in realistic, noisy, heavy-tailed environments. We also move beyond the day-to-day context of classical financial studies, exploring high-frequency data using a subsampling-based methodology. This allows us to probe FEPLS performance and the validity of its foundational assumptions across a variety of time scales and predictive settings.

Our contributions in this work include:
\begin{itemize}
    \item A concise evaluation of FEPLS's statistical assumptions and estimator stability in real-world data, along with practical guidance on parameter calibration and computational considerations, culminating in the first empirical analysis of FEPLS applied to high-frequency financial data across milliseconds to minutes.
    \item A detailed empirical evaluation of FEPLS under challenging market noise, providing clear illustrations of when and where the method succeeds or faces limitations.
\end{itemize}

Through this combination of theoretical insight, methodological clarification, and practical experimentation, we aim to both advance the understanding of FEPLS and provide a resource for practitioners seeking to apply it in domains where extremes govern the risk landscape.


















\section{Intuitive Understanding of the FEPLS Framework}

\subsection{Difference Between PLS and PCA}

The fundamental distinction between Partial Least Squares (PLS) and Principal Component Analysis (PCA) lies in their use of supervision. See Table~\ref{tab:pca_pls} in the appendix for a detailed comparison.

\textbf{FEPLS (PLS for extremes):}
FEPLS extends this supervised approach to the extreme regime. The fundamental idea is to find a \textbf{single direction} in the infinite-dimensional space of functional covariates that best explains extreme events. Think of it as finding the ``recipe'' for extreme market movements: if you could only look at one linear combination of all the intraday price movements, which combination would best predict tomorrow's crash?

\subsection{The Mathematical Intuition: From Covariance to Direction}

The optimization problem \eqref{eq:fepls} asks: ``Which direction $w$ maximizes the covariance between the projection $\langle w, X \rangle$ and $Y$, \textit{when we only look at extreme values of $Y$}?''

This is different from standard PLS because:
\begin{itemize}
    \item Standard PLS: Maximizes covariance over all data points (average behavior).
    \item FEPLS: Maximizes covariance \textit{conditionally} on $Y \ge y$ (tail behavior).
\end{itemize}

The key insight is that by conditioning on extremes, we focus the optimization on the regime that matters most for risk prediction.
















\section{Notation}

\subsection{Notation}

A summary of the main notation used in the FEPLS framework is provided in Table~\ref{tab:notation} in the appendix.








\section{Theoretical Framework}

\subsection{The FEPLS Problem}

Suppose we observe data pairs $(Y, X)$, where $Y$ is a real-valued variable we care about (for example, the size of a market move), and $X$ contains other observable information (for example, a time series or a function we record alongside $Y$). The central question is: \textbf{Given $X$ and $Y$, can we find a summary of $X$ that best explains the most extreme values of $Y$?}

FEPLS answers this by searching for a direction $w$ in the space of possible summaries of $X$ that is most strongly related to the largest values of $Y$. Formally, for a high threshold $y$, we solve:
\begin{align}
\label{eq:fepls}
w(y) := \argmax_{\zp{w} = 1} \Cov\big( \langle w, X \rangle, Y \mid Y \ge y\big), \quad y \in \R,
\end{align}
where $\langle w, X \rangle$ simply means forming a linear combination of $X$ using $w$ (for instance, by integrating $w(t) X(t)$ if $X$ is a function of $t$), and the $\Cov$ is computed only among the cases where $Y$ exceeds $y$---that is, focusing only on the extremes.


\subsection{Inverse Model: $g$ (Model) vs.\ $\varphi$ (Method)}

The paper \textbf{suppose} the inverse model
\begin{align}
\label{eq:single_index_model}
X = g(Y)\beta + \varepsilon, \quad \beta \in H, \quad \zp{\beta} = 1,
\end{align}
where $g \in \RV_\kappa(+\infty)$ ($\kappa > 0$) is an unknown link function and $\varepsilon: \Omega \to H$ is noise. When $Y$ is extreme, $g(Y)$ tends to dominate $X$, making the signal $g(Y)\beta$ far larger than the noise.

\begin{explanationbox}{What’s fixed and what’s chosen?}
\begin{itemize}
    \item \textbf{$g$ ($\kappa$):} Describes the underlying, real-world relationship between $Y$ and $X$ (imposed by the data-generating process; not under our control). Example: $g(y) = y^{0.5}$.
    \item \textbf{$\varphi$ ($\tau$):} A user-chosen, ``test'' function (tuning parameter); not part of nature, but introduced in the estimator $\hat{\beta}_\varphi$ to optimize statistical performance for extremes.
\end{itemize}
\end{explanationbox}


\begin{rhobox}{Moment Condition and Tuning of $\tau$}\label{rhobox:tau_tuning}
The first key condition, $0 < 2(\kappa+\tau)\gamma < 1$, ensures moments exist for reliable estimation. If $g$ (large $\kappa$) grows too fast, sums like $\sum X_i$ may diverge; picking a decreasing $\varphi$ (negative $\tau$) counteracts this, keeping sums finite.

The estimator uses weights $\varphi(Y_i)$ for extremes, e.g.
\[
\sum X_i\, \varphi(Y_i)\, \mathbf{1}_{\{Y_i \ge y\}}
\]
so tuning $\tau$ adjusts influence: big $\tau$ (positive) focuses on the deepest extremes (low bias, high variance); small or negative $\tau$ spreads weight (better stability).
\end{rhobox}







\subsection{Second-Order Regular Variation}

A new assumption is that the response variable $Y$ is heavy-tailed to the second order. Specifically, the tail quantile function $U(t) := F^-(1-1/t)$ belongs to the class of second-order regularly-varying functions:

\begin{definition}[Second-Order Regular Variation]
The function $U$ belongs to $2\RV_{\gamma,\rho}(+\infty)$ if there exist $\gamma \in (0,1)$, $\rho \le 0$ and an auxiliary function $A$ ultimately of constant sign with $A(t) \to 0$ as $t \to +\infty$ such that:
\begin{align}
\label{hyp:2rv}
\lim_{t \to +\infty} \frac{1}{A(t)}\Big( \frac{U(ty)}{U(t)} - y^{\gamma} \Big) = y^{\gamma} H_\rho(y) := y^{\gamma}\int_1^y u^{\rho-1}
\end{align}
\end{definition}

\paragraph{A variation.}

Although the original paper does not state it explicitly, this definition immediately yields that $A$ is of the form $A(t) = t^{\rho}$ which is important to derive convergence scaling law in \eqref{eq:convergence_scaling_law}.



\begin{rhobox}{Interpretation of $\rho$ 2nd-order RV}

\textbf{Why is Second-Order Regular Variation (2RV) necessary?}

The first-order regular variation ($\RV_\gamma$) tells us that the distribution ``resembles'' a Pareto distribution asymptotically, but in finite samples, we are never truly ``at infinity.'' The distribution is:
\[
\text{True Distribution} = \text{Pareto} + \text{Error}
\]

The 2RV quantifies this error through the auxiliary function $A(t)$. This becomes crucial when choosing $k$:

\begin{itemize}
    \item If $k$ is very small (deep in the tail), the approximation error is small (close to the limit), but the variance is huge (few points).
    \item If $k$ is larger (to stabilize variance), we move away from the extreme tail where the distribution deviates from pure Pareto. The approximation error grows.
\end{itemize}

The 2RV tells us \textit{how fast} this error grows. The condition $\sqrt{k}A(n/k) = O(1)$ balances:
\begin{itemize}
    \item $\sqrt{k}$: The statistical variance (noise)
    \item $A(n/k)$: The model bias (error quantified by 2RV)
\end{itemize}

This condition says: ``You may increase $k$ as long as your model error ($A$) remains smaller than your statistical noise ($\sqrt{k}$).''

In summary: 1RV ($\gamma$) gives the \textit{direction} of the tail (the slope), while 2RV ($\rho$) gives the \textit{straightness} of the tail (is it a perfect straight line in log-log scale, or is it curved?). 
If $\rho$ is very negative, the tail is almost straight, making estimation easier. If $\rho$ is close to 0, convergence is very slow.

\end{rhobox}





\subsection{Signal Dominance over Noise}
The tail of $g(Y)$ (signal channel) must be heavier than the tail of the noise $\varepsilon$. 
In practical terms, this ensures that among the largest observed $Y$ values, the associated signal $g(Y)\beta$ is
 much more extreme than the noise, so the extreme behavior is informative about $\beta$.

\textbf{Why necessary?} If the noise's tail is too fat, its fluctuations swamp the regression signal in the extremes, m
aking recovery impossible even with infinite data. \emph{If $\kappa$ increases (sharper $g$ growth), you can choose larger $k$ and 
variance decreases.}

That is why the model is only identifiable if the signal is dominant over the noise, under the inequality 
\begin{align}
\label{eq:signal_dominance}
q\kappa\gamma > 1.
\end{align}

We call this condition \textbf{signal dominance} for \textbf{identifiability}.
\end{rhobox}







\subsection{What do we estimate to ensure consistency ?}

To ensure that we are into the consistency inequality taht allows signal recovery, we need to estimate both $\gamma$ and $\rho$.
$\kappa$ cannot be estimated from the data.



so there is no best tau choice a priori, we need to construct estimator for a lot of different taus and find an interval 
that contains some stationarity in the beta estimate. We call this procedure \textbf{tau tuning} to satisfy the \textbf{integrability condition}.




\subsubsection{Choice of $k$}

When testing a particular tau, we need to choose a number of extreme observations $k$ to minimize the bias-variance tradeoff.
We call this procedure \textbf{k tuning} for \textbf{convergence}.

\begin{rhobox}{Optimal Choice of $k$}
Following the classical extreme-value theory framework (see Haan2007~\cite[Equation~(3.2.10)]), the optimal choice of $k$ balances bias (governed by 2RV, i.e., $A(n/k)$) and variance (proportional to $k^{-1/2}$):
\begin{quote}
\[
\mathrm{error}(n, k) \approx C_1\,A(n/k) + C_2\,k^{-1/2}
\]
\end{quote}
where $A(n/k)$ controls bias and $k^{-1/2}$ controls the noise variance.

Solving for the optimal tradeoff yields:
\[
k_n \sim c\, n^{-2\rho/(1-2\rho)}
\]
for some $c > 0$ (constant) and $\rho < 0$.
\end{rhobox}
Once you have chosen proper tau, estimated rho and gamma, you have chosen the number of observations to use, you have chosen k.

It remains that you have performed your FEPLS estimation, which convergence guarantee do you have ?





\paragraph{Threshold choice and growth of $y$.}

For any threshold $y \ge 0$, the FEPLS estimator is defined as
\[
\hat\beta_\varphi(y)
  := \frac{\hat v_\varphi(y)}{\|\hat v_\varphi(y)\|}, \qquad
\hat v_\varphi(y)
  = \frac{1}{n} \sum_{i=1}^n X_i \,\varphi(Y_i)\,\mathbf{1}_{\{Y_i \ge y\}}.
\]
Thus, $\hat\beta_\varphi(y)$ is a weighted combination of the $X_i$ whose $Y_i$ are in the tail, with weights set by the test function $\varphi$.

For consistency results, we consider a sequence of (deterministic) thresholds $(y_{n,k})$ such that
\[
y_{n,k} \sim U\!\left(\frac{n}{k}\right),\qquad n\to\infty,
\]
where $U$ is the tail quantile function of $Y$. In practice, $y_{n,k}$ is approximated by the $(n-k+1)$-th order statistic $Y_{n-k+1,n}$:
\[
Y_{n-k+1,n} \approx y_{n,k} \sim U\!\left(\frac{n}{k}\right)
\quad\text{as}\ n\to\infty.
\]

If the tail of $Y$ is regularly varying, i.e., $\bar F(y) = \mathbb{P}(Y>y) \in \mathrm{RV}_{-1/\gamma}(+\infty)$, then
\[
U(t) \sim C\, t^\gamma \quad(t\to\infty)
\]
for some $C>0$; for example, in the standard Pareto, $U(t)\sim t^\gamma$. For $t=n/k$,
\[
y_{n,k} \sim C\left(\frac{n}{k}\right)^{\gamma}.
\]
We consider intermediate sequences $k = k_n \to \infty$ with $k_n/n \to 0$ so $n/k_n \to \infty$, hence
\[
y_{n,k_n} \sim C\left(\frac{n}{k_n}\right)^{\gamma}
\to +\infty\quad\text{as }n\to\infty.
\]
So, the threshold $y$ grows with $n$ and $k_n$:
\[
y = Y_{n-k+1,n} \approx U\!\left(\frac{n}{k}\right)\to+\infty
\]
and more and more extreme observations are used as $n$ increases.



\section{Consistency Results}

The FEPLS estimator is
\begin{align}
\label{eq:estimator}
\hat{\beta}_\varphi(y) := \frac{\hat{v}_\varphi(y)}{\|\hat{v}_\varphi(y)\|}, \quad \text{where} \quad \hat{v}_\varphi(y) = \frac{1}{n} \sum_{i=1}^n X_i \varphi(Y_i) \mathbf{1}_{\{Y_i \ge y\}}.
\end{align}

\begin{theorem}[Consistency of FEPLS Estimator]
\label{thm:consistency}
Under the previous assumptions, the FEPLS estimator is consistent:
\begin{align}
\|\hat{\beta}_\varphi(Y_{n-k+1,n}) - \beta\| = O_{\p}(\delta_{n,k}) \xrightarrow[n \to +\infty]{} 0,
\end{align}
where the convergence rate is
\[
\delta_{n,k} := \left( g(y_{n,k}) \left(\frac{k}{n}\right)^{1/q} \right)^{-1}
\]
with $g$ and $q$ as in Section~\ref{sec:scalinglaw}. When $k \sim c\, n^{-2\rho/(1-2\rho)}$ for $\rho<0$ and $c>0$, and the bias condition $\sqrt{k}A(n/k)=O(1)$ holds, the rate satisfies
\begin{align}
\label{eq:convergence_rate_exact}
\delta_{n,k} = O\left( n^{(1/q-\gamma\kappa)/(1-2\rho)} \right).
\end{align}

Consistency requires (i) signal dominance $q\kappa\gamma > 1$ (signal tail heavier than noise) and (ii) convergence stability $2(\kappa+\tau)\gamma < 1$ (finite variance).
\end{theorem}

The rate $\delta_{n,k}$ depends on the tail of $Y$ (via $\gamma$ and $\rho$), noise integrability $q$, and the link function $g$ (via $\kappa$). See \cite{girard2023} for detailed proof and results.





From now we are all set to perform extensive empirical validation of the FEPLS estimator.



\section{Empirical Validation}
All our reproducible experiments are available in this \href{https://github.com/janisaiad/FEPLS}{repository}.

\subsection{Data Description}

\paragraph{OHLC data:}
We use one year of mid-frequency (5-minute) open-high-low-close (OHLC) market data for Hungary and Poland, sourced from the open and widely-used financial data aggregator \href{https://stooq.com/db/h/}{Stooq}. The dataset covers both the Hungarian (Budapest Stock Exchange) and Polish (Warsaw Stock Exchange) stock indices.

\paragraph{Data Characteristics:}
\begin{itemize}
    \item \textbf{Frequency:} 5-minute bars (OHLC)
    \item \textbf{Markets:} Hungary (\texttt{BUX}) 50 stocks, Poland (\texttt{WIG20}) 100 stocks
    \item \textbf{Size:} Approximately 50 megabytes compressed for 1 year (per market)
    \item \textbf{Coverage:} Full calendar year (about 50,000--60,000 bars per market)
    \item \textbf{Format:} CSV, standardized with columns for open, high, low, close, and volume
    \item \textbf{Source:} Freely available and reproducible from \href{https://stooq.com/db/h/}{https://stooq.com/db/h/}
\end{itemize}

The analysis uses X as the daily return curve from a stock A and Y as the next day's max return from a stock B.
So we reproduce the analysis from the paper using open-source data.


\paragraph{Tick-by-tick data:}

For the most granular analysis, we purchased 3 months of nanosecond-resolution tick-by-tick data from Databento, 
covering all limit order book events (including every limit order, cancelation, and execution) for p
rominent NASDAQ stocks: Google, Apple, American Airlines (AAL), Amazon, and Microsoft. 
This dataset, considered extremely high quality, cost approximately \$1000 and represents a gold standard for empirical financial 
microstructure research.

\begin{itemize}
    \item \textbf{Frequency:} Nanosecond-scale tick-by-tick (full limit order book)
    \item \textbf{Markets:} NASDAQ (Google, Apple, American Airlines (AAL), Amazon, Microsoft)
    \item \textbf{Duration:} 3 months
    \item \textbf{Events:} All limit order submissions, cancellations, executions (Level 3 order book data)
    \item \textbf{Size/Cost:} Approximately \$1,000 for the full period via Databento
    \item \textbf{Quality:} Highest market microstructure fidelity available
    \item \textbf{Source:} Commercially obtained from \href{https://databento.com/}{Databento} from previous work.
\end{itemize}















\subsection{Statistical Workflow for Empirical Validation}

We adhere to a transparent, reproducible workflow for FEPLS model validation using both Stooq (5-minute OHLC) and Databento (tick) data. The procedure is as follows:

\begin{enumerate}
    \item \textbf{Data Preparation:} Download raw data (Stooq or Databento). Select target stocks and extract daily return curves $X$ (covariate) and the scalar $Y$ (next-day maximum return), possibly from different stocks to capture cross-dependence.
    \item \textbf{Cleaning and Log Return Construction:} Compute 5-minute (or tick) log-returns, handle missing values (e.g., using linear interpolation or LOCF), and align data to uniform time grids.
    \item \textbf{Dependence Check:} Assess temporal dependence (volatility clustering) in extremes via autocorrelation plots; optionally thin the data by taking every $n$th day to reduce dependence.
    \item \textbf{Heavy-Tail Verification:} Plot the Hill estimator for $Y$ to confirm a heavy-tailed regime appropriate for FEPLS. Proceed only if the estimated tail index $\gamma$ is positive.
    \item \textbf{Correlation Check (Optional):} Compute canonical or ordinary correlation between projections $\langle X, \beta \rangle$ and $Y$ to confirm a tractable signal for dimension reduction.
    \item \textbf{Train/Test Chronological Split:} Split the data into training and testing periods. \textbf{ $\tau$ tuning} is carried out on the training set.
    \item \textbf{Parameter Calibration:}
    \begin{enumerate}
        \item Select a conservative test function parameter $\tau$ (e.g., $\tau = -1$).
        \item Optimize the number of extremes $k$ via a grid search: for each $k$, estimate $\hat{\beta}_{\tau, k}$ on the training set and maximize the in-extremes correlation between projections and $Y$.
    \end{enumerate}
    \item \textbf{Validation:} 
    \begin{itemize}
        \item Apply the fitted $\hat{\beta}$ to the test set.
        \item Project new curves $X^{\text{test}}$ onto $\hat{\beta}$ to obtain scalar scores.
        \item Estimate conditional Value-at-Risk (VaR) via quantile regression or local smoothing.
        \item Assess visual coverage (exceedance) and independence of VaR violations.
    \end{itemize}
\end{enumerate}

This workflow ensures both statistical rigor and full reproducibility for all empirical FEPLS validation results.

\section{Empirical Results}

We present a comprehensive empirical validation of FEPLS on Hungarian stock market data. The analysis focuses on cross-asset relationships, examining how intraday return curves from one stock predict extreme returns in another. We analyze multiple stock pairs, with detailed results for the 4IG $\to$ AKKO pair shown in Figure~\ref{fig:consistency_validation}. Additional results for 4IG paired with other stocks (Appeninn, Autowallis, BIF, CIG Pannonia, Dunahouse, GSPark, Richter, Alteo) are provided in the appendix.












\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/4ig_akko_tau_comparison.png}
    \caption{Empirical validation of FEPLS consistency: comparison across different $\tau$ values showing hypothesis verification, correlation analysis, and optimal threshold selection for the 4IG $\to$ AKKO pair.}
    \label{fig:consistency_validation}
    \end{figure}
    
    
    












































\section*{Acknowledgments}

We thank Charles-Albert Lehalle for providing the Databento data.

\onecolumn
\section{Appendix}

\subsection{Comparison between PCA and PLS}

\begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \footnotesize
    \begin{tabular}{p{3.5cm} p{4.5cm} p{4.5cm}}
    \toprule
     & \textbf{PCA (Unsupervised)} & \textbf{PLS (Supervised)} \\
    \midrule
    \textbf{Uses response variable?} 
        & No: PCA operates solely on the covariates $X$, ignoring any relationship with a response variable $Y$.
        & Yes: PLS explicitly incorporates the response $Y$ in the optimization. \\
    \textbf{Objective} 
        & Find directions that maximize the \emph{variance} of the projected data, i.e., $\argmax_{\|w\|=1} \Var(\langle w, X \rangle)$.
        & Find directions that maximize the \emph{covariance} between the projection and response, i.e., $\argmax_{\|w\|=1} \Cov(\langle w, X \rangle, Y)$.\\
    \textbf{Interpretation} 
        & The first principal component captures the direction of maximum variance in $X$, regardless of its relevance to predicting $Y$.
        & The PLS direction captures the direction in $X$ most correlated with $Y$, making it directly relevant to prediction. \\
    \textbf{Strengths/Limitations} 
        & Limitation: In regression contexts, high-variance directions can be orthogonal to the relationship with $Y$, so PCA may prioritize irrelevant directions.
        & Advantage: By focusing on covariance with $Y$, PLS finds directions that are both informative about $X$ and predictive of $Y$. \\
    \bottomrule
    \end{tabular}
    \caption{Comparison between PCA (Principal Component Analysis) and PLS (Partial Least Squares).}
    \label{tab:pca_pls}
    \end{table}
    
    \begin{table}[htbp]
    \centering
    \renewcommand{\arraystretch}{1.2}
    \footnotesize
    \begin{tabular}{p{2.5cm} p{5cm} p{2.5cm} p{5cm}}
    \toprule
    \multicolumn{2}{c}{\textbf{Column 1}} & \multicolumn{2}{c}{\textbf{Column 2}} \\
    \cmidrule(lr){1-2} \cmidrule(lr){3-4}
    \textbf{Symbol} & \textbf{Explanation} & \textbf{Symbol} & \textbf{Explanation} \\
    \midrule
    \multicolumn{4}{l}{\textbf{Regular Variation \& Auxiliary Functions}} \\
    $\RV_\tau(+\infty)$ & Regularly-varying (index $\tau$) & $2\RV_{\gamma, \rho}(+\infty)$ & Second-order RV \\
    $H_\rho(y)$ & $y^{\gamma}\int_1^y u^{\rho-1}\mathrm{d}u$ & $A(t)$ & Auxiliary function, $A(t) \to 0$ \\
    \midrule
    \multicolumn{4}{l}{\textbf{Model and FEPLS Notation}} \\
    $X = g(Y)\beta + \varepsilon$ & Inverse regression model & $\beta \in H$, $\zp{\beta}=1$ & Index vector (unit norm) \\
    $g \in \RV_\kappa(+\infty)$ & Link function (index $\kappa$) & $\varepsilon: \Omega \to H$ & Noise term \\
    $\varphi \in \RV_\tau(+\infty)$ & Test function (index $\tau$) & $w_\varphi(y)$ & Theoretical FEPLS direction \\
    $\hat{\beta}_\varphi(y)$ & Empirical FEPLS estimator & & \\
    \midrule
    \multicolumn{4}{l}{\textbf{Key Parameters}} \\
    $\gamma \in (0,1)$ & Tail index of $Y$ & $\rho \le 0$ & Second-order parameter \\
    $\kappa > 0$ & Link/model function index & $\tau \in \R$ & Test function index \\
    $k=k_n$ & Intermediate sequence & $y_{n,k} \sim U(n/k)$ & High threshold \\
    $\delta_{n,k}$ & $(g(y_{n,k})(k/n)^{1/q})^{-1}$ & $q > 2$ & Noise integrability order \\
    \bottomrule
    \end{tabular}
    \caption{Summary of main notation used in the FEPLS framework.}
    \label{tab:notation}
    \end{table}

\subsection{Additional Empirical Results: 4IG Stock Pairings}

This section presents detailed FEPLS analysis results for 4IG paired with multiple stocks from the Hungarian market: Appeninn, Autowallis, BIF, CIG Pannonia, Dunahouse, GSPark, Richter, and Alteo. 

The figures are organized by type:
\begin{itemize}
    \item \textbf{Hill, Q-Q, and tail plots} (e.g., \texttt{4ig\_akko\_hill\_qq\_tail.png}): Main diagnostic plots including exceedance analysis, Hill plot for tail index estimation, and Q-Q plot for goodness-of-fit assessment.
    \item \textbf{Conditional quantile} (e.g., \texttt{4ig\_akko\_conditional\_quantile.png}): Conditional quantile plots showing the relationship between projections and extreme responses.
    \item \textbf{Beta comparison} (e.g., \texttt{4ig\_akko\_tau\_-2.0\_-0.5.png}): Beta comparison plots comparing the estimated FEPLS direction $\hat{\beta}$ across different $\tau$ values.
\end{itemize}

\subsubsection{4IG with Appeninn}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_appeninn_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ Appeninn: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_appeninn}
\end{figure}

\subsubsection{4IG with Autowallis}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_autowallis_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ Autowallis: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_autowallis}
\end{figure}

\subsubsection{4IG with BIF}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_bif_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ BIF: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_bif}
\end{figure}

\subsubsection{4IG with CIG Pannonia}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_cigpannonia_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ CIG Pannonia: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_cigpannonia}
\end{figure}

\subsubsection{4IG with Dunahouse}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_dunahouse_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ Dunahouse: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_dunahouse}
\end{figure}

\subsubsection{4IG with GSPark}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_gspark_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ GSPark: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_gspark}
\end{figure}

\subsubsection{4IG with Richter}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_richter_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ Richter: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_richter}
\end{figure}

\subsubsection{4IG with Alteo}
\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_alteo_hill_qq_tail.png}
\caption{FEPLS analysis for 4IG $\to$ Alteo: exceedance analysis, Hill plot, and Q-Q plot.}
\label{fig:4ig_alteo}
\end{figure}

\subsection{Conditional Quantile Plots}

For each stock pair, we provide conditional quantile plots showing the relationship between FEPLS projections and extreme responses. Sample plots for selected pairs are shown below.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_akko_conditional_quantile.png}
\caption{Conditional quantile plot for 4IG $\to$ AKKO.}
\label{fig:4ig_akko_conditional}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_appeninn_conditional_quantile.png}
\caption{Conditional quantile plot for 4IG $\to$ Appeninn.}
\label{fig:4ig_appeninn_conditional}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_autowallis_conditional_quantile.png}
\caption{Conditional quantile plot for 4IG $\to$ Autowallis.}
\label{fig:4ig_autowallis_conditional}
\end{figure}

\subsection{Tau Comparison Plots}

For each stock pair, we provide detailed $\tau$ comparison plots (files with \texttt{comparison} in the name) showing the sensitivity of FEPLS estimates to the test function parameter. These plots include hypothesis verification ($2(\kappa+\tau)\gamma$), correlation analysis, and optimal threshold selection across different $\tau$ values. Sample plots for selected pairs are shown below.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_autowallis_tau_comparison.png}
\caption{$\tau$ comparison for 4IG $\to$ Autowallis.}
\label{fig:4ig_autowallis_tau}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_bif_tau_comparison.png}
\caption{$\tau$ comparison for 4IG $\to$ BIF.}
\label{fig:4ig_bif_tau}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.4\textwidth]{figures/4ig_alteo_tau_comparison.png}
\caption{$\tau$ comparison for 4IG $\to$ Alteo.}
\label{fig:4ig_alteo_tau}
\end{figure}

Additional figures for all stock pairs are available in the figures directory:
\begin{itemize}
    \item Hill, Q-Q, and tail plots (files with \texttt{hill\_qq\_tail} in the name) for diagnostic assessment
    \item Conditional quantile plots (files with \texttt{conditional\_quantile} in the name) showing the relationship between projections and extreme responses
    \item Beta comparison plots (4 digits in filename) comparing $\hat{\beta}$ across different $\tau$ pairs
\end{itemize}

\twocolumn

% --- Bibliography ---
\begin{thebibliography}{9}

\bibitem{girard2023}
Girard, S., and Pakzad, A. 2023.
\textit{Functional Extreme Partial Least Squares}.
Journal of Multivariate Analysis.

\bibitem{peyre2019}
Peyré, G., and Cuturi, M. 2019.
\textit{Computational optimal transport: With applications to data science}.
Foundations and Trends® in Machine Learning, 11(5-6), 355--607.

\bibitem{sejourne2019}
Séjourné, T., Feydy, J., Vialard, F. X., Trouvé, A., and Peyré, G. 2019.
\textit{Sinkhorn divergences for unbalanced optimal transport}.
arXiv preprint arXiv:1910.12958.

\bibitem{hill1975}
Hill, B. M. 1975.
\textit{A simple general approach to inference about the tail of a distribution}.
The Annals of Statistics, 3(5), 1163--1174.

\bibitem{de2007}
de Haan, L., and Ferreira, A. 2007.
\textit{Extreme value theory: an introduction}.
Springer Science \& Business Media.

\bibitem{resnick2007}
Resnick, S. I. 2007.
\textit{Heavy-tail phenomena: probabilistic and statistical modeling}.
Springer Science \& Business Media.

\bibitem{embrechts2013}
Embrechts, P., Klüppelberg, C., and Mikosch, T. 2013.
\textit{Modelling extremal events: for insurance and finance}.
Springer Science \& Business Media.

\bibitem{ramsay2005}
Ramsay, J. O., and Silverman, B. W. 2005.
\textit{Functional data analysis}.
Springer Science \& Business Media.

\bibitem{horvath2012}
Horváth, L., and Kokoszka, P. 2012.
\textit{Inference for functional data with applications}.
Springer Science \& Business Media.

\end{thebibliography}

\end{document}

